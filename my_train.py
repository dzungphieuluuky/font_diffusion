"""
FontDiffuser Training Script
‚úÖ Uses hash-based file naming with unicode characters
‚úÖ Discovers images from filesystem (not from checkpoint paths)
‚úÖ Compatible with results_checkpoint.json structure
"""

import os
import math
import time
import logging
import hashlib
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm.auto import tqdm

import torch
import torch.nn.functional as F
from torchvision import transforms
from torchvision.transforms import (
    RandomAffine,
    ColorJitter,
    RandomRotation,
    GaussianBlur,
)

from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from diffusers.optimization import get_scheduler

from dataset.my_font_dataset import MyFontDataset
from dataset.collate_fn import CollateFN
from configs.fontdiffuser import get_parser
from src import (
    FontDiffuserModel,
    ContentPerceptualLoss,
    build_unet,
    build_style_encoder,
    build_content_encoder,
    build_ddpm_scheduler,
    build_scr,
)
from utils import (
    save_args_to_yaml,
    x0_from_epsilon,
    reNormalize_img,
    normalize_mean_std,
)

try:
    from safetensors.torch import load_file as load_safetensors

    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False
    print("‚ö†Ô∏è  safetensors not installed. Only .pth files will be supported.")


logger = get_logger(__name__)


# ============================================================================
# Hash-based filename utilities
# ============================================================================


def compute_file_hash(char: str, style: str, font: str = "") -> str:
    """Compute deterministic hash for a (character, style, font) combination"""
    content = f"{char}_{style}_{font}"
    return hashlib.sha256(content.encode("utf-8")).hexdigest()[:8]


def parse_content_filename(filename: str) -> Optional[str]:
    """
    Parse content filename to extract character
    Format: U+XXXX_[char]_hash.png or U+XXXX_hash.png

    Returns:
        Character string or None if parsing fails
    """
    if not filename.endswith(".png"):
        return None

    stem = filename[:-4]  # Remove .png
    parts = stem.split("_")

    if len(parts) < 2:
        return None

    # First part should be codepoint
    codepoint = parts[0]
    if not codepoint.startswith("U+"):
        return None

    try:
        # Decode character from codepoint
        char_code = int(codepoint.replace("U+", ""), 16)
        char = chr(char_code)

        # Validate: parts[1] should be the character itself (if present)
        # or it could be directly the hash
        # Format: U+XXXX_char_hash.png or U+XXXX_hash.png

        if len(parts) >= 3:
            # Likely format: U+XXXX_char_hash.png
            # parts[1] = char, parts[2] = hash
            return char
        elif len(parts) == 2:
            # Format: U+XXXX_hash.png
            return char
        else:
            return None

    except (ValueError, OverflowError):
        return None


def parse_target_filename(filename: str) -> Optional[Tuple[str, str]]:
    """
    Parse target filename to extract character and style
    Format: U+XXXX_char_style_hash.png or U+XXXX_style_hash.png

    Returns:
        (character, style) tuple or None if parsing fails
    """
    if not filename.endswith(".png"):
        return None

    stem = filename[:-4]  # Remove .png
    parts = stem.split("_")

    if len(parts) < 3:
        return None

    # First part should be codepoint
    codepoint = parts[0]
    if not codepoint.startswith("U+"):
        return None

    try:
        # Decode character from codepoint
        char_code = int(codepoint.replace("U+", ""), 16)
        char = chr(char_code)

        # Hash is always the last part (8 hex characters)
        hash_val = parts[-1]

        # Validate hash is 8 hex chars
        if len(hash_val) != 8 or not all(
            c in "0123456789abcdef" for c in hash_val.lower()
        ):
            return None

        # parts[0] = codepoint (U+XXXX)
        # parts[1] = character itself (optional, if printable)
        # parts[2:-1] = style parts (can have underscores)
        # parts[-1] = hash

        # Check if parts[1] is the character (safe version of char)
        safe_char = char if char.isprintable() and char not in '<>:"/\\|?*' else ""

        if len(parts) >= 4 and parts[1] == safe_char:
            # Format: U+XXXX_char_style_hash
            # Extract style from parts[2:-1]
            style_parts = parts[2:-1]
        else:
            # Format: U+XXXX_style_hash
            # Extract style from parts[1:-1]
            style_parts = parts[1:-1]

        if not style_parts:
            return None

        style = "_".join(style_parts)
        return char, style

    except (ValueError, OverflowError, IndexError):
        return None


def discover_content_images(content_dir: str) -> Dict[str, str]:
    """
    Discover content images from actual filesystem
    ‚úÖ Uses hash-based naming: U+XXXX_[char]_hash.png
    ‚úÖ Handles case-insensitive file extensions
    ‚úÖ Provides detailed diagnostics

    Returns:
        Dict mapping character -> image_path
    """
    char_images = {}
    content_path = Path(content_dir)

    # ‚úÖ Debug: Check if path exists and print actual path
    if not content_path.exists():
        print(f"\n‚ùå ERROR: ContentImage directory not found!")
        print(f"   Expected: {content_path.resolve()}")
        print(f"   Does not exist: {not content_path.exists()}")
        print(f"   Is absolute: {content_path.is_absolute()}")
        raise FileNotFoundError(f"ContentImage directory not found: {content_dir}")

    print(f"\nüîç Discovering content images from {content_dir}...")
    print(f"   Full path: {content_path.resolve()}")

    # ‚úÖ Handle case-insensitive extensions (.png, .PNG, .Png, etc.)
    png_files = sorted(
        list(content_path.glob("*.png"))
        + list(content_path.glob("*.PNG"))
        + list(content_path.glob("*.Png"))
        + list(content_path.glob("*.pNg"))
    )

    # Remove duplicates while preserving order
    seen = set()
    unique_png_files = []
    for f in png_files:
        if f.name.lower() not in seen:
            seen.add(f.name.lower())
            unique_png_files.append(f)
    png_files = sorted(unique_png_files)

    if not png_files:
        # ‚úÖ More detailed error message
        print(f"\n‚ùå ERROR: No PNG files found in {content_dir}")
        print(f"   Directory contents:")
        if content_path.exists():
            items = list(content_path.iterdir())
            if items:
                for item in sorted(items)[:10]:  # Show first 10 items
                    item_type = "DIR" if item.is_dir() else "FILE"
                    print(f"     [{item_type}] {item.name}")
                if len(items) > 10:
                    print(f"     ... and {len(items) - 10} more items")
            else:
                print(f"     (directory is empty)")
        raise ValueError(
            f"No PNG files found in {content_dir}\n"
            f"   Expected format: U+XXXX_[char]_hash.png\n"
            f"   Check that images are actually present in this directory"
        )

    print(f"  üìä Found {len(png_files)} PNG files, parsing...")

    failed_files = []
    for img_file in tqdm(
        png_files,
        desc="  Scanning content",
        ncols=100,
        unit="file",
    ):
        char = parse_content_filename(img_file.name)

        if char is None:
            failed_files.append(img_file.name)
            tqdm.write(f"    ‚ö†Ô∏è  Failed to parse: {img_file.name}")
            continue

        # Store by character (not index)
        if char in char_images:
            tqdm.write(
                f"    ‚ö†Ô∏è  Duplicate character '{char}' (U+{ord(char):04X}): {img_file.name}\n"
                f"        Previous: {char_images[char]}\n"
                f"        Current:  {img_file}"
            )
        else:
            char_images[char] = str(img_file)

    if not char_images:
        print(f"\n‚ùå ERROR: No valid content images found!")
        print(f"   Total PNG files: {len(png_files)}")
        print(f"   Failed to parse: {len(failed_files)}")
        if failed_files:
            print(f"   Failed examples:")
            for fname in failed_files[:5]:
                print(f"     - {fname}")
        raise ValueError(f"No valid content images found in {content_dir}")

    print(f"  ‚úì Found {len(char_images)} valid content images")

    # ‚úÖ Show sample of parsed characters
    sample_chars = sorted(list(char_images.keys()))[:5]
    print(f"    Sample characters: {[f'{c} (U+{ord(c):04X})' for c in sample_chars]}")

    return char_images


def discover_target_images(target_dir: str) -> Dict[Tuple[str, str], str]:
    """
    Discover target images from actual filesystem
    ‚úÖ Uses hash-based naming: U+XXXX_[char]_style_hash.png
    ‚úÖ Handles case-insensitive file extensions
    ‚úÖ Provides detailed diagnostics

    Returns:
        Dict mapping (character, style) -> image_path
    """
    target_images = {}
    target_path = Path(target_dir)

    # ‚úÖ Debug: Check if path exists
    if not target_path.exists():
        print(f"\n‚ùå ERROR: TargetImage directory not found!")
        print(f"   Expected: {target_path.resolve()}")
        raise FileNotFoundError(f"TargetImage directory not found: {target_dir}")

    print(f"\nüîç Discovering target images from {target_dir}...")
    print(f"   Full path: {target_path.resolve()}")

    # Iterate through style directories
    style_dirs = sorted([d for d in target_path.iterdir() if d.is_dir()])

    if not style_dirs:
        print(f"\n‚ùå ERROR: No style directories found in {target_dir}")
        print(f"   Directory contents:")
        items = list(target_path.iterdir())
        for item in sorted(items)[:10]:
            item_type = "DIR" if item.is_dir() else "FILE"
            print(f"     [{item_type}] {item.name}")
        raise ValueError(f"No style directories found in {target_dir}")

    print(f"  üìä Found {len(style_dirs)} style directories")

    failed_styles = {}

    for style_dir in tqdm(style_dirs, desc="  Scanning styles", ncols=100, unit="dir"):
        style_name = style_dir.name

        # ‚úÖ Handle case-insensitive extensions
        png_files = sorted(
            list(style_dir.glob("*.png"))
            + list(style_dir.glob("*.PNG"))
            + list(style_dir.glob("*.Png"))
            + list(style_dir.glob("*.pNg"))
        )

        # Remove duplicates
        seen = set()
        unique_png_files = []
        for f in png_files:
            if f.name.lower() not in seen:
                seen.add(f.name.lower())
                unique_png_files.append(f)
        png_files = sorted(unique_png_files)

        if not png_files:
            tqdm.write(f"    ‚ö†Ô∏è  No images in style '{style_name}'")
            failed_styles[style_name] = 0
            continue

        failed_count = 0
        for img_file in png_files:
            parsed = parse_target_filename(img_file.name)

            if parsed is None:
                tqdm.write(f"    ‚ö†Ô∏è  Failed to parse: {img_file.name}")
                failed_count += 1
                continue

            char, parsed_style = parsed

            # ‚úÖ Validate style matches directory
            if parsed_style != style_name:
                tqdm.write(
                    f"    ‚ö†Ô∏è  Style mismatch in '{style_name}': {img_file.name}\n"
                    f"        Extracted: '{parsed_style}', Expected: '{style_name}'"
                )
                failed_count += 1
                continue

            # Check for duplicates
            if (char, style_name) in target_images:
                tqdm.write(
                    f"    ‚ö†Ô∏è  Duplicate ({char} U+{ord(char):04X}, {style_name}): {img_file.name}"
                )
            else:
                target_images[(char, style_name)] = str(img_file)

        if failed_count > 0:
            failed_styles[style_name] = failed_count

    if not target_images:
        print(f"\n‚ùå ERROR: No valid target images found!")
        print(f"   Total style directories: {len(style_dirs)}")
        print(f"   Failed to parse by style:")
        for style, count in failed_styles.items():
            print(f"     - {style}: {count} failed")
        raise ValueError(f"No valid target images found in {target_dir}")

    print(f"  ‚úì Found {len(target_images)} valid target images")

    # ‚úÖ Show statistics
    unique_styles = set(style for char, style in target_images.keys())
    unique_chars = set(char for char, style in target_images.keys())
    print(f"    Unique styles: {len(unique_styles)}")
    print(f"    Unique characters: {len(unique_chars)}")

    return target_images


def diagnose_image_discovery(data_root: str) -> None:
    """Diagnose image discovery issues"""
    print("\n" + "=" * 70)
    print("üîß IMAGE DISCOVERY DIAGNOSTICS")
    print("=" * 70)

    content_dir = os.path.join(data_root, "train", "ContentImage")
    target_dir = os.path.join(data_root, "train", "TargetImage")

    print(f"\nüìÅ Content Directory: {content_dir}")
    print(f"   Exists: {os.path.exists(content_dir)}")
    print(f"   Is directory: {os.path.isdir(content_dir)}")
    print(f"   Absolute path: {os.path.abspath(content_dir)}")

    if os.path.exists(content_dir):
        items = os.listdir(content_dir)
        print(f"   Items: {len(items)}")
        png_count = len([f for f in items if f.lower().endswith(".png")])
        print(f"   PNG files: {png_count}")
        print(f"   Sample files:")
        for f in sorted(items)[:5]:
            print(f"     - {f}")

    print(f"\nüìÅ Target Directory: {target_dir}")
    print(f"   Exists: {os.path.exists(target_dir)}")
    print(f"   Is directory: {os.path.isdir(target_dir)}")
    print(f"   Absolute path: {os.path.abspath(target_dir)}")

    if os.path.exists(target_dir):
        subdirs = [
            d
            for d in os.listdir(target_dir)
            if os.path.isdir(os.path.join(target_dir, d))
        ]
        print(f"   Style directories: {len(subdirs)}")
        for subdir in sorted(subdirs)[:5]:
            subdir_path = os.path.join(target_dir, subdir)
            png_files = [
                f for f in os.listdir(subdir_path) if f.lower().endswith(".png")
            ]
            print(f"     - {subdir}: {len(png_files)} images")

    print("=" * 70 + "\n")


def validate_image_paths(
    content_images: Dict[str, str],
    target_images: Dict[Tuple[str, str], str],
) -> None:
    """
    ‚úÖ Validate that all target images have corresponding content images
    """
    print(f"\nüìã Validating image pairs...")

    # Find which characters have content images
    content_chars = set(content_images.keys())

    # Find which (char, style) pairs exist
    existing_pairs = set(target_images.keys())

    # Extract unique styles and characters from existing pairs
    existing_styles = set(style for char, style in existing_pairs)
    target_chars = set(char for char, style in existing_pairs)

    print(f"  Content images:      {len(content_chars)} characters")
    print(f"  Target images:       {len(target_images)} (char, style) pairs")
    print(f"  Unique styles:       {len(existing_styles)}")
    print(f"  Unique chars in targets: {len(target_chars)}")

    # Find mismatches
    missing_content = target_chars - content_chars
    unused_content = content_chars - target_chars

    if missing_content:
        print(f"  ‚ö†Ô∏è  {len(missing_content)} target chars missing content images")
        # Show sample
        sample = sorted(list(missing_content))[:5]
        print(f"      Examples: {[f'{c} (U+{ord(c):04X})' for c in sample]}")

    if unused_content:
        print(f"  ‚ö†Ô∏è  {len(unused_content)} content images have no targets")
        # Show sample
        sample = sorted(list(unused_content))[:5]
        print(f"      Examples: {[f'{c} (U+{ord(c):04X})' for c in sample]}")

    # Only warn if there are actual mismatches
    if not missing_content and not unused_content:
        print(f"  ‚úÖ All content images have corresponding targets!")


def get_args():
    parser = get_parser()
    args = parser.parse_args()
    env_local_rank = int(os.environ.get("LOCAL_RANK", -1))
    if env_local_rank != -1 and env_local_rank != args.local_rank:
        args.local_rank = env_local_rank
    style_image_size = args.style_image_size
    content_image_size = args.content_image_size
    args.style_image_size = (style_image_size, style_image_size)
    args.content_image_size = (content_image_size, content_image_size)

    return args


def validate(model, val_dataloader, noise_scheduler, accelerator, args, global_step):
    """Run validation loop and return average validation loss"""
    model.eval()
    val_loss = 0.0
    num_batches = 0

    with torch.inference_mode():
        for val_samples in val_dataloader:
            content_images = val_samples["content_image"]
            style_images = val_samples["style_image"]
            target_images = val_samples["target_image"]

            noise = torch.randn_like(target_images)
            bsz = target_images.shape[0]
            timesteps = torch.randint(
                0,
                noise_scheduler.num_train_timesteps,
                (bsz,),
                device=target_images.device,
            ).long()
            noisy_target_images = noise_scheduler.add_noise(
                target_images, noise, timesteps
            )

            # Classifier-free validation (no masking)
            noise_pred, _ = model(
                x_t=noisy_target_images,
                timesteps=timesteps,
                style_images=style_images,
                content_images=content_images,
                content_encoder_downsample_size=args.content_encoder_downsample_size,
            )

            batch_val_loss = F.mse_loss(
                noise_pred.float(), noise.float(), reduction="mean"
            )
            val_loss += batch_val_loss.item()
            num_batches += 1

    avg_val_loss = val_loss / max(num_batches, 1)
    return avg_val_loss


def load_checkpoint_file(checkpoint_path: str, device="cpu"):
    """
    Load checkpoint from either .pth or .safetensors format

    Args:
        checkpoint_path: Path to checkpoint file (.pth or .safetensors)
        device: Device to load to

    Returns:
        Loaded state dict

    Raises:
        FileNotFoundError: If checkpoint doesn't exist
        ValueError: If unsupported file format
    """
    checkpoint_path = str(checkpoint_path)

    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"‚ùå Checkpoint not found: {checkpoint_path}")

    file_ext = os.path.splitext(checkpoint_path)[1].lower()

    try:
        if file_ext == ".safetensors":
            if not SAFETENSORS_AVAILABLE:
                raise ValueError(
                    f"‚ùå safetensors format not supported.\n"
                    f"   Install: pip install safetensors\n"
                    f"   Or use .pth format instead"
                )

            print(f"  üìÇ Loading safetensors: {os.path.basename(checkpoint_path)}")
            state_dict = load_safetensors(checkpoint_path)
            return state_dict

        elif file_ext == ".pth":
            print(f"  üìÇ Loading .pth: {os.path.basename(checkpoint_path)}")
            state_dict = torch.load(checkpoint_path, map_location=device)
            return state_dict

        else:
            raise ValueError(
                f"‚ùå Unsupported checkpoint format: {file_ext}\n"
                f"   Supported: .pth, .safetensors\n"
                f"   Got: {checkpoint_path}"
            )

    except Exception as e:
        raise RuntimeError(f"‚ùå Error loading checkpoint {checkpoint_path}: {e}")


def load_phase_1_checkpoint(args, unet, style_encoder, content_encoder):
    """
    Load Phase 1 checkpoint (unet, style_encoder, content_encoder)
    Handles both .pth and .safetensors formats

    Args:
        args: Training arguments
        unet: UNet model to load state into
        style_encoder: Style encoder model to load state into
        content_encoder: Content encoder model to load state into
    """
    if not args.phase_2:
        return

    ckpt_dir = Path(args.phase_1_ckpt_dir)

    if not ckpt_dir.exists():
        raise ValueError(f"‚ùå Phase 1 checkpoint directory not found: {ckpt_dir}")

    print(f"\nüì• Loading Phase 1 checkpoint from: {ckpt_dir}")
    print("=" * 60)

    # Define checkpoint files to load (try both formats)
    checkpoint_configs = [
        ("unet", unet),
        ("style_encoder", style_encoder),
        ("content_encoder", content_encoder),
    ]

    for model_name, model in checkpoint_configs:
        # Try .safetensors first, then .pth
        safetensors_path = ckpt_dir / f"{model_name}.safetensors"
        pth_path = ckpt_dir / f"{model_name}.pth"

        checkpoint_path = None

        if safetensors_path.exists():
            checkpoint_path = safetensors_path
        elif pth_path.exists():
            checkpoint_path = pth_path
        else:
            raise FileNotFoundError(
                f"‚ùå {model_name} checkpoint not found!\n"
                f"   Tried: {safetensors_path}\n"
                f"          {pth_path}"
            )

        try:
            state_dict = load_checkpoint_file(str(checkpoint_path))
            model.load_state_dict(state_dict)
            print(f"  ‚úì {model_name}: {checkpoint_path.name}")
        except Exception as e:
            raise RuntimeError(f"‚ùå Error loading {model_name}: {e}")

    print("=" * 60)
    print("‚úì Phase 1 checkpoint loaded successfully\n")


def load_scr_checkpoint(scr_ckpt_path: str, scr_model):
    """
    Load SCR (Style-Content contrastive) module checkpoint
    Handles both .pth and .safetensors formats

    Args:
        scr_ckpt_path: Path to SCR checkpoint
        scr_model: SCR model to load state into
    """
    scr_ckpt_path = Path(scr_ckpt_path)

    if not scr_ckpt_path.exists():
        raise FileNotFoundError(f"‚ùå SCR checkpoint not found: {scr_ckpt_path}")

    print(f"üì• Loading SCR module from: {scr_ckpt_path}")

    try:
        state_dict = load_checkpoint_file(str(scr_ckpt_path))
        scr_model.load_state_dict(state_dict)
        print(f"  ‚úì SCR module loaded: {scr_ckpt_path.name}\n")
    except Exception as e:
        raise RuntimeError(f"‚ùå Error loading SCR module: {e}")


def save_checkpoint(model, accelerator, args, global_step, is_best=False):
    """Save model checkpoint in both .pth and .safetensors format"""
    if is_best:
        save_dir = f"{args.output_dir}/best_checkpoint"
    else:
        save_dir = f"{args.output_dir}/global_step_{global_step}"

    os.makedirs(save_dir, exist_ok=True)

    # Save as .pth (original format)
    torch.save(model.unet.state_dict(), f"{save_dir}/unet.pth")
    torch.save(model.style_encoder.state_dict(), f"{save_dir}/style_encoder.pth")
    torch.save(model.content_encoder.state_dict(), f"{save_dir}/content_encoder.pth")
    torch.save(model, f"{save_dir}/total_model.pth")

    # ‚úÖ ALSO save as .safetensors if available
    if SAFETENSORS_AVAILABLE:
        try:
            from safetensors.torch import save_file as save_safetensors

            save_safetensors(model.unet.state_dict(), f"{save_dir}/unet.safetensors")
            save_safetensors(
                model.style_encoder.state_dict(),
                f"{save_dir}/style_encoder.safetensors",
            )
            save_safetensors(
                model.content_encoder.state_dict(),
                f"{save_dir}/content_encoder.safetensors",
            )
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not save safetensors format: {e}")

    logging.info(
        f"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}] "
        f"Saved checkpoint at global step {global_step}"
    )
    print(f"‚úì Saved checkpoint at global step {global_step}")


def main():
    args = get_args()

    # Add missing arguments with defaults
    if not hasattr(args, "val_interval"):
        args.val_interval = 100  # Validate every 100 steps

    # ===== Validation: Check checkpoint interval vs max_train_steps =====
    if args.ckpt_interval > args.max_train_steps:
        raise ValueError(
            f"‚ùå ERROR: ckpt_interval ({args.ckpt_interval}) is larger than "
            f"max_train_steps ({args.max_train_steps})!\n"
            f"   Set ckpt_interval to a value < {args.max_train_steps}\n"
            f"   Suggested: ckpt_interval = {args.max_train_steps // 4}"
        )

    if args.max_train_steps % args.ckpt_interval != 0:
        recommended = (args.max_train_steps // args.ckpt_interval) * args.ckpt_interval
        print(
            f"‚ö†Ô∏è  WARNING: max_train_steps ({args.max_train_steps}) is not divisible by "
            f"ckpt_interval ({args.ckpt_interval}).\n"
            f"   Last checkpoint will be at step {recommended}, final model will be saved separately."
        )
        logging.warning(
            f"max_train_steps ({args.max_train_steps}) not divisible by "
            f"ckpt_interval ({args.ckpt_interval})"
        )

    logging_dir = f"{args.output_dir}/{args.logging_dir}"

    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_dir=logging_dir,
    )

    if accelerator.is_main_process:
        os.makedirs(args.output_dir, exist_ok=True)

    logging.basicConfig(
        filename=f"{args.output_dir}/fontdiffuser_training.log",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    # Set training seed
    if args.seed is not None:
        set_seed(args.seed)

    # ============================================================================
    # ‚úÖ DISCOVER IMAGES FROM FILESYSTEM (hash-based naming)
    # ============================================================================
    print(f"\nüìÇ Discovering images from filesystem...")
    print("=" * 60)

    content_dir = os.path.join(args.data_root, "train", "ContentImage")
    target_dir = os.path.join(args.data_root, "train", "TargetImage")

    # ‚úÖ Add diagnostics before discovery
    diagnose_image_discovery(args.data_root)

    content_images = discover_content_images(content_dir)
    target_images = discover_target_images(target_dir)
    validate_image_paths(content_images, target_images)

    print("=" * 60)

    # Load model and noise_scheduler
    unet = build_unet(args=args)
    style_encoder = build_style_encoder(args=args)
    content_encoder = build_content_encoder(args=args)
    noise_scheduler = build_ddpm_scheduler(args)

    # ‚úÖ Load Phase 1 checkpoint with proper error handling
    if args.phase_2:
        try:
            load_phase_1_checkpoint(args, unet, style_encoder, content_encoder)
        except (FileNotFoundError, ValueError, RuntimeError) as e:
            print(f"\n{e}")
            logging.error(str(e))
            raise

    model = FontDiffuserModel(
        unet=unet, style_encoder=style_encoder, content_encoder=content_encoder
    )

    # Build content perceptual loss
    perceptual_loss = ContentPerceptualLoss()

    scr = None
    if args.phase_2:
        scr = build_scr(args=args)
        try:
            load_scr_checkpoint(args.scr_ckpt_path, scr)
        except (FileNotFoundError, RuntimeError) as e:
            print(f"\n{e}")
            logging.error(str(e))
            raise

        scr.requires_grad_(False)
        logging.info(f"Loaded SCR module from: {args.scr_ckpt_path}")

    # Load the training dataset with augmentation
    content_transforms = transforms.Compose(
        [
            transforms.Resize(
                args.content_image_size,
                interpolation=transforms.InterpolationMode.BILINEAR,
            ),
            RandomRotation(degrees=5),
            RandomAffine(degrees=0, translate=(0.05, 0.05), shear=(-5, 5)),
            ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),
            GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ]
    )

    style_transforms = transforms.Compose(
        [
            transforms.Resize(
                args.style_image_size,
                interpolation=transforms.InterpolationMode.BILINEAR,
            ),
            RandomRotation(degrees=5),
            ColorJitter(brightness=0.15, contrast=0.15),
            GaussianBlur(kernel_size=3, sigma=(0.1, 0.5)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ]
    )

    target_transforms = transforms.Compose(
        [
            transforms.Resize(
                (args.resolution, args.resolution),
                interpolation=transforms.InterpolationMode.BILINEAR,
            ),
            RandomRotation(degrees=3),
            ColorJitter(brightness=0.1, contrast=0.15),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ]
    )

    train_font_dataset = MyFontDataset(
        args=args,
        phase="train",
        transforms=[content_transforms, style_transforms, target_transforms],
        scr=args.phase_2,
    )

    train_dataloader = torch.utils.data.DataLoader(
        train_font_dataset,
        shuffle=True,
        batch_size=args.train_batch_size,
        collate_fn=CollateFN(),
    )

    # Load validation dataset (without augmentation)
    val_content_transforms = transforms.Compose(
        [
            transforms.Resize(
                args.content_image_size,
                interpolation=transforms.InterpolationMode.BILINEAR,
            ),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ]
    )

    val_style_transforms = transforms.Compose(
        [
            transforms.Resize(
                args.style_image_size,
                interpolation=transforms.InterpolationMode.BILINEAR,
            ),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ]
    )

    val_target_transforms = transforms.Compose(
        [
            transforms.Resize(
                (args.resolution, args.resolution),
                interpolation=transforms.InterpolationMode.BILINEAR,
            ),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ]
    )

    val_font_dataset = MyFontDataset(
        args=args,
        phase="val",
        transforms=[
            val_content_transforms,
            val_style_transforms,
            val_target_transforms,
        ],
        scr=args.phase_2,
    )

    val_dataloader = torch.utils.data.DataLoader(
        val_font_dataset,
        shuffle=False,
        batch_size=args.train_batch_size,
        collate_fn=CollateFN(),
    )

    # Build optimizer and learning rate scheduler
    if args.scale_lr:
        args.learning_rate = (
            args.learning_rate
            * args.gradient_accumulation_steps
            * args.train_batch_size
            * accelerator.num_processes
        )

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.learning_rate,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.adam_weight_decay,
        eps=args.adam_epsilon,
    )

    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,
        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
    )

    # Accelerate preparation
    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, lr_scheduler
    )

    val_dataloader = accelerator.prepare(val_dataloader)

    # Move SCR module to target device
    if scr is not None:
        scr = scr.to(accelerator.device)

    # Initialize trackers and save config
    if accelerator.is_main_process:
        accelerator.init_trackers(
            args.experience_name,
            config={
                "max_train_steps": args.max_train_steps,
                "train_batch_size": args.train_batch_size,
                "learning_rate": args.learning_rate,
                "perceptual_coefficient": args.perceptual_coefficient,
                "offset_coefficient": args.offset_coefficient,
                "sc_coefficient": args.sc_coefficient if args.phase_2 else 0,
                "phase_2": args.phase_2,
                "seed": args.seed,
            },
        )
        save_args_to_yaml(
            args=args,
            output_file=f"{args.output_dir}/{args.experience_name}_config.yaml",
        )

    # Setup progress bar
    progress_bar = tqdm(
        range(args.max_train_steps), disable=not accelerator.is_local_main_process
    )
    progress_bar.set_description("Steps")

    # Training epoch setup
    num_update_steps_per_epoch = math.ceil(
        len(train_dataloader) / args.gradient_accumulation_steps
    )
    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

    global_step = 0
    best_val_loss = float("inf")

    logging.info(
        f"Starting training with {num_train_epochs} epochs, "
        f"{num_update_steps_per_epoch} steps per epoch"
    )
    logging.info(
        f"Max training steps: {args.max_train_steps}, "
        f"Checkpoint interval: {args.ckpt_interval}"
    )

    for epoch in range(num_train_epochs):
        epoch_train_loss = 0.0
        epoch_diff_loss = 0.0
        epoch_percep_loss = 0.0
        epoch_offset_loss = 0.0
        epoch_sc_loss = 0.0
        epoch_steps = 0

        for step, samples in enumerate(train_dataloader):
            model.train()
            content_images = samples["content_image"]
            style_images = samples["style_image"]
            target_images = samples["target_image"]
            nonorm_target_images = samples["nonorm_target_image"]

            with accelerator.accumulate(model):
                # Sample noise
                noise = torch.randn_like(target_images)
                bsz = target_images.shape[0]

                # Sample random timesteps
                timesteps = torch.randint(
                    0,
                    noise_scheduler.num_train_timesteps,
                    (bsz,),
                    device=target_images.device,
                ).long()

                # Add noise (forward diffusion)
                noisy_target_images = noise_scheduler.add_noise(
                    target_images, noise, timesteps
                )

                # Classifier-free guidance training strategy
                context_mask = torch.bernoulli(torch.zeros(bsz) + args.drop_prob)
                for i, mask_value in enumerate(context_mask):
                    if mask_value == 1:
                        content_images[i, :, :, :] = 1
                        style_images[i, :, :, :] = 1

                # Predict noise and offset
                noise_pred, offset_out_sum = model(
                    x_t=noisy_target_images,
                    timesteps=timesteps,
                    style_images=style_images,
                    content_images=content_images,
                    content_encoder_downsample_size=args.content_encoder_downsample_size,
                )

                # Calculate individual losses
                # ...existing code...

                # Calculate individual losses
                diff_loss = F.mse_loss(
                    noise_pred.float(), noise.float(), reduction="mean"
                )

                # Perceptual loss (content preservation)
                pred_original = x0_from_epsilon(
                    scheduler=noise_scheduler,
                    noise_pred=noise_pred,
                    x_t=noisy_target_images,
                    timesteps=timesteps,
                )
                norm_pred_ori = normalize_mean_std(pred_original)

                percep_loss = perceptual_loss.calculate_loss(
                    reNormalize_img(content_images), norm_pred_ori
                )

                # Offset loss (style consistency)
                offset_loss = torch.abs(offset_out_sum).mean()

                # Total loss (Phase 1)
                loss = (
                    diff_loss
                    + args.perceptual_coefficient * percep_loss
                    + args.offset_coefficient * offset_loss
                )

                # ‚úÖ Phase 2: Add Style-Content Contrastive (SC) loss
                sc_loss = torch.tensor(0.0).to(accelerator.device)
                if args.phase_2 and scr is not None:
                    neg_images = samples.get("neg_images", None)
                    if neg_images is not None:
                        # Compute SC loss
                        sc_loss = scr(
                            content_images=content_images,
                            style_images=style_images,
                            neg_images=neg_images,
                            noise_pred=noise_pred,
                            timesteps=timesteps,
                        )
                        loss = loss + args.sc_coefficient * sc_loss

                # Backward pass
                accelerator.backward(loss)

                # Gradient clipping
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            # Track losses
            epoch_train_loss += loss.item()
            epoch_diff_loss += diff_loss.item()
            epoch_percep_loss += percep_loss.item()
            epoch_offset_loss += offset_loss.item()
            if args.phase_2:
                epoch_sc_loss += sc_loss.item()
            epoch_steps += 1

            # Update progress bar
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1

                # Log training metrics
                logs = {
                    "train_loss": loss.item(),
                    "diff_loss": diff_loss.item(),
                    "percep_loss": percep_loss.item(),
                    "offset_loss": offset_loss.item(),
                    "lr": lr_scheduler.get_last_lr()[0],
                }
                if args.phase_2:
                    logs["sc_loss"] = sc_loss.item()

                progress_bar.set_postfix(**logs)
                accelerator.log(logs, step=global_step)

                # ‚úÖ Run validation
                if global_step % args.val_interval == 0:
                    avg_val_loss = validate(
                        model,
                        val_dataloader,
                        noise_scheduler,
                        accelerator,
                        args,
                        global_step,
                    )

                    accelerator.log({"val_loss": avg_val_loss}, step=global_step)

                    logging.info(
                        f"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}] "
                        f"Step {global_step}: val_loss={avg_val_loss:.4f}"
                    )

                    # Save best checkpoint
                    if avg_val_loss < best_val_loss:
                        best_val_loss = avg_val_loss
                        if accelerator.is_main_process:
                            save_checkpoint(
                                model, accelerator, args, global_step, is_best=True
                            )
                            print(
                                f"‚úì New best model saved! val_loss={avg_val_loss:.4f}"
                            )
                            logging.info(
                                f"Saved best checkpoint at step {global_step} "
                                f"with val_loss={avg_val_loss:.4f}"
                            )

                # ‚úÖ Save regular checkpoints
                if global_step % args.ckpt_interval == 0:
                    if accelerator.is_main_process:
                        save_checkpoint(model, accelerator, args, global_step)

            # Stop if max steps reached
            if global_step >= args.max_train_steps:
                break

        # Epoch summary
        avg_epoch_loss = epoch_train_loss / max(epoch_steps, 1)
        avg_diff_loss = epoch_diff_loss / max(epoch_steps, 1)
        avg_percep_loss = epoch_percep_loss / max(epoch_steps, 1)
        avg_offset_loss = epoch_offset_loss / max(epoch_steps, 1)

        log_msg = (
            f"Epoch {epoch + 1}/{num_train_epochs} completed - "
            f"avg_loss: {avg_epoch_loss:.4f}, "
            f"diff: {avg_diff_loss:.4f}, "
            f"percep: {avg_percep_loss:.4f}, "
            f"offset: {avg_offset_loss:.4f}"
        )

        if args.phase_2:
            avg_sc_loss = epoch_sc_loss / max(epoch_steps, 1)
            log_msg += f", sc: {avg_sc_loss:.4f}"

        logging.info(log_msg)
        print(f"\n{log_msg}")

        if global_step >= args.max_train_steps:
            break

    # ‚úÖ Save final checkpoint
    if accelerator.is_main_process:
        save_checkpoint(model, accelerator, args, global_step, is_best=False)
        print(f"\n‚úì Training complete! Final checkpoint saved at step {global_step}")
        logging.info(f"Training completed at step {global_step}")

    accelerator.end_training()


if __name__ == "__main__":
    main()
