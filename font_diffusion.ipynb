{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a46ef",
   "metadata": {
    "id": "BWFvN9XJxf9K",
    "outputId": "1ca6b669-956b-493b-e7ce-6402053d5585",
    "papermill": {
     "duration": 12.857369,
     "end_time": "2025-12-30T18:53:35.066181",
     "exception": false,
     "start_time": "2025-12-30T18:53:22.208812",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Environment Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if \"MPLBACKEND\" in os.environ:\n",
    "    del os.environ[\"MPLBACKEND\"]\n",
    "    print(\"MPLBACKEND environment variable cleared.\")\n",
    "\n",
    "# 2. Clone the repository\n",
    "!rm -rf FontDiffusion\n",
    "!git clone https://github.com/dzungphieuluuky/FontDiffusion.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd8666",
   "metadata": {
    "id": "sxdyquWfaqdm",
    "outputId": "f4738958-8ecc-4e48-bfda-9a798d92165f",
    "papermill": {
     "duration": 0.019157,
     "end_time": "2025-12-30T18:53:35.092303",
     "exception": false,
     "start_time": "2025-12-30T18:53:35.073146",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython import get_ipython\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def configure_environment_paths():\n",
    "    \"\"\"Detect environment and configure paths\"\"\"\n",
    "    try:\n",
    "        if \"google.colab\" in str(get_ipython()):\n",
    "            print(\"‚úÖ Environment: Google Colab\")\n",
    "            base_data_path = \"/content/\"\n",
    "            base_output_path = \"/content/\"\n",
    "            environment_name = \"colab\"\n",
    "        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
    "            print(\"‚úÖ Environment: Kaggle\")\n",
    "            base_data_path = \"/kaggle/input/\"\n",
    "            base_output_path = \"/kaggle/working/\"\n",
    "            environment_name = \"kaggle\"\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Environment: Local/Unknown\")\n",
    "            base_data_path = \"./data/\"\n",
    "            base_output_path = \"./output/\"\n",
    "            environment_name = \"local\"\n",
    "    except NameError:\n",
    "        print(\"‚ö†Ô∏è Non-interactive session. Using local paths.\")\n",
    "        base_data_path = \"./data/\"\n",
    "        base_output_path = \"./output/\"\n",
    "        environment_name = \"local\"\n",
    "    os.makedirs(base_output_path, exist_ok=True)\n",
    "    print(f\"üìÇ Data Path: {base_data_path}\")\n",
    "    print(f\"üì¶ Output Path: {base_output_path}\")\n",
    "    return base_data_path, base_output_path, environment_name\n",
    "\n",
    "\n",
    "def load_secret(key_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Loads a secret key from the appropriate environment (Colab, Kaggle, or local env vars).\n",
    "    Args:\n",
    "        key_name (str): The name of the secret key to load (e.g., \"WANDB_API_KEY\", \"HF_TOKEN\").\n",
    "    Returns:\n",
    "        Optional[str]: The secret key value if found, otherwise None.\n",
    "    \"\"\"\n",
    "    env = ENV_NAME\n",
    "    secret_value = None\n",
    "    print(f\"Attempting to load secret '{key_name}' from '{env}' environment...\")\n",
    "    try:\n",
    "        if env == \"colab\":\n",
    "            from google.colab import userdata\n",
    "\n",
    "            secret_value = userdata.get(key_name)\n",
    "        elif env == \"kaggle\":\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "            user_secrets = UserSecretsClient()\n",
    "            secret_value = user_secrets.get_secret(key_name)\n",
    "        else:  # Local environment\n",
    "            secret_value = os.getenv(key_name)\n",
    "        if not secret_value:\n",
    "            print(f\"‚ö†Ô∏è Secret '{key_name}' not found in the {env} environment.\")\n",
    "            return None\n",
    "        print(f\"‚úÖ Successfully loaded secret '{key_name}'.\")\n",
    "        return secret_value\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred while loading secret '{key_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "INPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b4150",
   "metadata": {
    "id": "ET_mqyek9bwj",
    "outputId": "aae81910-51d7-4409-b8d0-f862b1ea1fe7",
    "papermill": {
     "duration": 61.239828,
     "end_time": "2025-12-30T18:54:36.338205",
     "exception": false,
     "start_time": "2025-12-30T18:53:35.098377",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!uv pip install --upgrade pip\n",
    "!uv pip install -r FontDiffusion/my_requirements.txt\n",
    "# 3. Install PyTorch 1.13\n",
    "%cd {OUTPUT_PATH}\n",
    "# Force reinstall torch 1.13 to match the model's training environment\n",
    "# !uv pip uninstall torch torchvision\n",
    "# !uv pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "!uv pip install torch torchvision\n",
    "# 4. Install other dependencies\n",
    "print(\"\\n‚¨áÔ∏è Installing Dependencies (Manually fixed)...\")\n",
    "# Install xformers compatible with Torch 1.13\n",
    "!uv pip install xformers==0.0.16 -q\n",
    "\n",
    "# Install original dependencies\n",
    "!uv pip install transformers==4.33.1 accelerate==0.23.0 diffusers==0.22.0\n",
    "!uv pip install gradio==4.8.0 pyyaml pygame opencv-python info-nce-pytorch kornia\n",
    "# -----------------------------------------------------------------\n",
    "!uv pip install lpips scikit-image pytorch-fid\n",
    "!sudo apt-get update && sudo apt-get install dos2unix\n",
    "!uv pip install gdown\n",
    "!uv pip install wandb\n",
    "print(\"\\n‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd517dfe",
   "metadata": {
    "id": "9PsLgUs0cYmO",
    "outputId": "77e74ba9-f348-4ffb-e675-0717fd7e74a7",
    "papermill": {
     "duration": 12.524295,
     "end_time": "2025-12-30T18:54:48.878013",
     "exception": false,
     "start_time": "2025-12-30T18:54:36.353718",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# KAGGLE CELL #1: Download checkpoint\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(OUTPUT_PATH)\n",
    "# Download from Hub\n",
    "if not os.path.exists(\"ckpt\") or not list(Path(\"ckpt\").glob(\"*.safetensors\")):\n",
    "    print(\"üì• Downloading checkpoint from Hugging Face Hub...\\n\")\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=\"dzungpham/font-diffusion-weights\",\n",
    "        local_dir=\"ckpt\",\n",
    "        allow_patterns=\"*.safetensors\",\n",
    "        force_download=False,\n",
    "    )\n",
    "    print(\"\\n‚úÖ Download complete!\")\n",
    "else:\n",
    "    print(\"‚úÖ Checkpoint already downloaded\")\n",
    "# Verify\n",
    "print(\"\\nüìÇ Files in ckpt/:\")\n",
    "for file in os.listdir(\"ckpt\"):\n",
    "    if file.endswith(\".safetensors\"):\n",
    "        size = os.path.getsize(f\"ckpt/{file}\") / (1024**2)\n",
    "        print(f\"  ‚úì {file} ({size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e8ea2",
   "metadata": {
    "id": "ecfc18e0",
    "outputId": "7b925027-747c-4cb1-977e-2bee34d1865f",
    "papermill": {
     "duration": 0.023805,
     "end_time": "2025-12-30T18:54:48.917163",
     "exception": false,
     "start_time": "2025-12-30T18:54:48.893358",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Unzipping all archived files\n",
    "import os\n",
    "import glob\n",
    "from zipfile import ZipFile\n",
    "\n",
    "zip_file_paths = glob.glob(os.path.join(INPUT_PATH, \"*.zip\"))\n",
    "\n",
    "if not zip_file_paths:\n",
    "    print(f\"No .zip files found in {INPUT_PATH}.\")\n",
    "else:\n",
    "    for zip_file_path in zip_file_paths:\n",
    "        if os.path.exists(zip_file_path):\n",
    "            print(f\"Unzipping {zip_file_path}...\")\n",
    "            !unzip -o {zip_file_path} -d ./\n",
    "            print(f\"Unzipping of {zip_file_path} complete.\")\n",
    "        else:\n",
    "            print(f\"Error: The file {zip_file_path} was not found (post-glob check).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51941368",
   "metadata": {
    "id": "Mx5uS5WQaqdn",
    "outputId": "951a03aa-416d-4e8d-ca55-121d410bb302",
    "papermill": {
     "duration": 1.62157,
     "end_time": "2025-12-30T18:54:50.594793",
     "exception": false,
     "start_time": "2025-12-30T18:54:48.973223",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def convert_csv_to_chars_txt(\n",
    "    input_csv_path: str, output_txt_path: str, column_name: str = \"word\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, extracts text from a specified column, and writes each character\n",
    "    to a new line in a plain text file.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): The full path to the input CSV file.\n",
    "        output_txt_path (str): The full path for the output text file.\n",
    "        column_name (str): The name of the column in the CSV file containing the text.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_csv_path):\n",
    "        print(\n",
    "            f\"Error: Input CSV file not found at '{input_csv_path}'. Please ensure the file is uploaded.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file '{input_csv_path}': {e}\")\n",
    "        return\n",
    "\n",
    "    if column_name not in df.columns:\n",
    "        print(\n",
    "            f\"Error: Column '{column_name}' not found in the CSV file '{input_csv_path}'.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    all_characters = []\n",
    "    # Ensure the column values are treated as strings before iterating over them\n",
    "    for item in df[column_name].astype(str).dropna().tolist():\n",
    "        for char in item:\n",
    "            all_characters.append(char)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)\n",
    "\n",
    "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(all_characters))\n",
    "    print(\n",
    "        f\"Successfully converted '{input_csv_path}' to '{output_txt_path}', with one character per line.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Example Usage (demonstration with a dummy file) ---\n",
    "# As the original file 'Ds_300_ChuNom_TuTao.csv' was not found in the previous execution,\n",
    "# let's create a dummy file to demonstrate the function's usage.\n",
    "print(\"\\n--- Demonstrating function with a dummy CSV file ---\")\n",
    "dummy_csv_path = os.path.join(OUTPUT_PATH, \"dummy_data.csv\")\n",
    "dummy_output_txt_path = os.path.join(OUTPUT_PATH, \"dummy_chars.txt\")\n",
    "\n",
    "# Create a dummy CSV file\n",
    "dummy_data = {\"word\": [\"hello\", \"world\", \"python\"]}\n",
    "pd.DataFrame(dummy_data).to_csv(dummy_csv_path, index=False)\n",
    "print(f\"Created a dummy CSV file at: {dummy_csv_path}\")\n",
    "\n",
    "convert_csv_to_chars_txt(dummy_csv_path, dummy_output_txt_path)\n",
    "\n",
    "# --- How to use with your actual file ---\n",
    "# Uncomment the lines below and replace 'your_actual_file.csv' and 'your_output.txt'\n",
    "# with the correct paths for your use case.\n",
    "#\n",
    "# original_csv_file = os.path.join(INPUT_PATH, \"Ds_300_ChuNom_TuTao.csv\") # Or the full path to your CSV\n",
    "# original_output_txt = os.path.join(OUTPUT_PATH, \"nom_tu_tao.txt\") # Or your desired output path\n",
    "# convert_csv_to_chars_txt(original_csv_file, original_output_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4cf20b",
   "metadata": {
    "id": "Sxz63qgifNlV",
    "outputId": "c86d1ccf-dcc8-44ec-d61f-9035dd9d0369",
    "papermill": {
     "duration": 0.140282,
     "end_time": "2025-12-30T18:54:50.749810",
     "exception": false,
     "start_time": "2025-12-30T18:54:50.609528",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls -larth {OUTPUT_PATH}/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cff682",
   "metadata": {
    "id": "MvEJIiH5fNlV",
    "outputId": "95a0a309-3e6b-482b-c971-cf93efac61bf",
    "papermill": {
     "duration": 0.104394,
     "end_time": "2025-12-30T18:54:50.869230",
     "exception": false,
     "start_time": "2025-12-30T18:54:50.764836",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd {OUTPUT_PATH}\n",
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = load_secret(\"HF_TOKEN\")\n",
    "login(HF_TOKEN)\n",
    "HF_USERNAME = \"dzungpham\"\n",
    "\n",
    "# ==========================================\n",
    "# EXPORT / DOWNLOAD DATASET COMMANDS\n",
    "# ==========================================\n",
    "\n",
    "# Train Split\n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/train\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"train\" \\\n",
    "  --token HF_TOKEN \n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/train_original\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"train_original\" \\\n",
    "  --token HF_TOKEN \n",
    "# Validation: Unseen Both\n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/val_unseen_both\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"val_unseen_both\" \\\n",
    "  --token HF_TOKEN \n",
    "# Validation: Seen Style, Unseen Char\n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/val_seen_style_unseen_char\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"val_seen_style_unseen_char\" \\\n",
    "  --token HF_TOKEN \n",
    "# Validation: Unseen Style, Seen Char\n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/val_unseen_style_seen_char\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"val_unseen_style_seen_char\" \\\n",
    "  --token HF_TOKEN \n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/test\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"test\" \\\n",
    "  --token HF_TOKEN \n",
    "print(\"SUCCESSFULLY EXPORT HF DATASET TO LOCAL DIRECTORY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1fcbec-67c3-4f6c-91bb-67baef5aef94",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!uv pip install --upgrade \"huggingface-hub>=0.15.1,<1.0\" accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29deed1d",
   "metadata": {
    "id": "gma02BZvhx8I",
    "outputId": "a8a54761-e26c-488d-d87d-cd14d686e77f",
    "papermill": {
     "duration": 10.53661,
     "end_time": "2025-12-30T18:55:01.421093",
     "exception": false,
     "start_time": "2025-12-30T18:54:50.884483",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# already change sample_batch file to save all data in train_original\n",
    "%cd {OUTPUT_PATH}\n",
    "!python FontDiffusion/sample_batch.py \\\n",
    "    --characters \"FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\" \\\n",
    "    --style_images \"FontDiffusion/styles_images\" \\\n",
    "    --ckpt_dir \"ckpt/\" \\\n",
    "    --ttf_path \"FontDiffusion/fonts\" \\\n",
    "    --output_dir \"my_dataset/train_original\" \\\n",
    "    --resume_from \"my_dataset/train_original/results_checkpoint.json\" \\\n",
    "    --num_inference_steps 20 \\\n",
    "    --guidance_scale 7.5 \\\n",
    "    --start_line 1 \\\n",
    "    --end_line 50 \\\n",
    "    --batch_size 24 \\\n",
    "    --save_interval 1 \\\n",
    "    --channels_last \\\n",
    "    --seed 42 \\\n",
    "    --compile \\\n",
    "    --enable_xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9250a14",
   "metadata": {
    "id": "XoppW2x5fNlW",
    "outputId": "759bca6c-0025-454a-b3b6-ae8170f7edb4",
    "papermill": {
     "duration": 0.236541,
     "end_time": "2025-12-30T18:55:01.673705",
     "exception": false,
     "start_time": "2025-12-30T18:55:01.437164",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python FontDiffusion/create_validation_split.py \\\n",
    "  --data_root my_dataset \\\n",
    "  --val_ratio 0.2 \\\n",
    "  --test_ratio 0.1 \\\n",
    "  --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a9392",
   "metadata": {
    "id": "v-a7paEbfNlW",
    "outputId": "c792e189-6d31-4114-be92-644d4372efa7",
    "papermill": {
     "duration": 21.070659,
     "end_time": "2025-12-30T18:55:22.760587",
     "exception": false,
     "start_time": "2025-12-30T18:55:01.689928",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- RAW DATA (Before Splitting) ---\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/train_original\" \\\n",
    "  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n",
    "  --split \"train_original\" \\\n",
    "  --private \\\n",
    "  --token \"{HF_TOKEN}\"\n",
    "\n",
    "# --- ORGANIZED SPLITS (After Splitting) ---\n",
    "\n",
    "# Train Split\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/train\" \\\n",
    "  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n",
    "  --split \"train\" \\\n",
    "  --private \\\n",
    "  --token \"{HF_TOKEN}\"\n",
    "\n",
    "# Test Split\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/test\" \\\n",
    "  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n",
    "  --split \"test\" \\\n",
    "  --private \\\n",
    "  --token \"{HF_TOKEN}\"\n",
    "\n",
    "# Validation: Unseen Both\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/val_unseen_both\" \\\n",
    "  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n",
    "  --split \"val_unseen_both\" \\\n",
    "  --private \\\n",
    "  --token \"{HF_TOKEN}\"\n",
    "\n",
    "# Validation: Seen Style, Unseen Char\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/val_seen_style_unseen_char\" \\\n",
    "  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n",
    "  --split \"val_seen_style_unseen_char\" \\\n",
    "  --private \\\n",
    "  --token \"{HF_TOKEN}\"\n",
    "\n",
    "# Validation: Unseen Style, Seen Char\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/val_unseen_style_seen_char\" \\\n",
    "  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n",
    "  --split \"val_unseen_style_seen_char\" \\\n",
    "  --private \\\n",
    "  --token \"{HF_TOKEN}\"\n",
    "print(\"SUCCESSFULLY UPLOAD LOCAL MY_DATASET TO HUGGINGFACE DATASETS SPACE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87caab2",
   "metadata": {
    "id": "zXSQHJ3xVOSc",
    "outputId": "4d89a176-c286-442f-eb6e-01b39c994e64",
    "papermill": {
     "duration": 1.992585,
     "end_time": "2025-12-30T18:55:24.769269",
     "exception": false,
     "start_time": "2025-12-30T18:55:22.776684",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267634e8",
   "metadata": {
    "id": "FxxJ9qy4KIZH",
    "outputId": "f554de7b-f455-4a5b-f8e5-4b6cba3f756b",
    "papermill": {
     "duration": 0.021927,
     "end_time": "2025-12-30T18:55:24.807644",
     "exception": false,
     "start_time": "2025-12-30T18:55:24.785717",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Load your Weights & Biases API key from a secure store\n",
    "wandb_key = load_secret(\"WANDB_API_KEY\")\n",
    "wandb.login(key=wandb_key)\n",
    "\n",
    "# Run the training script with the corrected flag syntax\n",
    "!python FontDiffusion/my_train.py \\\n",
    "    --seed=123 \\\n",
    "    --experience_name=FontDiffuser_training_phase_1 \\\n",
    "    --data_root=my_dataset \\\n",
    "    --output_dir=outputs/FontDiffuser \\\n",
    "    --report_to=wandb \\\n",
    "    --resolution=96 \\\n",
    "    --style_image_size=96 \\\n",
    "    --content_image_size=96 \\\n",
    "    --content_encoder_downsample_size=3 \\\n",
    "    --channel_attn=True \\\n",
    "    --content_start_channel=64 \\\n",
    "    --style_start_channel=64 \\\n",
    "    --train_batch_size=8 \\\n",
    "    --perceptual_coefficient=0.03 \\\n",
    "    --offset_coefficient=0.7 \\\n",
    "    --max_train_steps=2000 \\\n",
    "    --ckpt_interval=1000 \\\n",
    "    --gradient_accumulation_steps=1 \\\n",
    "    --log_interval=50 \\\n",
    "    --learning_rate=1e-4 \\\n",
    "    --lr_scheduler=linear \\\n",
    "    --lr_warmup_steps=10000 \\\n",
    "    --drop_prob=0.1 \\\n",
    "    --mixed_precision=no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a3326-6bfa-4fba-bbcc-e2e7be996c7f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls -lr outputs/FontDiffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8136e",
   "metadata": {
    "id": "J4bplsS6pQna",
    "papermill": {
     "duration": 0.022471,
     "end_time": "2025-12-30T18:55:24.845778",
     "exception": false,
     "start_time": "2025-12-30T18:55:24.823307",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training phase 2\n",
    "!wandb login\n",
    "!python FontDiffusion/my_train.py \\\n",
    "    --seed=123 \\\n",
    "    --experience_name=\"FontDiffuser_training_phase_2\" \\\n",
    "    --data_root=\"my_dataset\" \\\n",
    "    --output_dir=\"outputs/FontDiffuser\" \\\n",
    "    --report_to=\"wandb\" \\\n",
    "    --phase_2 \\\n",
    "    --phase_1_ckpt_dir=\"outputs/FontDiffuser/global_step_2000\" \\\n",
    "    --scr_ckpt_path=\"ckpt/scr_210000.pth\" \\\n",
    "    --sc_coefficient=0.05 \\\n",
    "    --num_neg=13 \\\n",
    "    --resolution=96 \\\n",
    "    --style_image_size=96 \\\n",
    "    --content_image_size=96 \\\n",
    "    --content_encoder_downsample_size=3 \\\n",
    "    --channel_attn=True \\\n",
    "    --content_start_channel=64 \\\n",
    "    --style_start_channel=64 \\\n",
    "    --train_batch_size=8 \\\n",
    "    --perceptual_coefficient=0.03 \\\n",
    "    --offset_coefficient=0.4 \\\n",
    "    --max_train_steps=100 \\\n",
    "    --ckpt_interval=50 \\\n",
    "    --gradient_accumulation_steps=2 \\\n",
    "    --log_interval=50 \\\n",
    "    --learning_rate=1e-5 \\\n",
    "    --lr_scheduler=\"constant\" \\\n",
    "    --lr_warmup_steps=1000 \\\n",
    "    --drop_prob=0.1 \\\n",
    "    --mixed_precision=\"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c45e2f",
   "metadata": {
    "id": "nF8opWokKcMS",
    "outputId": "2838655d-5d24-4808-813a-39f20ec239a8",
    "papermill": {
     "duration": 0.217876,
     "end_time": "2025-12-30T18:55:25.079820",
     "exception": false,
     "start_time": "2025-12-30T18:55:24.861944",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python /content/FontDiffusion/pth2safetensores.py \\\n",
    "    --weights_dir \"ckpt\" \\\n",
    "    --repo_id \"dzungpham/font-diffusion-weights\" \\\n",
    "    --token \"{HF_TOKEN}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868b20b",
   "metadata": {
    "id": "kTz9WZ9ylBZx",
    "outputId": "ccb61bf4-7551-4269-aee7-b0742fe1517a",
    "papermill": {
     "duration": 0.031197,
     "end_time": "2025-12-30T18:55:25.126961",
     "exception": false,
     "start_time": "2025-12-30T18:55:25.095764",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def find_result_folders(base_path: Path, pattern_name: str) -> List[Path]:\n",
    "    return [p for p in base_path.glob(pattern_name) if p.is_dir()]\n",
    "\n",
    "\n",
    "def zip_folder(folder_path: Path, output_base_path: Path) -> bool:\n",
    "    folder_name = folder_path.name\n",
    "    zip_path = output_base_path / f\"{folder_name}.zip\"\n",
    "    try:\n",
    "        print(f\"   -> Zipping folder: {folder_name}...\")\n",
    "        with zipfile.ZipFile(\n",
    "            zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED\n",
    "        ) as zipf:\n",
    "            for file_path in folder_path.rglob(\"*\"):\n",
    "                if file_path.is_file():\n",
    "                    arcname = file_path.relative_to(folder_path.parent)\n",
    "                    zipf.write(file_path, arcname)\n",
    "        print(f\"   ‚úÖ Created ZIP: {zip_path.name}\")\n",
    "        return True\n",
    "    except Exception as exc:\n",
    "        print(f\"   ‚ùå Failed to zip {folder_name}: {exc}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def zip_stats_results_folders(output_base_path: str, pattern_name: str) -> None:\n",
    "    base = Path(output_base_path)\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "    result_folders = find_result_folders(base, pattern_name)\n",
    "    if not result_folders:\n",
    "        print(f\"‚ö†Ô∏è No folders matching '*dataset' found in '{output_base_path}'.\")\n",
    "        return\n",
    "    print(f\"üîç Found {len(result_folders)} result folder(s) to zip.\")\n",
    "    successful = sum(1 for folder in result_folders if zip_folder(folder, base))\n",
    "    print(\n",
    "        f\"\\n‚úÖ DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n",
    "        if not output_root:\n",
    "            raise ValueError(\"OUTPUT_PATH not defined\")\n",
    "        zip_stats_results_folders(\n",
    "            output_base_path=OUTPUT_PATH, pattern_name=\"outputs/FontDiffuser/global*\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 127.636509,
   "end_time": "2025-12-30T18:55:25.961447",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-30T18:53:18.324938",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}