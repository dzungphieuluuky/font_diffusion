{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95a46ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T10:02:41.465562Z",
     "iopub.status.busy": "2025-12-31T10:02:41.465196Z",
     "iopub.status.idle": "2025-12-31T10:02:54.286944Z",
     "shell.execute_reply": "2025-12-31T10:02:54.286242Z",
     "shell.execute_reply.started": "2025-12-31T10:02:41.465501Z"
    },
    "id": "BWFvN9XJxf9K",
    "outputId": "1ca6b669-956b-493b-e7ce-6402053d5585",
    "papermill": {
     "duration": 12.857369,
     "end_time": "2025-12-30T18:53:35.066181",
     "exception": false,
     "start_time": "2025-12-30T18:53:22.208812",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPLBACKEND environment variable cleared.\n",
      "Cloning into 'FontDiffusion'...\n",
      "remote: Enumerating objects: 20213, done.\u001b[K\n",
      "remote: Counting objects: 100% (5025/5025), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4973/4973), done.\u001b[K\n",
      "remote: Total 20213 (delta 112), reused 4915 (delta 50), pack-reused 15188 (from 3)\u001b[K\n",
      "Receiving objects: 100% (20213/20213), 277.36 MiB | 33.92 MiB/s, done.\n",
      "Resolving deltas: 100% (675/675), done.\n",
      "Updating files: 100% (137/137), done.\n"
     ]
    }
   ],
   "source": [
    "# @title Environment Setup\n",
    "import os\n",
    "import sys\n",
    "if 'MPLBACKEND' in os.environ:\n",
    "    del os.environ['MPLBACKEND']\n",
    "    print(\"MPLBACKEND environment variable cleared.\")\n",
    "\n",
    "# 2. Clone the repository\n",
    "!rm -rf FontDiffusion\n",
    "!git clone https://github.com/dzungphieuluuky/FontDiffusion.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cdd8666",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T10:02:54.289059Z",
     "iopub.status.busy": "2025-12-31T10:02:54.288741Z",
     "iopub.status.idle": "2025-12-31T10:02:54.298521Z",
     "shell.execute_reply": "2025-12-31T10:02:54.297757Z",
     "shell.execute_reply.started": "2025-12-31T10:02:54.289035Z"
    },
    "id": "sxdyquWfaqdm",
    "outputId": "f4738958-8ecc-4e48-bfda-9a798d92165f",
    "papermill": {
     "duration": 0.019157,
     "end_time": "2025-12-30T18:53:35.092303",
     "exception": false,
     "start_time": "2025-12-30T18:53:35.073146",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment: Kaggle\n",
      "üìÇ Data Path: /kaggle/input/\n",
      "üì¶ Output Path: /kaggle/working/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython import get_ipython\n",
    "from typing import Optional\n",
    "\n",
    "def configure_environment_paths():\n",
    "    \"\"\"Detect environment and configure paths\"\"\"\n",
    "    try:\n",
    "        if \"google.colab\" in str(get_ipython()):\n",
    "            print(\"‚úÖ Environment: Google Colab\")\n",
    "            base_data_path = \"/content/\"\n",
    "            base_output_path = \"/content/\"\n",
    "            environment_name = \"colab\"\n",
    "        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
    "            print(\"‚úÖ Environment: Kaggle\")\n",
    "            base_data_path = \"/kaggle/input/\"\n",
    "            base_output_path = \"/kaggle/working/\"\n",
    "            environment_name = \"kaggle\"\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Environment: Local/Unknown\")\n",
    "            base_data_path = \"./data/\"\n",
    "            base_output_path = \"./output/\"\n",
    "            environment_name = \"local\"\n",
    "    except NameError:\n",
    "        print(\"‚ö†Ô∏è Non-interactive session. Using local paths.\")\n",
    "        base_data_path = \"./data/\"\n",
    "        base_output_path = \"./output/\"\n",
    "        environment_name = \"local\"\n",
    "    os.makedirs(base_output_path, exist_ok=True)\n",
    "    print(f\"üìÇ Data Path: {base_data_path}\")\n",
    "    print(f\"üì¶ Output Path: {base_output_path}\")\n",
    "    return base_data_path, base_output_path, environment_name\n",
    "def load_secret(key_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Loads a secret key from the appropriate environment (Colab, Kaggle, or local env vars).\n",
    "    Args:\n",
    "        key_name (str): The name of the secret key to load (e.g., \"WANDB_API_KEY\", \"HF_TOKEN\").\n",
    "    Returns:\n",
    "        Optional[str]: The secret key value if found, otherwise None.\n",
    "    \"\"\"\n",
    "    env = ENV_NAME\n",
    "    secret_value = None\n",
    "    print(f\"Attempting to load secret '{key_name}' from '{env}' environment...\")\n",
    "    try:\n",
    "        if env == \"colab\":\n",
    "            from google.colab import userdata\n",
    "            secret_value = userdata.get(key_name)\n",
    "        elif env == \"kaggle\":\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            user_secrets = UserSecretsClient()\n",
    "            secret_value = user_secrets.get_secret(key_name)\n",
    "        else: # Local environment\n",
    "            secret_value = os.getenv(key_name)\n",
    "        if not secret_value:\n",
    "            print(f\"‚ö†Ô∏è Secret '{key_name}' not found in the {env} environment.\")\n",
    "            return None\n",
    "        print(f\"‚úÖ Successfully loaded secret '{key_name}'.\")\n",
    "        return secret_value\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred while loading secret '{key_name}': {e}\")\n",
    "        return None\n",
    "INPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a73b4150",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T10:02:54.299693Z",
     "iopub.status.busy": "2025-12-31T10:02:54.299482Z",
     "iopub.status.idle": "2025-12-31T10:03:57.668970Z",
     "shell.execute_reply": "2025-12-31T10:03:57.668133Z",
     "shell.execute_reply.started": "2025-12-31T10:02:54.299671Z"
    },
    "id": "ET_mqyek9bwj",
    "outputId": "aae81910-51d7-4409-b8d0-f862b1ea1fe7",
    "papermill": {
     "duration": 61.239828,
     "end_time": "2025-12-30T18:54:36.338205",
     "exception": false,
     "start_time": "2025-12-30T18:53:35.098377",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 159ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 106ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 153ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpip\u001b[0m\u001b[2m==24.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpip\u001b[0m\u001b[2m==25.3\u001b[0m\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
      "\u001b[2K  \u001b[31m√ó\u001b[0m No solution found when resolving dependencies:                                  \u001b[0m\n",
      "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mBecause you require importlib-metadata==4.6.4 and\n",
      "\u001b[31m      \u001b[0mimportlib-metadata==8.0.0, we can conclude that your requirements are\n",
      "\u001b[31m      \u001b[0munsatisfiable.\n",
      "/kaggle/working\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m38 packages\u001b[0m \u001b[2min 104ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m10 packages\u001b[0m \u001b[2min 38ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m10 packages\u001b[0m \u001b[2min 18.43s\u001b[0m\u001b[0m.0.70                        \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.5.3.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.3.61\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.6.82\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.3.83\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      "\n",
      "‚¨áÔ∏è Installing Dependencies (Manually fixed)...\n",
      "  \u001b[31m√ó\u001b[0m Failed to build `xformers==0.0.16`\n",
      "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mThe build backend returned an error\n",
      "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mCall to `setuptools.build_meta:__legacy__.build_wheel` failed (exit\n",
      "\u001b[31m      \u001b[0mstatus: 1)\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
      "\u001b[31m      \u001b[0mError in sitecustomize; set PYTHONVERBOSE for traceback:\n",
      "\u001b[31m      \u001b[0mModuleNotFoundError: No module named 'wrapt'\n",
      "\u001b[31m      \u001b[0mTraceback (most recent call last):\n",
      "\u001b[31m      \u001b[0m  File \"<string>\", line 14, in <module>\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmp8b7xPt/lib/python3.11/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 331, in get_requires_for_build_wheel\n",
      "\u001b[31m      \u001b[0m    return self._get_build_requires(config_settings, requirements=[])\n",
      "\u001b[31m      \u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmp8b7xPt/lib/python3.11/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 301, in _get_build_requires\n",
      "\u001b[31m      \u001b[0m    self.run_setup()\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmp8b7xPt/lib/python3.11/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 512, in run_setup\n",
      "\u001b[31m      \u001b[0m    super().run_setup(setup_script=setup_script)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmp8b7xPt/lib/python3.11/site-packages/setuptools/build_meta.py\",\n",
      "\u001b[31m      \u001b[0mline 317, in run_setup\n",
      "\u001b[31m      \u001b[0m    exec(code, locals())\n",
      "\u001b[31m      \u001b[0m  File \"<string>\", line 23, in <module>\n",
      "\u001b[31m      \u001b[0mModuleNotFoundError: No module named 'torch'\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This error likely indicates that `\u001b[36mxformers@0.0.16\u001b[39m` depends\n",
      "\u001b[31m      \u001b[0mon `\u001b[36mtorch\u001b[39m`, but doesn't declare it as a build dependency. If\n",
      "\u001b[31m      \u001b[0m`\u001b[36mxformers\u001b[39m` is a first-party package, consider adding `\u001b[36mtorch\u001b[39m` to its\n",
      "\u001b[31m      \u001b[0m`\u001b[32mbuild-system.requires\u001b[39m`. Otherwise, `\u001b[32muv pip install torch\u001b[39m` into the\n",
      "\u001b[31m      \u001b[0menvironment and re-run with `\u001b[32m--no-build-isolation\u001b[39m`.\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m56 packages\u001b[0m \u001b[2min 130ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m4 packages\u001b[0m \u001b[2min 493ms\u001b[0m\u001b[0m                                             \n",
      "\u001b[2mUninstalled \u001b[1m4 packages\u001b[0m \u001b[2min 529ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 43ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==0.23.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mdiffusers\u001b[0m\u001b[2m==0.34.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiffusers\u001b[0m\u001b[2m==0.22.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.13.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.53.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.33.1\u001b[0m\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m100 packages\u001b[0m \u001b[2min 145ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m8 packages\u001b[0m \u001b[2min 1.17s\u001b[0m\u001b[0m                                             \n",
      "\u001b[2mUninstalled \u001b[1m7 packages\u001b[0m \u001b[2min 358ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m8 packages\u001b[0m \u001b[2min 79ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mgradio\u001b[0m\u001b[2m==5.38.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgradio\u001b[0m\u001b[2m==4.8.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mgradio-client\u001b[0m\u001b[2m==1.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgradio-client\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1minfo-nce-pytorch\u001b[0m\u001b[2m==0.1.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==2.1.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mopencv-python\u001b[0m\u001b[2m==4.12.0.88\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopencv-python\u001b[0m\u001b[2m==4.11.0.86\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==10.4.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtomlkit\u001b[0m\u001b[2m==0.13.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtomlkit\u001b[0m\u001b[2m==0.12.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==11.0.3\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe package `typer==0.16.0` does not have an extra named `all`\u001b[0m\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m47 packages\u001b[0m \u001b[2min 58ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 28ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlpips\u001b[0m\u001b[2m==0.1.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytorch-fid\u001b[0m\u001b[2m==0.3.0\u001b[0m\n",
      "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]           \n",
      "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n",
      "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.6 kB]\n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,225 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,860 kB]\n",
      "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease   \n",
      "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,573 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]     \n",
      "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
      "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,287 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,633 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,966 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
      "Fetched 38.5 MB in 3s (12.0 MB/s)                            \n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  dos2unix\n",
      "0 upgraded, 1 newly installed, 0 to remove and 192 not upgraded.\n",
      "Need to get 384 kB of archives.\n",
      "After this operation, 1,367 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dos2unix amd64 7.4.2-2 [384 kB]\n",
      "Fetched 384 kB in 1s (500 kB/s)   \n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Selecting previously unselected package dos2unix.\n",
      "(Reading database ... 128639 files and directories currently installed.)\n",
      "Preparing to unpack .../dos2unix_7.4.2-2_amd64.deb ...\n",
      "Unpacking dos2unix (7.4.2-2) ...\n",
      "Setting up dos2unix (7.4.2-2) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 114ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 113ms\u001b[0m\u001b[0m\n",
      "\n",
      "‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\n"
     ]
    }
   ],
   "source": [
    "!uv pip install --upgrade pip\n",
    "!uv pip install -r FontDiffusion/my_requirements.txt\n",
    "# 3. Install PyTorch 1.13\n",
    "%cd {OUTPUT_PATH}\n",
    "# Force reinstall torch 1.13 to match the model's training environment\n",
    "# !uv pip uninstall torch torchvision\n",
    "# !uv pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "!uv pip install torch torchvision\n",
    "# 4. Install other dependencies\n",
    "print(\"\\n‚¨áÔ∏è Installing Dependencies (Manually fixed)...\")\n",
    "# Install xformers compatible with Torch 1.13\n",
    "!uv pip install xformers==0.0.16 -q\n",
    "\n",
    "# Install original dependencies\n",
    "!uv pip install transformers==4.33.1 accelerate==0.23.0 diffusers==0.22.0\n",
    "!uv pip install gradio==4.8.0 pyyaml pygame opencv-python info-nce-pytorch kornia\n",
    "# -----------------------------------------------------------------\n",
    "!uv pip install lpips scikit-image pytorch-fid\n",
    "!sudo apt-get update && sudo apt-get install dos2unix\n",
    "!uv pip install gdown\n",
    "!uv pip install wandb\n",
    "print(\"\\n‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd517dfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T10:03:57.670414Z",
     "iopub.status.busy": "2025-12-31T10:03:57.670162Z",
     "iopub.status.idle": "2025-12-31T10:04:00.743922Z",
     "shell.execute_reply": "2025-12-31T10:04:00.742771Z",
     "shell.execute_reply.started": "2025-12-31T10:03:57.670384Z"
    },
    "id": "9PsLgUs0cYmO",
    "outputId": "77e74ba9-f348-4ffb-e675-0717fd7e74a7",
    "papermill": {
     "duration": 12.524295,
     "end_time": "2025-12-30T18:54:48.878013",
     "exception": false,
     "start_time": "2025-12-30T18:54:36.353718",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading checkpoint from Hugging Face Hub...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2481c20557814fd58c9e0cc56154b285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f952d6e8527a4cb88082c69d6935d451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "content_encoder.safetensors:   0%|          | 0.00/4.76M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355933a65ea640039963e71ba687c9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "style_encoder.safetensors:   0%|          | 0.00/82.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44125dfddc2e417394469a01e3390ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet.safetensors:   0%|          | 0.00/315M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Download complete!\n",
      "\n",
      "üìÇ Files in ckpt/:\n",
      "  ‚úì unet.safetensors (300.34 MB)\n",
      "  ‚úì content_encoder.safetensors (4.54 MB)\n",
      "  ‚úì style_encoder.safetensors (78.58 MB)\n"
     ]
    }
   ],
   "source": [
    "# KAGGLE CELL #1: Download checkpoint\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "os.chdir(OUTPUT_PATH)\n",
    "# Download from Hub\n",
    "if not os.path.exists(\"ckpt\") or not list(Path(\"ckpt\").glob(\"*.safetensors\")):\n",
    "    print(\"üì• Downloading checkpoint from Hugging Face Hub...\\n\")\n",
    "    from huggingface_hub import snapshot_download\n",
    "    snapshot_download(\n",
    "        repo_id=\"dzungpham/font-diffusion-weights\",\n",
    "        local_dir=\"ckpt\",\n",
    "        allow_patterns=\"*.safetensors\",\n",
    "        force_download=False\n",
    "    )\n",
    "    print(\"\\n‚úÖ Download complete!\")\n",
    "else:\n",
    "    print(\"‚úÖ Checkpoint already downloaded\")\n",
    "# Verify\n",
    "print(\"\\nüìÇ Files in ckpt/:\")\n",
    "for file in os.listdir(\"ckpt\"):\n",
    "    if file.endswith(\".safetensors\"):\n",
    "        size = os.path.getsize(f\"ckpt/{file}\") / (1024**2)\n",
    "        print(f\"  ‚úì {file} ({size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767e8ea2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T10:04:00.746986Z",
     "iopub.status.busy": "2025-12-31T10:04:00.746375Z",
     "iopub.status.idle": "2025-12-31T10:04:00.758229Z",
     "shell.execute_reply": "2025-12-31T10:04:00.757350Z",
     "shell.execute_reply.started": "2025-12-31T10:04:00.746952Z"
    },
    "id": "ecfc18e0",
    "outputId": "7b925027-747c-4cb1-977e-2bee34d1865f",
    "papermill": {
     "duration": 0.023805,
     "end_time": "2025-12-30T18:54:48.917163",
     "exception": false,
     "start_time": "2025-12-30T18:54:48.893358",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No .zip files found in /kaggle/input/.\n"
     ]
    }
   ],
   "source": [
    "# @title Unzipping all archived files\n",
    "import os\n",
    "import glob\n",
    "from zipfile import ZipFile\n",
    "\n",
    "zip_file_paths = glob.glob(os.path.join(INPUT_PATH, '*.zip'))\n",
    "\n",
    "if not zip_file_paths:\n",
    "    print(f'No .zip files found in {INPUT_PATH}.')\n",
    "else:\n",
    "    for zip_file_path in zip_file_paths:\n",
    "        if os.path.exists(zip_file_path):\n",
    "            print(f'Unzipping {zip_file_path}...')\n",
    "            !unzip -o {zip_file_path} -d ./\n",
    "            print(f'Unzipping of {zip_file_path} complete.')\n",
    "        else:\n",
    "            print(f'Error: The file {zip_file_path} was not found (post-glob check).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51941368",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T10:04:00.760146Z",
     "iopub.status.busy": "2025-12-31T10:04:00.759197Z",
     "iopub.status.idle": "2025-12-31T10:04:03.822636Z",
     "shell.execute_reply": "2025-12-31T10:04:03.821847Z",
     "shell.execute_reply.started": "2025-12-31T10:04:00.760125Z"
    },
    "id": "Mx5uS5WQaqdn",
    "outputId": "951a03aa-416d-4e8d-ca55-121d410bb302",
    "papermill": {
     "duration": 1.62157,
     "end_time": "2025-12-30T18:54:50.594793",
     "exception": false,
     "start_time": "2025-12-30T18:54:48.973223",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Demonstrating function with a dummy CSV file ---\n",
      "Created a dummy CSV file at: /kaggle/working/dummy_data.csv\n",
      "Successfully converted '/kaggle/working/dummy_data.csv' to '/kaggle/working/dummy_chars.txt', with one character per line.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def convert_csv_to_chars_txt(input_csv_path: str, output_txt_path: str, column_name: str = 'word'):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, extracts text from a specified column, and writes each character\n",
    "    to a new line in a plain text file.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): The full path to the input CSV file.\n",
    "        output_txt_path (str): The full path for the output text file.\n",
    "        column_name (str): The name of the column in the CSV file containing the text.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_csv_path):\n",
    "        print(f\"Error: Input CSV file not found at '{input_csv_path}'. Please ensure the file is uploaded.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file '{input_csv_path}': {e}\")\n",
    "        return\n",
    "\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found in the CSV file '{input_csv_path}'.\")\n",
    "        return\n",
    "\n",
    "    all_characters = []\n",
    "    # Ensure the column values are treated as strings before iterating over them\n",
    "    for item in df[column_name].astype(str).dropna().tolist():\n",
    "        for char in item:\n",
    "            all_characters.append(char)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)\n",
    "\n",
    "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(all_characters))\n",
    "    print(f\"Successfully converted '{input_csv_path}' to '{output_txt_path}', with one character per line.\")\n",
    "\n",
    "# --- Example Usage (demonstration with a dummy file) ---\n",
    "# As the original file 'Ds_300_ChuNom_TuTao.csv' was not found in the previous execution,\n",
    "# let's create a dummy file to demonstrate the function's usage.\n",
    "print(\"\\n--- Demonstrating function with a dummy CSV file ---\")\n",
    "dummy_csv_path = os.path.join(OUTPUT_PATH, \"dummy_data.csv\")\n",
    "dummy_output_txt_path = os.path.join(OUTPUT_PATH, \"dummy_chars.txt\")\n",
    "\n",
    "# Create a dummy CSV file\n",
    "dummy_data = {'word': ['hello', 'world', 'python']}\n",
    "pd.DataFrame(dummy_data).to_csv(dummy_csv_path, index=False)\n",
    "print(f\"Created a dummy CSV file at: {dummy_csv_path}\")\n",
    "\n",
    "convert_csv_to_chars_txt(dummy_csv_path, dummy_output_txt_path)\n",
    "\n",
    "# --- How to use with your actual file ---\n",
    "# Uncomment the lines below and replace 'your_actual_file.csv' and 'your_output.txt'\n",
    "# with the correct paths for your use case.\n",
    "#\n",
    "# original_csv_file = os.path.join(INPUT_PATH, \"Ds_300_ChuNom_TuTao.csv\") # Or the full path to your CSV\n",
    "# original_output_txt = os.path.join(OUTPUT_PATH, \"nom_tu_tao.txt\") # Or your desired output path\n",
    "# convert_csv_to_chars_txt(original_csv_file, original_output_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f4cf20b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T10:04:03.823591Z",
     "iopub.status.busy": "2025-12-31T10:04:03.823347Z",
     "iopub.status.idle": "2025-12-31T10:04:04.724388Z",
     "shell.execute_reply": "2025-12-31T10:04:04.723626Z",
     "shell.execute_reply.started": "2025-12-31T10:04:03.823572Z"
    },
    "id": "Sxz63qgifNlV",
    "outputId": "c86d1ccf-dcc8-44ec-d61f-9035dd9d0369",
    "papermill": {
     "duration": 0.140282,
     "end_time": "2025-12-30T18:54:50.749810",
     "exception": false,
     "start_time": "2025-12-30T18:54:50.609528",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 384M\n",
      "drwxr-xr-x 3 root root 4.0K Dec 31 10:03 .cache\n",
      "-rw-r--r-- 1 root root 4.6M Dec 31 10:03 content_encoder.safetensors\n",
      "-rw-r--r-- 1 root root  79M Dec 31 10:04 style_encoder.safetensors\n",
      "-rw-r--r-- 1 root root 301M Dec 31 10:04 unet.safetensors\n",
      "drwxr-xr-x 3 root root 4.0K Dec 31 10:04 .\n",
      "drwxr-xr-x 5 root root 4.0K Dec 31 10:04 ..\n"
     ]
    }
   ],
   "source": [
    "!ls -larth {OUTPUT_PATH}/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92cff682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T10:04:04.725884Z",
     "iopub.status.busy": "2025-12-31T10:04:04.725575Z",
     "iopub.status.idle": "2025-12-31T10:05:32.515715Z",
     "shell.execute_reply": "2025-12-31T10:05:32.514613Z",
     "shell.execute_reply.started": "2025-12-31T10:04:04.725851Z"
    },
    "id": "MvEJIiH5fNlV",
    "outputId": "95a0a309-3e6b-482b-c971-cf93efac61bf",
    "papermill": {
     "duration": 0.104394,
     "end_time": "2025-12-30T18:54:50.869230",
     "exception": false,
     "start_time": "2025-12-30T18:54:50.764836",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Attempting to load secret 'HF_TOKEN' from 'kaggle' environment...\n",
      "‚úÖ Successfully loaded secret 'HF_TOKEN'.\n",
      "\n",
      "============================================================\n",
      "EXPORTING DATASET TO DISK\n",
      "============================================================\n",
      "\n",
      "üì• Loading dataset from Hub...\n",
      "   Repository: dzungpham/font-diffusion-generated-data\n",
      "   Split: train\n",
      "README.md: 1.29kB [00:00, 559kB/s]\n",
      "data/train-00000-of-00001.parquet: 100%|‚ñà‚ñà‚ñà| 39.5M/39.5M [00:00<00:00, 41.6MB/s]\n",
      "data/val-00000-of-00001.parquet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 262k/262k [00:00<00:00, 1.42MB/s]\n",
      "data/val_unseen_both-00000-of-00001.parq(‚Ä¶): 100%|‚ñà| 2.52M/2.52M [00:00<00:00, 5\n",
      "data/val_seen_style_unseen_char-00000-of(‚Ä¶): 100%|‚ñà| 8.02M/8.02M [00:00<00:00, 2\n",
      "data/val_unseen_style_seen_char-00000-of(‚Ä¶): 100%|‚ñà| 10.5M/10.5M [00:00<00:00, 2\n",
      "data/train_original-00000-of-00001.parqu(‚Ä¶): 100%|‚ñà| 68.8M/68.8M [00:00<00:00, 1\n",
      "data/test-00000-of-00001.parquet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 646k/646k [00:00<00:00, 939kB/s]\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 2585/2585 [00:00<00:00, 18583.20 examples/s]\n",
      "Generating val split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:00<00:00, 5866.16 examples/s]\n",
      "Generating val_unseen_both split: 100%|‚ñà| 240/240 [00:00<00:00, 20559.89 example\n",
      "Generating val_seen_style_unseen_char split: 100%|‚ñà| 880/880 [00:00<00:00, 32620\n",
      "Generating val_unseen_style_seen_char split: 100%|‚ñà| 705/705 [00:00<00:00, 27745\n",
      "Generating train_original split: 100%|‚ñà| 4500/4500 [00:00<00:00, 26434.91 exampl\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [00:00<00:00, 8711.57 examples/s]\n",
      "‚úì Loaded dataset with 2585 samples from Hub\n",
      "\n",
      "üì• Attempting to load results_checkpoint.json from Hub...\n",
      "results_checkpoint.json: 1.57MB [00:00, 387MB/s]\n",
      "  ‚úì Loaded results_checkpoint.json (4500 generations)\n",
      "\n",
      "Exporting images from dataset...\n",
      "\n",
      "üìù Exporting content images...\n",
      "Content images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2585/2585 [00:05<00:00, 435.11it/s]\n",
      "‚úì Exported content images\n",
      "\n",
      "üé® Exporting target images...\n",
      "Target images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2585/2585 [00:06<00:00, 407.33it/s]\n",
      "‚úì Exported target images\n",
      "\n",
      "‚úÖ Using original results_checkpoint.json\n",
      "\n",
      "üíæ Saving results_checkpoint.json...\n",
      "  ‚úì Saved results_checkpoint.json (4500 generations)\n",
      "\n",
      "üìä Metadata Statistics:\n",
      "  Total generations: 4500\n",
      "  Total characters: 300\n",
      "  Total styles: 15\n",
      "  Fonts: NomNaTongLight2\n",
      "\n",
      "============================================================\n",
      "‚úÖ EXPORT COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Files created:\n",
      "  ‚úì my_dataset/train/ContentImage/\n",
      "  ‚úì my_dataset/train/TargetImage/\n",
      "  ‚úì my_dataset/train/results_checkpoint.json\n",
      "\n",
      "============================================================\n",
      "EXPORTING DATASET TO DISK\n",
      "============================================================\n",
      "\n",
      "üì• Loading dataset from Hub...\n",
      "   Repository: dzungpham/font-diffusion-generated-data\n",
      "   Split: train_original\n",
      "‚úì Loaded dataset with 4500 samples from Hub\n",
      "\n",
      "üì• Attempting to load results_checkpoint.json from Hub...\n",
      "  ‚úì Loaded results_checkpoint.json (4500 generations)\n",
      "\n",
      "Exporting images from dataset...\n",
      "\n",
      "üìù Exporting content images...\n",
      "Content images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4500/4500 [00:10<00:00, 434.82it/s]\n",
      "‚úì Exported content images\n",
      "\n",
      "üé® Exporting target images...\n",
      "Target images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4500/4500 [00:11<00:00, 405.03it/s]\n",
      "‚úì Exported target images\n",
      "\n",
      "‚úÖ Using original results_checkpoint.json\n",
      "\n",
      "üíæ Saving results_checkpoint.json...\n",
      "  ‚úì Saved results_checkpoint.json (4500 generations)\n",
      "\n",
      "üìä Metadata Statistics:\n",
      "  Total generations: 4500\n",
      "  Total characters: 300\n",
      "  Total styles: 15\n",
      "  Fonts: NomNaTongLight2\n",
      "\n",
      "============================================================\n",
      "‚úÖ EXPORT COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Files created:\n",
      "  ‚úì my_dataset/train_original/ContentImage/\n",
      "  ‚úì my_dataset/train_original/TargetImage/\n",
      "  ‚úì my_dataset/train_original/results_checkpoint.json\n",
      "\n",
      "============================================================\n",
      "EXPORTING DATASET TO DISK\n",
      "============================================================\n",
      "\n",
      "üì• Loading dataset from Hub...\n",
      "   Repository: dzungpham/font-diffusion-generated-data\n",
      "   Split: val_unseen_both\n",
      "‚úì Loaded dataset with 240 samples from Hub\n",
      "\n",
      "üì• Attempting to load results_checkpoint.json from Hub...\n",
      "  ‚úì Loaded results_checkpoint.json (4500 generations)\n",
      "\n",
      "Exporting images from dataset...\n",
      "\n",
      "üìù Exporting content images...\n",
      "Content images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 438.24it/s]\n",
      "‚úì Exported content images\n",
      "\n",
      "üé® Exporting target images...\n",
      "Target images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 447.62it/s]\n",
      "‚úì Exported target images\n",
      "\n",
      "‚úÖ Using original results_checkpoint.json\n",
      "\n",
      "üíæ Saving results_checkpoint.json...\n",
      "  ‚úì Saved results_checkpoint.json (4500 generations)\n",
      "\n",
      "üìä Metadata Statistics:\n",
      "  Total generations: 4500\n",
      "  Total characters: 300\n",
      "  Total styles: 15\n",
      "  Fonts: NomNaTongLight2\n",
      "\n",
      "============================================================\n",
      "‚úÖ EXPORT COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Files created:\n",
      "  ‚úì my_dataset/val_unseen_both/ContentImage/\n",
      "  ‚úì my_dataset/val_unseen_both/TargetImage/\n",
      "  ‚úì my_dataset/val_unseen_both/results_checkpoint.json\n",
      "\n",
      "============================================================\n",
      "EXPORTING DATASET TO DISK\n",
      "============================================================\n",
      "\n",
      "üì• Loading dataset from Hub...\n",
      "   Repository: dzungpham/font-diffusion-generated-data\n",
      "   Split: val_seen_style_unseen_char\n",
      "‚úì Loaded dataset with 880 samples from Hub\n",
      "\n",
      "üì• Attempting to load results_checkpoint.json from Hub...\n",
      "  ‚úì Loaded results_checkpoint.json (4500 generations)\n",
      "\n",
      "Exporting images from dataset...\n",
      "\n",
      "üìù Exporting content images...\n",
      "Content images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 880/880 [00:02<00:00, 434.20it/s]\n",
      "‚úì Exported content images\n",
      "\n",
      "üé® Exporting target images...\n",
      "Target images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 880/880 [00:02<00:00, 414.67it/s]\n",
      "‚úì Exported target images\n",
      "\n",
      "‚úÖ Using original results_checkpoint.json\n",
      "\n",
      "üíæ Saving results_checkpoint.json...\n",
      "  ‚úì Saved results_checkpoint.json (4500 generations)\n",
      "\n",
      "üìä Metadata Statistics:\n",
      "  Total generations: 4500\n",
      "  Total characters: 300\n",
      "  Total styles: 15\n",
      "  Fonts: NomNaTongLight2\n",
      "\n",
      "============================================================\n",
      "‚úÖ EXPORT COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Files created:\n",
      "  ‚úì my_dataset/val_seen_style_unseen_char/ContentImage/\n",
      "  ‚úì my_dataset/val_seen_style_unseen_char/TargetImage/\n",
      "  ‚úì my_dataset/val_seen_style_unseen_char/results_checkpoint.json\n",
      "\n",
      "============================================================\n",
      "EXPORTING DATASET TO DISK\n",
      "============================================================\n",
      "\n",
      "üì• Loading dataset from Hub...\n",
      "   Repository: dzungpham/font-diffusion-generated-data\n",
      "   Split: val_unseen_style_seen_char\n",
      "‚úì Loaded dataset with 705 samples from Hub\n",
      "\n",
      "üì• Attempting to load results_checkpoint.json from Hub...\n",
      "  ‚úì Loaded results_checkpoint.json (4500 generations)\n",
      "\n",
      "Exporting images from dataset...\n",
      "\n",
      "üìù Exporting content images...\n",
      "Content images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 705/705 [00:01<00:00, 451.29it/s]\n",
      "‚úì Exported content images\n",
      "\n",
      "üé® Exporting target images...\n",
      "Target images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 705/705 [00:01<00:00, 442.77it/s]\n",
      "‚úì Exported target images\n",
      "\n",
      "‚úÖ Using original results_checkpoint.json\n",
      "\n",
      "üíæ Saving results_checkpoint.json...\n",
      "  ‚úì Saved results_checkpoint.json (4500 generations)\n",
      "\n",
      "üìä Metadata Statistics:\n",
      "  Total generations: 4500\n",
      "  Total characters: 300\n",
      "  Total styles: 15\n",
      "  Fonts: NomNaTongLight2\n",
      "\n",
      "============================================================\n",
      "‚úÖ EXPORT COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Files created:\n",
      "  ‚úì my_dataset/val_unseen_style_seen_char/ContentImage/\n",
      "  ‚úì my_dataset/val_unseen_style_seen_char/TargetImage/\n",
      "  ‚úì my_dataset/val_unseen_style_seen_char/results_checkpoint.json\n",
      "\n",
      "============================================================\n",
      "EXPORTING DATASET TO DISK\n",
      "============================================================\n",
      "\n",
      "üì• Loading dataset from Hub...\n",
      "   Repository: dzungpham/font-diffusion-generated-data\n",
      "   Split: test\n",
      "‚úì Loaded dataset with 41 samples from Hub\n",
      "\n",
      "üì• Attempting to load results_checkpoint.json from Hub...\n",
      "  ‚úì Loaded results_checkpoint.json (4500 generations)\n",
      "\n",
      "Exporting images from dataset...\n",
      "\n",
      "üìù Exporting content images...\n",
      "Content images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [00:00<00:00, 397.77it/s]\n",
      "‚úì Exported content images\n",
      "\n",
      "üé® Exporting target images...\n",
      "Target images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [00:00<00:00, 360.59it/s]\n",
      "‚úì Exported target images\n",
      "\n",
      "‚úÖ Using original results_checkpoint.json\n",
      "\n",
      "üíæ Saving results_checkpoint.json...\n",
      "  ‚úì Saved results_checkpoint.json (4500 generations)\n",
      "\n",
      "üìä Metadata Statistics:\n",
      "  Total generations: 4500\n",
      "  Total characters: 300\n",
      "  Total styles: 15\n",
      "  Fonts: NomNaTongLight2\n",
      "\n",
      "============================================================\n",
      "‚úÖ EXPORT COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Files created:\n",
      "  ‚úì my_dataset/test/ContentImage/\n",
      "  ‚úì my_dataset/test/TargetImage/\n",
      "  ‚úì my_dataset/test/results_checkpoint.json\n",
      "SUCCESSFULLY EXPORT HF DATASET TO LOCAL DIRECTORY\n"
     ]
    }
   ],
   "source": [
    "%cd {OUTPUT_PATH}\n",
    "from huggingface_hub import login\n",
    "HF_TOKEN = load_secret(\"HF_TOKEN\")\n",
    "login(HF_TOKEN)\n",
    "HF_USERNAME = \"dzungpham\"\n",
    "\n",
    "# ==========================================\n",
    "# EXPORT / DOWNLOAD DATASET COMMANDS\n",
    "# ==========================================\n",
    "\n",
    "# Train Split\n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/train\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"train\" \\\n",
    "  --token HF_TOKEN \n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/train_original\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"train_original\" \\\n",
    "  --token HF_TOKEN \n",
    "# Validation: Unseen Both\n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/val_unseen_both\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"val_unseen_both\" \\\n",
    "  --token HF_TOKEN \n",
    "# Validation: Seen Style, Unseen Char\n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/val_seen_style_unseen_char\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"val_seen_style_unseen_char\" \\\n",
    "  --token HF_TOKEN \n",
    "# Validation: Unseen Style, Seen Char\n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/val_unseen_style_seen_char\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"val_unseen_style_seen_char\" \\\n",
    "  --token HF_TOKEN \n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/test\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"test\" \\\n",
    "  --token HF_TOKEN \n",
    "print(\"SUCCESSFULLY EXPORT HF DATASET TO LOCAL DIRECTORY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d77b3a8e-60ec-4299-ad46-a82ee2852629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T10:07:01.577850Z",
     "iopub.status.busy": "2025-12-31T10:07:01.577138Z",
     "iopub.status.idle": "2025-12-31T10:07:01.945603Z",
     "shell.execute_reply": "2025-12-31T10:07:01.944645Z",
     "shell.execute_reply.started": "2025-12-31T10:07:01.577817Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m12 packages\u001b[0m \u001b[2min 56ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 56ms\u001b[0m\u001b[0m                                               \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m25.2                              \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.26.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.25.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install \"huggingface-hub==0.25.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29deed1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T10:07:06.346050Z",
     "iopub.status.busy": "2025-12-31T10:07:06.345117Z",
     "iopub.status.idle": "2025-12-31T10:11:25.809415Z",
     "shell.execute_reply": "2025-12-31T10:11:25.808516Z",
     "shell.execute_reply.started": "2025-12-31T10:07:06.346017Z"
    },
    "id": "gma02BZvhx8I",
    "outputId": "a8a54761-e26c-488d-d87d-cd14d686e77f",
    "papermill": {
     "duration": 10.53661,
     "end_time": "2025-12-30T18:55:01.421093",
     "exception": false,
     "start_time": "2025-12-30T18:54:50.884483",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.11/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2025-12-31 10:07:15.584350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767175635.778279    1266 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767175635.834535    1266 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "\n",
      "============================================================\n",
      "FONTDIFFUSER - SIMPLIFIED (Only results_checkpoint.json)\n",
      "============================================================\n",
      "Loading characters from lines 1 to 50 (total: 10174 lines)\n",
      "Successfully loaded 50 single characters.\n",
      "\n",
      "Initializing font manager...\n",
      "\n",
      "============================================================\n",
      "Loading 15 fonts from directory...\n",
      "============================================================\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: HAN NOM A\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: HAN NOM B\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: Han-Nom Kai 1.00\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: Han-Nom-Khai-Regular-300623\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: Han-nom Minh 1.42\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: HanaMinA\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: HanaMinA\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: HanaMinB\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: HanaMinB\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: HanaMinC\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: NomNaTong-Regular\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: NomNaTong-Regular\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: NomNaTong-Regular2\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: NomNaTongLight\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "‚úì Loaded: NomNaTongLight2\n",
      "============================================================\n",
      "Successfully loaded 12 fonts\n",
      "\n",
      "\n",
      "üìä Configuration:\n",
      "  Dataset split: train_original\n",
      "  Number of Characters: 50 (lines 1-50)\n",
      "  Number of Styles: 15\n",
      "  Output Directory: my_dataset/train_original\n",
      "  Save Interval: Every 1 styles\n",
      "‚úì Loaded checkpoint (4500 generations)\n",
      "‚úì Loaded results.json:\n",
      "  Characters: 300\n",
      "  Styles: 15\n",
      "  Existing pairs: 4500\n",
      "\n",
      "Loading FontDiffuser pipeline...\n",
      "Loading FontDiffuser pipeline...\n",
      "Load the down block  DownBlock2D\n",
      "Load the down block  MCADownBlock2D\n",
      "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
      "Load the down block  MCADownBlock2D\n",
      "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
      "Load the down block  DownBlock2D\n",
      "Load the up block  UpBlock2D\n",
      "Load the up block  StyleRSIUpBlock2D\n",
      "Load the up block  StyleRSIUpBlock2D\n",
      "Load the up block  UpBlock2D\n",
      "Param count for Ds initialized parameters: 20591296\n",
      "Get CG-GAN Style Encoder!\n",
      "Param count for Ds initialized parameters: 1187008\n",
      "Get CG-GAN Content Encoder!\n",
      "‚úì Loaded model state_dict successfully\n",
      "Converting to channels-last memory format...\n",
      "‚úì Model moved to device\n",
      "‚úì Loaded training DDPM scheduler successfully\n",
      "‚úì Loaded DPM-Solver pipeline successfully\n",
      "\n",
      "============================================================\n",
      "Compiling pipeline with torch.compile...\n",
      "============================================================\n",
      "‚ö†Ô∏è  Warning: torch.compile failed: \n",
      "  Continuing without compilation...\n",
      "  Note: torch.compile requires PyTorch 2.0+ and may not work with all models\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 233M/233M [00:01<00:00, 191MB/s]\n",
      "Loading model from: /usr/local/lib/python3.11/dist-packages/lpips/weights/v0.1/alex.pth\n",
      "üì• Resuming: 4500 pairs already processed\n",
      "\n",
      "======================================================================\n",
      "                        BATCH IMAGE GENERATION                        \n",
      "======================================================================\n",
      "Fonts:              12\n",
      "Styles:             15\n",
      "Characters:         50\n",
      "Batch size:         24\n",
      "Inference steps:    20\n",
      "Save interval:      Every 1 styles\n",
      "Output dir:         my_dataset/train_original\n",
      "======================================================================\n",
      "\n",
      "^C\n",
      "\n",
      "\n",
      "‚ö† Generation interrupted by user!\n",
      "üíæ Saving emergency checkpoint...\n"
     ]
    }
   ],
   "source": [
    "# already change sample_batch file to save all data in train_original\n",
    "%cd {OUTPUT_PATH}\n",
    "!python FontDiffusion/sample_batch.py \\\n",
    "    --characters \"FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\" \\\n",
    "    --style_images \"FontDiffusion/styles_images\" \\\n",
    "    --ckpt_dir \"ckpt/\" \\\n",
    "    --ttf_path \"FontDiffusion/fonts\" \\\n",
    "    --output_dir \"my_dataset/train_original\" \\\n",
    "    --resume_from \"my_dataset/train_original/results_checkpoint.json\" \\\n",
    "    --num_inference_steps 20 \\\n",
    "    --guidance_scale 7.5 \\\n",
    "    --start_line 1 \\\n",
    "    --end_line 50 \\\n",
    "    --batch_size 24 \\\n",
    "    --save_interval 1 \\\n",
    "    --channels_last \\\n",
    "    --seed 42 \\\n",
    "    --compile \\\n",
    "    --enable_xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9250a14",
   "metadata": {
    "id": "XoppW2x5fNlW",
    "outputId": "759bca6c-0025-454a-b3b6-ae8170f7edb4",
    "papermill": {
     "duration": 0.236541,
     "end_time": "2025-12-30T18:55:01.673705",
     "exception": false,
     "start_time": "2025-12-30T18:55:01.437164",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python FontDiffusion/create_validation_split.py \\\n",
    "  --data_root my_dataset \\\n",
    "  --val_ratio 0.2 \\\n",
    "  --test_ratio 0.1 \\\n",
    "  --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a9392",
   "metadata": {
    "id": "v-a7paEbfNlW",
    "outputId": "c792e189-6d31-4114-be92-644d4372efa7",
    "papermill": {
     "duration": 21.070659,
     "end_time": "2025-12-30T18:55:22.760587",
     "exception": false,
     "start_time": "2025-12-30T18:55:01.689928",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- RAW DATA (Before Splitting) ---\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/train_original\" \\\n",
    "  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n",
    "  --split \"train_original\" \\\n",
    "  --private \\\n",
    "  --token \"{HF_TOKEN}\"\n",
    "\n",
    "# --- ORGANIZED SPLITS (After Splitting) ---\n",
    "\n",
    "# Train Split\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/train\" \\\n",
    "  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n",
    "  --split \"train\" \\\n",
    "  --private \\\n",
    "  --token \"{HF_TOKEN}\"\n",
    "\n",
    "# Test Split\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/test\" \\\n",
    "  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n",
    "  --split \"test\" \\\n",
    "  --private \\\n",
    "  --token \"{HF_TOKEN}\"\n",
    "\n",
    "# Validation: Unseen Both\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/val_unseen_both\" \\\n",
    "  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n",
    "  --split \"val_unseen_both\" \\\n",
    "  --private \\\n",
    "  --token \"{HF_TOKEN}\"\n",
    "\n",
    "# Validation: Seen Style, Unseen Char\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/val_seen_style_unseen_char\" \\\n",
    "  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n",
    "  --split \"val_seen_style_unseen_char\" \\\n",
    "  --private \\\n",
    "  --token \"{HF_TOKEN}\"\n",
    "\n",
    "# Validation: Unseen Style, Seen Char\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/val_unseen_style_seen_char\" \\\n",
    "  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n",
    "  --split \"val_unseen_style_seen_char\" \\\n",
    "  --private \\\n",
    "  --token \"{HF_TOKEN}\"\n",
    "print(\"SUCCESSFULLY UPLOAD LOCAL MY_DATASET TO HUGGINGFACE DATASETS SPACE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87caab2",
   "metadata": {
    "id": "zXSQHJ3xVOSc",
    "outputId": "4d89a176-c286-442f-eb6e-01b39c994e64",
    "papermill": {
     "duration": 1.992585,
     "end_time": "2025-12-30T18:55:24.769269",
     "exception": false,
     "start_time": "2025-12-30T18:55:22.776684",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267634e8",
   "metadata": {
    "id": "FxxJ9qy4KIZH",
    "outputId": "f554de7b-f455-4a5b-f8e5-4b6cba3f756b",
    "papermill": {
     "duration": 0.021927,
     "end_time": "2025-12-30T18:55:24.807644",
     "exception": false,
     "start_time": "2025-12-30T18:55:24.785717",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TRAINING PHASE 1\n",
    "import wandb\n",
    "\n",
    "# Load your Weights & Biases API key from a secure store\n",
    "wandb_key = load_secret(\"WANDB_API_KEY\")\n",
    "wandb.login(key=wandb_key)\n",
    "\n",
    "# Run the training script with the corrected flag syntax\n",
    "!python FontDiffusion/my_train.py \\\n",
    "    --seed=123 \\\n",
    "    --experience_name=FontDiffuser_training_phase_1 \\\n",
    "    --data_root=my_dataset \\\n",
    "    --output_dir=outputs/FontDiffuser \\\n",
    "    --report_to=wandb \\\n",
    "    --resolution=96 \\\n",
    "    --style_image_size=96 \\\n",
    "    --content_image_size=96 \\\n",
    "    --content_encoder_downsample_size=3 \\\n",
    "    --channel_attn=True \\\n",
    "    --content_start_channel=64 \\\n",
    "    --style_start_channel=64 \\\n",
    "    --train_batch_size=8 \\\n",
    "    --perceptual_coefficient=0.03 \\\n",
    "    --offset_coefficient=0.7 \\\n",
    "    --max_train_steps=2000 \\\n",
    "    --ckpt_interval=1000 \\\n",
    "    --val_interval=200 \\\n",
    "    --gradient_accumulation_steps=1 \\\n",
    "    --log_interval=50 \\\n",
    "    --learning_rate=1e-4 \\\n",
    "    --lr_scheduler=linear \\\n",
    "    --lr_warmup_steps=10000 \\\n",
    "    --drop_prob=0.1 \\\n",
    "    --mixed_precision=no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a3326-6bfa-4fba-bbcc-e2e7be996c7f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls -lr outputs/FontDiffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8136e",
   "metadata": {
    "id": "J4bplsS6pQna",
    "papermill": {
     "duration": 0.022471,
     "end_time": "2025-12-30T18:55:24.845778",
     "exception": false,
     "start_time": "2025-12-30T18:55:24.823307",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TRAINING PHASE 2\n",
    "!wandb login\n",
    "!python FontDiffusion/my_train.py \\\n",
    "    --seed=123 \\\n",
    "    --experience_name=\"FontDiffuser_training_phase_2\" \\\n",
    "    --data_root=\"my_dataset\" \\\n",
    "    --output_dir=\"outputs/FontDiffuser\" \\\n",
    "    --report_to=\"wandb\" \\\n",
    "    --phase_2 \\\n",
    "    --phase_1_ckpt_dir=\"outputs/FontDiffuser/global_step_2000\" \\\n",
    "    --scr_ckpt_path=\"ckpt/scr_210000.pth\" \\\n",
    "    --sc_coefficient=0.05 \\\n",
    "    --num_neg=13 \\\n",
    "    --resolution=96 \\\n",
    "    --style_image_size=96 \\\n",
    "    --content_image_size=96 \\\n",
    "    --content_encoder_downsample_size=3 \\\n",
    "    --channel_attn=True \\\n",
    "    --content_start_channel=64 \\\n",
    "    --style_start_channel=64 \\\n",
    "    --train_batch_size=8 \\\n",
    "    --perceptual_coefficient=0.03 \\\n",
    "    --offset_coefficient=0.4 \\\n",
    "    --max_train_steps=100 \\\n",
    "    --ckpt_interval=50 \\\n",
    "    --gradient_accumulation_steps=2 \\\n",
    "    --log_interval=50 \\\n",
    "    --learning_rate=1e-5 \\\n",
    "    --lr_scheduler=\"constant\" \\\n",
    "    --lr_warmup_steps=1000 \\\n",
    "    --drop_prob=0.1 \\\n",
    "    --mixed_precision=\"no\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c45e2f",
   "metadata": {
    "id": "nF8opWokKcMS",
    "outputId": "2838655d-5d24-4808-813a-39f20ec239a8",
    "papermill": {
     "duration": 0.217876,
     "end_time": "2025-12-30T18:55:25.079820",
     "exception": false,
     "start_time": "2025-12-30T18:55:24.861944",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python FontDiffusion/pth2safetensors.py \\\n",
    "    --weights_dir \"ckpt\" \\\n",
    "    --repo_id \"dzungpham/font-diffusion-weights\" \\\n",
    "    --token \"{HF_TOKEN}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868b20b",
   "metadata": {
    "id": "kTz9WZ9ylBZx",
    "outputId": "ccb61bf4-7551-4269-aee7-b0742fe1517a",
    "papermill": {
     "duration": 0.031197,
     "end_time": "2025-12-30T18:55:25.126961",
     "exception": false,
     "start_time": "2025-12-30T18:55:25.095764",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "def find_result_folders(base_path: Path, pattern_name: str) -> List[Path]:\n",
    "    return [p for p in base_path.glob(pattern_name) if p.is_dir()]\n",
    "\n",
    "def zip_folder(folder_path: Path, output_base_path: Path) -> bool:\n",
    "    folder_name = folder_path.name\n",
    "    zip_path = output_base_path / f\"{folder_name}.zip\"\n",
    "    try:\n",
    "        print(f\"   -> Zipping folder: {folder_name}...\")\n",
    "        with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for file_path in folder_path.rglob(\"*\"):\n",
    "                if file_path.is_file():\n",
    "                    arcname = file_path.relative_to(folder_path.parent)\n",
    "                    zipf.write(file_path, arcname)\n",
    "        print(f\"   ‚úÖ Created ZIP: {zip_path.name}\")\n",
    "        return True\n",
    "    except Exception as exc:\n",
    "        print(f\"   ‚ùå Failed to zip {folder_name}: {exc}\")\n",
    "        return False\n",
    "\n",
    "def zip_stats_results_folders(output_base_path: str, pattern_name: str) -> None:\n",
    "    base = Path(output_base_path)\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "    result_folders = find_result_folders(base, pattern_name)\n",
    "    if not result_folders:\n",
    "        print(f\"‚ö†Ô∏è No folders matching '*dataset' found in '{output_base_path}'.\")\n",
    "        return\n",
    "    print(f\"üîç Found {len(result_folders)} result folder(s) to zip.\")\n",
    "    successful = sum(1 for folder in result_folders if zip_folder(folder, base))\n",
    "    print(f\"\\n‚úÖ DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n",
    "        if not output_root:\n",
    "            raise ValueError(\"OUTPUT_PATH not defined\")\n",
    "        zip_stats_results_folders(\n",
    "            output_base_path=OUTPUT_PATH,\n",
    "            pattern_name=\"outputs/FontDiffuser/global*\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 127.636509,
   "end_time": "2025-12-30T18:55:25.961447",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-30T18:53:18.324938",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
