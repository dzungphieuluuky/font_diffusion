{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a46ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a95a46ef",
    "outputId": "d76d28cd-6292-42bf-fffa-a8c7efb86ed0",
    "papermill": {
     "duration": 12.857369,
     "end_time": "2025-12-30T18:53:35.066181",
     "exception": false,
     "start_time": "2025-12-30T18:53:22.208812",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Environment Setup\n",
    "import os\n",
    "import sys\n",
    "if 'MPLBACKEND' in os.environ:\n",
    "    del os.environ['MPLBACKEND']\n",
    "    print(\"MPLBACKEND environment variable cleared.\")\n",
    "\n",
    "# 2. Clone the repository\n",
    "!rm -rf FontDiffusion\n",
    "!git clone https://github.com/dzungphieuluuky/FontDiffusion.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd8666",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cdd8666",
    "outputId": "8834f4e4-fc28-455c-a66c-d15b00de080a",
    "papermill": {
     "duration": 0.019157,
     "end_time": "2025-12-30T18:53:35.092303",
     "exception": false,
     "start_time": "2025-12-30T18:53:35.073146",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from IPython import get_ipython\n",
    "from typing import Optional\n",
    "\n",
    "def configure_environment_paths():\n",
    "    try:\n",
    "        if \"google.colab\" in str(get_ipython()):\n",
    "            print(\"âœ… Environment: Google Colab\")\n",
    "            base_data_path = \"/content/\"\n",
    "            base_output_path = \"/content/\"\n",
    "            environment_name = \"colab\"\n",
    "        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
    "            print(\"âœ… Environment: Kaggle\")\n",
    "            base_data_path = \"/kaggle/input/\"\n",
    "            base_output_path = \"/kaggle/working/\"\n",
    "            environment_name = \"kaggle\"\n",
    "        else:\n",
    "            print(\"âš ï¸ Environment: Local/Unknown\")\n",
    "            base_data_path = \"./data/\"\n",
    "            base_output_path = \"./output/\"\n",
    "            environment_name = \"local\"\n",
    "    except NameError:\n",
    "        print(\"âš ï¸ Non-interactive session. Using local paths.\")\n",
    "        base_data_path = \"./data/\"\n",
    "        base_output_path = \"./output/\"\n",
    "        environment_name = \"local\"\n",
    "    os.makedirs(base_output_path, exist_ok=True)\n",
    "    print(f\"ðŸ“‚ Data Path: {base_data_path}\")\n",
    "    print(f\"ðŸ“¦ Output Path: {base_output_path}\")\n",
    "    return base_data_path, base_output_path, environment_name\n",
    "\n",
    "def load_secret(key_name: str) -> Optional[str]:\n",
    "    env = ENV_NAME\n",
    "    secret_value = None\n",
    "    print(f\"Attempting to load secret '{key_name}' from '{env}' environment...\")\n",
    "    try:\n",
    "        if env == \"colab\":\n",
    "            from google.colab import userdata\n",
    "            secret_value = userdata.get(key_name)\n",
    "        elif env == \"kaggle\":\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            user_secrets = UserSecretsClient()\n",
    "            secret_value = user_secrets.get_secret(key_name)\n",
    "        else:\n",
    "            secret_value = os.getenv(key_name)\n",
    "        if not secret_value:\n",
    "            print(f\"âš ï¸ Secret '{key_name}' not found in the {env} environment.\")\n",
    "            return None\n",
    "        print(f\"âœ… Successfully loaded secret '{key_name}'.\")\n",
    "        return secret_value\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ An error occurred while loading secret '{key_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "def print_system_info():\n",
    "    print(\"\\nðŸ”§ System Information\")\n",
    "    print(f\"Python version: {sys.version.split()[0]}\")\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"CUDA version: {torch.version.cuda}\")\n",
    "            print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        else:\n",
    "            print(\"CUDA not available\")\n",
    "    except ImportError:\n",
    "        print(\"PyTorch not installed\")\n",
    "    finally:\n",
    "      !nvidia-smi\n",
    "\n",
    "INPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()\n",
    "is_kaggle = (\"kaggle\" in ENV_NAME)\n",
    "is_colab = not is_kaggle\n",
    "print_system_info()\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_key = load_secret(\"WANDB_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN = load_secret('HF_TOKEN')\n",
    "\n",
    "# Now, these libraries will log in automatically\n",
    "import wandb\n",
    "import huggingface_hub\n",
    "\n",
    "wandb.login() \n",
    "huggingface_hub.login(token=os.environ[\"HF_TOKEN\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b4150",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a73b4150",
    "outputId": "97db2cec-8e2d-438b-e5f8-38df08b7f59e",
    "papermill": {
     "duration": 61.239828,
     "end_time": "2025-12-30T18:54:36.338205",
     "exception": false,
     "start_time": "2025-12-30T18:53:35.098377",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!uv pip install --upgrade pip\n",
    "# 3. Install PyTorch 1.13\n",
    "%cd {OUTPUT_PATH}\n",
    "# Force reinstall torch 1.13 to match the model's training environment\n",
    "# !uv pip uninstall torch torchvision\n",
    "# !uv pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "!uv pip install torch==2.9 torchvision\n",
    "# 4. Install other dependencies\n",
    "\n",
    "print(\"\\nâ¬‡ï¸ Installing Dependencies (Manually fixed)...\")\n",
    "# Install xformers compatible with Torch 1.13\n",
    "!uv pip install xformers==0.0.16 -q\n",
    "\n",
    "# Install original dependencies\n",
    "!uv pip install transformers==4.33.1 accelerate==0.23.0 diffusers==0.22.0\n",
    "!uv pip install gradio==4.8.0 pyyaml pygame opencv-python info-nce-pytorch kornia\n",
    "# -----------------------------------------------------------------\n",
    "!uv pip install lpips scikit-image pytorch-fid\n",
    "!sudo apt-get update && sudo apt-get install dos2unix\n",
    "!uv pip install gdown tqdm\n",
    "!uv pip install wandb\n",
    "!uv pip install --upgrade pyarrow datasets\n",
    "print(\"\\nâœ… Environment setup complete. You can now proceed to Block 2 (Inference).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd517dfe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd517dfe",
    "outputId": "d83605e9-f5dc-4862-d1c9-b138a96ca47a",
    "papermill": {
     "duration": 12.524295,
     "end_time": "2025-12-30T18:54:48.878013",
     "exception": false,
     "start_time": "2025-12-30T18:54:36.353718",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# KAGGLE CELL #1: Download checkpoint\n",
    "if is_colab:\n",
    "  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\n",
    "else:\n",
    "  !uv pip install --upgrade \"huggingface-hub==0.25.2\" \"protobuf<5.0.0\" \"numpy<2.0.0\"\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "os.chdir(OUTPUT_PATH)\n",
    "# Download from Hub\n",
    "if not os.path.exists(\"ckpt\") or not list(Path(\"ckpt\").glob(\"*.safetensors\")):\n",
    "    print(\"ðŸ“¥ Downloading checkpoint from Hugging Face Hub...\\n\")\n",
    "    from huggingface_hub import snapshot_download\n",
    "    snapshot_download(\n",
    "        repo_id=\"dzungpham/font-diffusion-weights\",\n",
    "        local_dir=\"ckpt\",\n",
    "        allow_patterns=\"*.safetensors\",\n",
    "        force_download=False\n",
    "    )\n",
    "    print(\"\\nâœ… Download complete!\")\n",
    "else:\n",
    "    print(\"âœ… Checkpoint already downloaded\")\n",
    "# Verify\n",
    "print(\"\\nðŸ“‚ Files in ckpt/:\")\n",
    "for file in os.listdir(\"ckpt\"):\n",
    "    if file.endswith(\".safetensors\"):\n",
    "        size = os.path.getsize(f\"ckpt/{file}\") / (1024**2)\n",
    "        print(f\"  âœ“ {file} ({size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e8ea2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "767e8ea2",
    "outputId": "20185e27-e772-4823-e6bc-d9bd6d0b39a1",
    "papermill": {
     "duration": 0.023805,
     "end_time": "2025-12-30T18:54:48.917163",
     "exception": false,
     "start_time": "2025-12-30T18:54:48.893358",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Unzipping all archived files\n",
    "import os\n",
    "import glob\n",
    "from zipfile import ZipFile\n",
    "\n",
    "zip_file_paths = glob.glob(os.path.join(INPUT_PATH, '*.zip'))\n",
    "\n",
    "if not zip_file_paths:\n",
    "    print(f'No .zip files found in {INPUT_PATH}.')\n",
    "else:\n",
    "    for zip_file_path in zip_file_paths:\n",
    "        if os.path.exists(zip_file_path):\n",
    "            print(f'Unzipping {zip_file_path}...')\n",
    "            !unzip -o {zip_file_path} -d ./\n",
    "            print(f'Unzipping of {zip_file_path} complete.')\n",
    "        else:\n",
    "            print(f'Error: The file {zip_file_path} was not found (post-glob check).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4cf20b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f4cf20b",
    "outputId": "335f4192-47e7-451a-e14f-e0bd69fbdfc9",
    "papermill": {
     "duration": 0.140282,
     "end_time": "2025-12-30T18:54:50.749810",
     "exception": false,
     "start_time": "2025-12-30T18:54:50.609528",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Model files:\")\n",
    "!ls -larth {OUTPUT_PATH}/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cff682",
   "metadata": {
    "id": "92cff682",
    "papermill": {
     "duration": 0.104394,
     "end_time": "2025-12-30T18:54:50.869230",
     "exception": false,
     "start_time": "2025-12-30T18:54:50.764836",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd {OUTPUT_PATH}\n",
    "# ==========================================\n",
    "# EXPORT / DOWNLOAD DATASET COMMANDS\n",
    "# ==========================================\n",
    "HF_USERNAME = \"dzungpham\"\n",
    "# Train Split\n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/train_original\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"train_original\" \\\n",
    "  --token HF_TOKEN\n",
    "\n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/train\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"train\" \\\n",
    "  --token HF_TOKEN\n",
    "# Validation: Unseen Both\n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/val\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"val\" \\\n",
    "  --token HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9c1d6-dd60-479c-92c4-2f653e4d48fd",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Fonts currently in fonts/ folder\")\n",
    "!ls -lt FontDiffusion/fonts\n",
    "print(\"Styles in style_images/ folder\")\n",
    "!ls -l FontDiffusion/styles_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29deed1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29deed1d",
    "outputId": "749b50d0-75e3-4d36-e509-919188feb64c",
    "papermill": {
     "duration": 10.53661,
     "end_time": "2025-12-30T18:55:01.421093",
     "exception": false,
     "start_time": "2025-12-30T18:54:50.884483",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if is_colab:\n",
    "  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\n",
    "else:\n",
    "  !uv pip install --upgrade \"huggingface-hub==0.25.2\" \"protobuf<5.0.0\" \"numpy<2.0.0\"\n",
    "%cd {OUTPUT_PATH}\n",
    "!accelerate launch --num_processes 1 \\\n",
    "    FontDiffusion/sample_batch.py \\\n",
    "    --characters \"FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\" \\\n",
    "    --style_images \"FontDiffusion/styles_images\" \\\n",
    "    --ckpt_dir \"ckpt/\" \\\n",
    "    --ttf_path \"FontDiffusion/fonts/NomNaTong-Regular.otf\" \\\n",
    "    --output_dir \"my_dataset/train_original\" \\\n",
    "    --num_inference_steps 20 \\\n",
    "    --guidance_scale 7.5 \\\n",
    "    --start_line 301 \\\n",
    "    --end_line 600 \\\n",
    "    --batch_size 35 \\\n",
    "    --save_interval 1 \\\n",
    "    --channels_last \\\n",
    "    --seed 42 \\\n",
    "    --compile \\\n",
    "    --enable_xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997103e5-221c-40a1-a0d1-83a93e1030f7",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!find my_dataset/train_original/ContentImage -type f | wc -l\n",
    "!find my_dataset/train_original/TargetImage -type f | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9250a14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-02T04:39:25.243505Z",
     "iopub.status.busy": "2026-01-02T04:39:25.243143Z",
     "iopub.status.idle": "2026-01-02T04:39:27.551960Z",
     "shell.execute_reply": "2026-01-02T04:39:27.551218Z",
     "shell.execute_reply.started": "2026-01-02T04:39:25.243445Z"
    },
    "id": "f9250a14",
    "outputId": "0f834d09-da00-4aa4-f486-6e70981b4137",
    "papermill": {
     "duration": 0.236541,
     "end_time": "2025-12-30T18:55:01.673705",
     "exception": false,
     "start_time": "2025-12-30T18:55:01.437164",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-02 04:39:25,499 | INFO | âœ“ Using source directory: my_dataset/train_original\n",
      "2026-01-02 04:39:25,504 | INFO | \n",
      "============================================================\n",
      "2026-01-02 04:39:25,504 | INFO | FONTDIFFUSION VALIDATION SPLIT CREATOR\n",
      "2026-01-02 04:39:25,504 | INFO | ============================================================\n",
      "2026-01-02 04:39:25,504 | INFO | \n",
      "============================================================\n",
      "2026-01-02 04:39:25,504 | INFO | ANALYZING TRAINING DATA\n",
      "2026-01-02 04:39:25,504 | INFO | ============================================================\n",
      "2026-01-02 04:39:25,504 | INFO | \n",
      "ðŸ” Scanning content images...\n",
      "Content images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 599/599 [00:00<00:00, 482687.43img/s]\n",
      "2026-01-02 04:39:25,509 | INFO |   âœ“ Found 599 content images\n",
      "2026-01-02 04:39:25,510 | INFO | \n",
      "ðŸ” Scanning target images...\n",
      "Styles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 343.89style/s]\n",
      "2026-01-02 04:39:25,554 | INFO |   âœ“ Found 8985 valid target images\n",
      "2026-01-02 04:39:25,554 | INFO | \n",
      "ðŸ” Validating content â†” target pairs...\n",
      "Validating pairs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8985/8985 [00:00<00:00, 2455902.34pair/s]\n",
      "2026-01-02 04:39:25,559 | INFO | ============================================================\n",
      "2026-01-02 04:39:25,560 | INFO | ðŸ“Š DATA ANALYSIS SUMMARY\n",
      "2026-01-02 04:39:25,560 | INFO | ============================================================\n",
      "2026-01-02 04:39:25,560 | INFO | Content images found:        599\n",
      "2026-01-02 04:39:25,560 | INFO | Target images scanned:       8,985\n",
      "2026-01-02 04:39:25,560 | INFO |   â”œâ”€ Parse errors:          0\n",
      "2026-01-02 04:39:25,560 | INFO |   â””â”€ Style mismatches:      0\n",
      "2026-01-02 04:39:25,560 | INFO | Target images after filter:  8,985\n",
      "2026-01-02 04:39:25,560 | INFO | Missing content images:      0\n",
      "2026-01-02 04:39:25,560 | INFO | Final valid pairs:           8,985\n",
      "2026-01-02 04:39:25,560 | INFO | ============================================================\n",
      "2026-01-02 04:39:25,560 | INFO | \n",
      "============================================================\n",
      "2026-01-02 04:39:25,560 | INFO | CREATING TRAIN/VAL SPLITS (random char & style)\n",
      "2026-01-02 04:39:25,560 | INFO | ============================================================\n",
      "2026-01-02 04:39:25,561 | INFO | \n",
      "ðŸ“Š Split Statistics:\n",
      "2026-01-02 04:39:25,561 | INFO |   Total chars: 599 â†’ train: 480, val: 119\n",
      "2026-01-02 04:39:25,561 | INFO |   Total styles: 15 â†’ train: 12, val: 3\n",
      "2026-01-02 04:39:25,561 | INFO |   train:\n",
      "2026-01-02 04:39:25,561 | INFO |     Chars: 480\n",
      "2026-01-02 04:39:25,562 | INFO |     Styles: 12\n",
      "2026-01-02 04:39:25,562 | INFO |   val:\n",
      "2026-01-02 04:39:25,562 | INFO |     Chars: 119\n",
      "2026-01-02 04:39:25,562 | INFO |     Styles: 3\n",
      "2026-01-02 04:39:25,562 | INFO | \n",
      "ðŸ“ CREATING TRAIN SPLIT...\n",
      "2026-01-02 04:39:25,562 | INFO |   ðŸ“¥ Copying content images for train...\n",
      "2026-01-02 04:39:25,684 | INFO |   ðŸ“¥ Copying target images for train...        \n",
      "2026-01-02 04:39:27,113 | INFO |   âœ“ train: 480 content, 5,760 target (skipped: 0)\n",
      "2026-01-02 04:39:27,113 | INFO |   ðŸ“‹ Filtering checkpoint for train...\n",
      "2026-01-02 04:39:27,227 | INFO |     âœ“ Saved: 5,760/8,985 generations           \n",
      "2026-01-02 04:39:27,229 | INFO | ðŸ“ CREATING VAL SPLIT...\n",
      "2026-01-02 04:39:27,229 | INFO |   ðŸ“¥ Copying content images for val...\n",
      "2026-01-02 04:39:27,256 | INFO |   ðŸ“¥ Copying target images for val...          \n",
      "2026-01-02 04:39:27,356 | INFO |   âœ“ val: 119 content, 357 target (skipped: 0)  \n",
      "2026-01-02 04:39:27,356 | INFO |   ðŸ“‹ Filtering checkpoint for val...\n",
      "2026-01-02 04:39:27,408 | INFO |     âœ“ Saved: 357/8,985 generations             \n",
      "2026-01-02 04:39:27,410 | INFO | âœ“ Saved split metadata to my_dataset/split_info.json\n",
      "2026-01-02 04:39:27,412 | INFO | \n",
      "============================================================\n",
      "2026-01-02 04:39:27,412 | INFO | âœ“ SPLIT CREATION COMPLETE\n",
      "2026-01-02 04:39:27,412 | INFO | ============================================================\n",
      "2026-01-02 04:39:27,412 | INFO | \n",
      "âœ… Created:\n",
      "2026-01-02 04:39:27,412 | INFO |   ðŸ“ train/\n",
      "2026-01-02 04:39:27,412 | INFO |     â”œâ”€â”€ ContentImage/ (training chars)\n",
      "2026-01-02 04:39:27,412 | INFO |     â”œâ”€â”€ TargetImage/ (training styles)\n",
      "2026-01-02 04:39:27,412 | INFO |     â””â”€â”€ results_checkpoint.json (filtered)\n",
      "2026-01-02 04:39:27,412 | INFO |   ðŸ“ val/\n",
      "2026-01-02 04:39:27,412 | INFO |     â”œâ”€â”€ ContentImage/ (validation chars)\n",
      "2026-01-02 04:39:27,412 | INFO |     â”œâ”€â”€ TargetImage/ (validation styles)\n",
      "2026-01-02 04:39:27,412 | INFO |     â””â”€â”€ results_checkpoint.json (filtered)\n",
      "2026-01-02 04:39:27,412 | INFO | \n",
      "ðŸ’¡ Guarantees:\n",
      "2026-01-02 04:39:27,412 | INFO |   âœ“ Every target has matching content\n",
      "2026-01-02 04:39:27,412 | INFO |   âœ“ Checkpoint contains only relevant generations\n",
      "2026-01-02 04:39:27,412 | INFO |   âœ“ Train and val are completely disjoint\n"
     ]
    }
   ],
   "source": [
    "!python FontDiffusion/create_validation_split.py \\\n",
    "  --data_root my_dataset \\\n",
    "  --val_ratio 0.2 \\\n",
    "  --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79508d80-fac1-4318-9174-a32613a557e3",
   "metadata": {
    "id": "79508d80-fac1-4318-9174-a32613a557e3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!uv pip install --upgrade pyarrow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f97e84-cd8c-49a9-86bd-fce456be56a4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# remove_unparseable_files.py\n",
    "\n",
    "with open(\"my_dataset/unparseable_files.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    paths = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "import os\n",
    "\n",
    "for path in paths:\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "            print(f\"Deleted: {path}\")\n",
    "        else:\n",
    "            print(f\"Not found: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vRL8QovYCvLY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-02T04:39:37.656640Z",
     "iopub.status.busy": "2026-01-02T04:39:37.656344Z",
     "iopub.status.idle": "2026-01-02T04:40:55.252069Z",
     "shell.execute_reply": "2026-01-02T04:40:55.251050Z",
     "shell.execute_reply.started": "2026-01-02T04:39:37.656615Z"
    },
    "id": "vRL8QovYCvLY",
    "outputId": "08301c52-4ae1-4268-c516-2ff8bd834783",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FONTDIFFUSION DATASET CREATOR\n",
      "============================================================\n",
      "\n",
      "Data dir: my_dataset/train_original\n",
      "Repo: dzungpham/font-diffusion-generated-data\n",
      "Push to Hub: True\n",
      "âœ“ Validated directory structure\n",
      "  Content images: my_dataset/train_original/ContentImage\n",
      "  Target images: my_dataset/train_original/TargetImage\n",
      "  Results checkpoint: my_dataset/train_original/results_checkpoint.json\n",
      "\n",
      "============================================================\n",
      "BUILDING DATASET\n",
      "============================================================\n",
      "\n",
      "âœ“ Loaded results_checkpoint.json\n",
      "  Generations: 8985\n",
      "  Characters: 599\n",
      "  Styles: 15\n",
      "\n",
      "ðŸ–¼ï¸  Loading 8985 image pairs...\n",
      "Loading image pairs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.98k/8.98k [00:07<00:00, 1.18kpair/s]\n",
      "âœ“ Loaded 8985 samples\n",
      "\n",
      "============================================================\n",
      "PUSHING TO HUB\n",
      "============================================================\n",
      "Repository: dzungpham/font-diffusion-generated-data\n",
      "Split: train_original\n",
      "Uploading the dataset shards:   0%|                  | 0/1 [00:00<?, ? shards/s]\n",
      "Map:   0%|                                      | 0/8985 [00:00<?, ? examples/s]\u001b[A\n",
      "Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 6601/8985 [00:00<00:00, 17523.15 examples/s]\u001b[A\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8985/8985 [00:00<00:00, 15307.79 examples/s]\u001b[A\n",
      "\n",
      "Creating parquet from Arrow format:   0%|                 | 0/2 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  9.89ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.25s/ shards]\n",
      "\n",
      "âœ“ Successfully pushed to Hub!\n",
      "  URL: https://huggingface.co/datasets/dzungpham/font-diffusion-generated-data\n",
      "\n",
      "âœ… COMPLETE!\n",
      "\n",
      "============================================================\n",
      "FONTDIFFUSION DATASET CREATOR\n",
      "============================================================\n",
      "\n",
      "Data dir: my_dataset/train\n",
      "Repo: dzungpham/font-diffusion-generated-data\n",
      "Push to Hub: True\n",
      "âœ“ Validated directory structure\n",
      "  Content images: my_dataset/train/ContentImage\n",
      "  Target images: my_dataset/train/TargetImage\n",
      "  Results checkpoint: my_dataset/train/results_checkpoint.json\n",
      "\n",
      "============================================================\n",
      "BUILDING DATASET\n",
      "============================================================\n",
      "\n",
      "âœ“ Loaded results_checkpoint.json\n",
      "  Generations: 5760\n",
      "  Characters: 480\n",
      "  Styles: 12\n",
      "\n",
      "ðŸ–¼ï¸  Loading 5760 image pairs...\n",
      "Loading image pairs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.76k/5.76k [00:04<00:00, 1.16kpair/s]\n",
      "âœ“ Loaded 5760 samples\n",
      "\n",
      "============================================================\n",
      "PUSHING TO HUB\n",
      "============================================================\n",
      "Repository: dzungpham/font-diffusion-generated-data\n",
      "Split: train\n",
      "Uploading the dataset shards:   0%|                  | 0/1 [00:00<?, ? shards/s]\n",
      "Map:   0%|                                      | 0/5760 [00:00<?, ? examples/s]\u001b[A\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5760/5760 [00:00<00:00, 14851.57 examples/s]\u001b[A\n",
      "\n",
      "Creating parquet from Arrow format:   0%|                 | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.85ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.09 shards/s]\n",
      "README.md: 3.05kB [00:00, 12.0MB/s]\n",
      "\n",
      "âœ“ Successfully pushed to Hub!\n",
      "  URL: https://huggingface.co/datasets/dzungpham/font-diffusion-generated-data\n",
      "\n",
      "âœ… COMPLETE!\n",
      "\n",
      "============================================================\n",
      "FONTDIFFUSION DATASET CREATOR\n",
      "============================================================\n",
      "\n",
      "Data dir: my_dataset/val\n",
      "Repo: dzungpham/font-diffusion-generated-data\n",
      "Push to Hub: True\n",
      "âœ“ Validated directory structure\n",
      "  Content images: my_dataset/val/ContentImage\n",
      "  Target images: my_dataset/val/TargetImage\n",
      "  Results checkpoint: my_dataset/val/results_checkpoint.json\n",
      "\n",
      "============================================================\n",
      "BUILDING DATASET\n",
      "============================================================\n",
      "\n",
      "âœ“ Loaded results_checkpoint.json\n",
      "  Generations: 357\n",
      "  Characters: 119\n",
      "  Styles: 3\n",
      "\n",
      "ðŸ–¼ï¸  Loading 357 image pairs...\n",
      "Loading image pairs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 357/357 [00:00<00:00, 1.20kpair/s]\n",
      "âœ“ Loaded 357 samples\n",
      "\n",
      "============================================================\n",
      "PUSHING TO HUB\n",
      "============================================================\n",
      "Repository: dzungpham/font-diffusion-generated-data\n",
      "Split: val\n",
      "Uploading the dataset shards:   0%|                  | 0/1 [00:00<?, ? shards/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 357/357 [00:00<00:00, 19035.45 examples/s]\u001b[A\n",
      "\n",
      "Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 80.84ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:11<00:00, 11.31s/ shards]\n",
      "README.md: 3.05kB [00:00, 13.0MB/s]\n",
      "\n",
      "âœ“ Successfully pushed to Hub!\n",
      "  URL: https://huggingface.co/datasets/dzungpham/font-diffusion-generated-data\n",
      "\n",
      "âœ… COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "HF_USERNAME = \"dzungpham\"\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/train_original\" \\\n",
    "  --repo_id dzungpham/font-diffusion-generated-data \\\n",
    "  --split \"train_original\" \\\n",
    "  --token {HF_TOKEN}\n",
    "\n",
    "# Train Split\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/train\" \\\n",
    "  --repo_id dzungpham/font-diffusion-generated-data \\\n",
    "  --split \"train\" \\\n",
    "  --token {HF_TOKEN}\n",
    "\n",
    "# Train Split\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/val\" \\\n",
    "  --repo_id dzungpham/font-diffusion-generated-data \\\n",
    "  --split \"val\" \\\n",
    "  --token {HF_TOKEN}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87caab2",
   "metadata": {
    "id": "a87caab2",
    "papermill": {
     "duration": 1.992585,
     "end_time": "2025-12-30T18:55:24.769269",
     "exception": false,
     "start_time": "2025-12-30T18:55:22.776684",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267634e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T04:43:59.401482Z",
     "iopub.status.busy": "2026-01-02T04:43:59.401134Z",
     "iopub.status.idle": "2026-01-02T04:47:50.896613Z",
     "shell.execute_reply": "2026-01-02T04:47:50.895674Z",
     "shell.execute_reply.started": "2026-01-02T04:43:59.401454Z"
    },
    "id": "267634e8",
    "papermill": {
     "duration": 0.021927,
     "end_time": "2025-12-30T18:55:24.807644",
     "exception": false,
     "start_time": "2025-12-30T18:55:24.785717",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m12 packages\u001b[0m \u001b[2min 78ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2mAudited \u001b[1m12 packages\u001b[0m \u001b[2min 0.12ms\u001b[0m\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `2`\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.11/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.11/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2026-01-02 04:44:09.202585: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-02 04:44:09.202600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767329049.225566    1532 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767329049.225569    1531 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767329049.233230    1531 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1767329049.233268    1532 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.13)pygame 2.6.1 (SDL 2.28.4, Python 3.11.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Load the down block  DownBlock2D\n",
      "Load the down block  DownBlock2D\n",
      "Load the down block  MCADownBlock2D\n",
      "Load the down block  MCADownBlock2D\n",
      "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
      "Load the down block  MCADownBlock2D\n",
      "Load the down block  MCADownBlock2D\n",
      "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
      "Load the down block  DownBlock2D\n",
      "Load the down block  DownBlock2D\n",
      "Load the up block  UpBlock2D\n",
      "Load the up block  UpBlock2D\n",
      "Load the up block  StyleRSIUpBlock2D\n",
      "Load the up block  StyleRSIUpBlock2D\n",
      "Load the up block  StyleRSIUpBlock2D\n",
      "Load the up block  UpBlock2D\n",
      "Load the up block  StyleRSIUpBlock2D\n",
      "Load the up block  UpBlock2D\n",
      "Param count for Ds initialized parameters: 20591296\n",
      "Get CG-GAN Style Encoder!\n",
      "Param count for Ds initialized parameters: 1187008\n",
      "Get CG-GAN Content Encoder!\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Param count for Ds initialized parameters: 20591296\n",
      "Get CG-GAN Style Encoder!\n",
      "Param count for Ds initialized parameters: 1187008\n",
      "Get CG-GAN Content Encoder!\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdungngocpham171\u001b[0m (\u001b[33mdungngocpham171-university-of-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20260102_044419-e28gq05h\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdandy-butterfly-21\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dungngocpham171-university-of-science/FontDiffuser_training_phase_1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dungngocpham171-university-of-science/FontDiffuser_training_phase_1/runs/e28gq05h\u001b[0m\n",
      "Steps:   0%|                                            | 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/diffusers/configuration_utils.py:134: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "/usr/local/lib/python3.11/dist-packages/diffusers/configuration_utils.py:134: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "/kaggle/working/FontDiffusion/src/model.py:34: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.style_encoder'.\n",
      "  style_img_feature, _, _ = self.style_encoder(style_images)\n",
      "/kaggle/working/FontDiffusion/src/model.py:34: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.style_encoder'.\n",
      "  style_img_feature, _, _ = self.style_encoder(style_images)\n",
      "/kaggle/working/FontDiffusion/src/model.py:42: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.content_encoder'.\n",
      "  content_img_feature, content_residual_features = self.content_encoder(\n",
      "/kaggle/working/FontDiffusion/src/model.py:42: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.content_encoder'.\n",
      "  content_img_feature, content_residual_features = self.content_encoder(\n",
      "/kaggle/working/FontDiffusion/src/model.py:47: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.content_encoder'.\n",
      "  style_content_feature, style_content_res_features = self.content_encoder(\n",
      "/kaggle/working/FontDiffusion/src/model.py:47: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.content_encoder'.\n",
      "  style_content_feature, style_content_res_features = self.content_encoder(\n",
      "/kaggle/working/FontDiffusion/src/model.py:59: FutureWarning: Accessing config attribute `unet` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'unet' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.unet'.\n",
      "  out = self.unet(\n",
      "/kaggle/working/FontDiffusion/src/model.py:59: FutureWarning: Accessing config attribute `unet` directly via 'FontDiffuserModel' object attribute is deprecated. Please access 'unet' over 'FontDiffuserModel's config object instead, e.g. 'unet.config.unet'.\n",
      "  out = self.unet(\n",
      "Steps:   0%|                       | 0/200 [00:01<?, ?it/s, lr=0, step_loss=5.7]/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:841: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [128, 128, 1, 1], strides() = [128, 1, 128, 128]\n",
      "bucket_view.sizes() = [128, 128, 1, 1], strides() = [128, 1, 1, 1] (Triggered internally at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:334.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:841: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [128, 128, 1, 1], strides() = [128, 1, 128, 128]\n",
      "bucket_view.sizes() = [128, 128, 1, 1], strides() = [128, 1, 1, 1] (Triggered internally at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:334.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Steps:  50%|â–ˆâ–ˆâ–ˆâ–Œ   | 100/200 [03:23<03:38,  2.19s/it, lr=9.9e-7, step_loss=4.36]Traceback (most recent call last):\n",
      "  File \"/kaggle/working/FontDiffusion/train.py\", line 348, in <module>\n",
      "    main()\n",
      "  File \"/kaggle/working/FontDiffusion/train.py\", line 313, in main\n",
      "    torch.save(model.unet.state_dict(), f\"{save_dir}/unet.pth\")\n",
      "               ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1964, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'DistributedDataParallel' object has no attribute 'unet'\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/kaggle/working/FontDiffusion/train.py\", line 348, in <module>\n",
      "[rank0]:     main()\n",
      "[rank0]:   File \"/kaggle/working/FontDiffusion/train.py\", line 313, in main\n",
      "[rank0]:     torch.save(model.unet.state_dict(), f\"{save_dir}/unet.pth\")\n",
      "[rank0]:                ^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1964, in __getattr__\n",
      "[rank0]:     raise AttributeError(\n",
      "[rank0]: AttributeError: 'DistributedDataParallel' object has no attribute 'unet'\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mdandy-butterfly-21\u001b[0m at: \u001b[34mhttps://wandb.ai/dungngocpham171-university-of-science/FontDiffuser_training_phase_1/runs/e28gq05h\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20260102_044419-e28gq05h/logs\u001b[0m\n",
      "W0102 04:47:49.693000 1523 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 1532 closing signal SIGTERM\n",
      "E0102 04:47:49.907000 1523 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 0 (pid: 1531) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 977, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 646, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "FontDiffusion/train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2026-01-02_04:47:49\n",
      "  host      : 0e1df859923c\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1531)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# TRAINING PHASE 1\n",
    "if is_colab:\n",
    "  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\n",
    "else:\n",
    "  !uv pip install --upgrade \"huggingface-hub==0.25.2\"\n",
    "import wandb\n",
    "\n",
    "MAX_TRAIN_STEPS = 1000 # @param {type:\"integer\"}\n",
    "!accelerate launch FontDiffusion/train.py \\\n",
    "    --seed=123 \\\n",
    "    --experience_name=\"FontDiffuser_training_phase_1\" \\\n",
    "    --data_root=\"my_dataset\" \\\n",
    "    --output_dir=\"outputs/FontDiffuser\" \\\n",
    "    --report_to=\"wandb\" \\\n",
    "      \\\n",
    "    --resolution=96 \\\n",
    "    --style_image_size=96 \\\n",
    "    --content_image_size=96 \\\n",
    "    --content_encoder_downsample_size=3 \\\n",
    "    --channel_attn=True \\\n",
    "    --content_start_channel=64 \\\n",
    "    --style_start_channel=64 \\\n",
    "      \\\n",
    "    --train_batch_size=8 \\\n",
    "    --gradient_accumulation_steps=2 \\\n",
    "    --perceptual_coefficient=0.05 \\\n",
    "    --offset_coefficient=0.7 \\\n",
    "    --max_train_steps=${MAX_TRAIN_STEPS} \\\n",
    "    --ckpt_interval=${MAX_TRAIN_STEPS // 2} \\\n",
    "    --log_interval=50 \\\n",
    "      \\\n",
    "    --learning_rate=1e-4 \\\n",
    "    --lr_scheduler=\"linear\" \\\n",
    "    --lr_warmup_steps=10000 \\\n",
    "    --drop_prob=0.1 \\\n",
    "    --mixed_precision=\"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a3326-6bfa-4fba-bbcc-e2e7be996c7f",
   "metadata": {
    "id": "cb2a3326-6bfa-4fba-bbcc-e2e7be996c7f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls -lr outputs/FontDiffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8136e",
   "metadata": {
    "id": "97f8136e",
    "papermill": {
     "duration": 0.022471,
     "end_time": "2025-12-30T18:55:24.845778",
     "exception": false,
     "start_time": "2025-12-30T18:55:24.823307",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TRAINING PHASE 2\n",
    "!wandb login\n",
    "\n",
    "MAX_TRAIN_STEPS = 1000 # @param {type:\"integer\"}\n",
    "!python FontDiffusion/my_train.py \\\n",
    "    --seed=123 \\\n",
    "    --experience_name=\"FontDiffuser_training_phase_2\" \\\n",
    "    --data_root=\"my_dataset\" \\\n",
    "    --output_dir=\"outputs/FontDiffuser\" \\\n",
    "    --report_to=\"wandb\" \\\n",
    "    --phase_2 \\\n",
    "    --phase_1_ckpt_dir=\"outputs/FontDiffuser/global_step_2000\" \\\n",
    "    --scr_ckpt_path=\"ckpt/scr_210000.pth\" \\\n",
    "    \\\n",
    "    --sc_coefficient=0.05 \\\n",
    "    --num_neg=13 \\\n",
    "    --resolution=96 \\\n",
    "    --style_image_size=96 \\\n",
    "    --content_image_size=96 \\\n",
    "    --content_encoder_downsample_size=3 \\\n",
    "    --channel_attn=True \\\n",
    "    --content_start_channel=64 \\\n",
    "    --style_start_channel=64 \\\n",
    "    \\\n",
    "    --train_batch_size=8 \\\n",
    "    --gradient_accumulation_steps=2 \\\n",
    "    --perceptual_coefficient=0.05 \\\n",
    "    --offset_coefficient=0.7 \\\n",
    "    --max_train_steps=${MAX_TRAIN_STEPS} \\\n",
    "    --ckpt_interval=${MAX_TRAIN_STEPS // 2} \\\n",
    "    --log_interval=50 \\\n",
    "    \\\n",
    "    --learning_rate=1e-5 \\\n",
    "    --lr_scheduler=\"constant\" \\\n",
    "    --lr_warmup_steps=1000 \\\n",
    "    --drop_prob=0.1 \\\n",
    "    --mixed_precision=\"no\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c45e2f",
   "metadata": {
    "id": "88c45e2f",
    "papermill": {
     "duration": 0.217876,
     "end_time": "2025-12-30T18:55:25.079820",
     "exception": false,
     "start_time": "2025-12-30T18:55:24.861944",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python FontDiffusion/upload_models.py \\\n",
    "    --weights_dir \"outputs/FontDiffuser\" \\\n",
    "    --repo_id \"dzungpham/font-diffusion-weights\" \\\n",
    "    --token \"{HF_TOKEN}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868b20b",
   "metadata": {
    "id": "5868b20b",
    "papermill": {
     "duration": 0.031197,
     "end_time": "2025-12-30T18:55:25.126961",
     "exception": false,
     "start_time": "2025-12-30T18:55:25.095764",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "def find_result_folders(base_path: Path, pattern_name: str) -> List[Path]:\n",
    "    return [p for p in base_path.glob(pattern_name) if p.is_dir()]\n",
    "\n",
    "def zip_folder(folder_path: Path, output_base_path: Path) -> bool:\n",
    "    folder_name = folder_path.name\n",
    "    zip_path = output_base_path / f\"{folder_name}.zip\"\n",
    "    try:\n",
    "        print(f\"   -> Zipping folder: {folder_name}...\")\n",
    "        with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for file_path in folder_path.rglob(\"*\"):\n",
    "                if file_path.is_file():\n",
    "                    arcname = file_path.relative_to(folder_path.parent)\n",
    "                    zipf.write(file_path, arcname)\n",
    "        print(f\"   âœ… Created ZIP: {zip_path.name}\")\n",
    "        return True\n",
    "    except Exception as exc:\n",
    "        print(f\"   âŒ Failed to zip {folder_name}: {exc}\")\n",
    "        return False\n",
    "\n",
    "def zip_stats_results_folders(output_base_path: str, pattern_name: str) -> None:\n",
    "    base = Path(output_base_path)\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "    result_folders = find_result_folders(base, pattern_name)\n",
    "    if not result_folders:\n",
    "        print(f\"âš ï¸ No folders matching '*dataset' found in '{output_base_path}'.\")\n",
    "        return\n",
    "    print(f\"ðŸ” Found {len(result_folders)} result folder(s) to zip.\")\n",
    "    successful = sum(1 for folder in result_folders if zip_folder(folder, base))\n",
    "    print(f\"\\nâœ… DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n",
    "        if not output_root:\n",
    "            raise ValueError(\"OUTPUT_PATH not defined\")\n",
    "        zip_stats_results_folders(\n",
    "            output_base_path=OUTPUT_PATH,\n",
    "            pattern_name=\"my_dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 127.636509,
   "end_time": "2025-12-30T18:55:25.961447",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-30T18:53:18.324938",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
