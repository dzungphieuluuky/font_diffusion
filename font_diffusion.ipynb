{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-28T10:41:44.379383Z",
          "iopub.status.busy": "2025-12-28T10:41:44.378903Z",
          "iopub.status.idle": "2025-12-28T10:42:10.309903Z",
          "shell.execute_reply": "2025-12-28T10:42:10.308650Z",
          "shell.execute_reply.started": "2025-12-28T10:41:44.379353Z"
        },
        "id": "BWFvN9XJxf9K",
        "outputId": "e84cb626-aa12-458d-e864-6f542195078a",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MPLBACKEND environment variable cleared.\n",
            "Cloning into 'FontDiffusion'...\n",
            "remote: Enumerating objects: 20000, done.\u001b[K\n",
            "remote: Counting objects: 100% (4812/4812), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4810/4810), done.\u001b[K\n",
            "remote: Total 20000 (delta 3), reused 4809 (delta 2), pack-reused 15188 (from 3)\u001b[K\n",
            "Receiving objects: 100% (20000/20000), 277.04 MiB | 30.64 MiB/s, done.\n",
            "Resolving deltas: 100% (566/566), done.\n",
            "Updating files: 100% (4917/4917), done.\n"
          ]
        }
      ],
      "source": [
        "# @title Environment Setup\n",
        "import os\n",
        "import sys\n",
        "if 'MPLBACKEND' in os.environ:\n",
        "    del os.environ['MPLBACKEND']\n",
        "    print(\"MPLBACKEND environment variable cleared.\")\n",
        "\n",
        "# 2. Clone the repository\n",
        "!rm -rf FontDiffusion\n",
        "!git clone https://github.com/dzungphieuluuky/FontDiffusion.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-28T10:29:48.372873Z",
          "iopub.status.busy": "2025-12-28T10:29:48.372367Z",
          "iopub.status.idle": "2025-12-28T10:29:48.380165Z",
          "shell.execute_reply": "2025-12-28T10:29:48.379601Z",
          "shell.execute_reply.started": "2025-12-28T10:29:48.372846Z"
        },
        "id": "sxdyquWfaqdm",
        "outputId": "0027a1db-7720-429c-f0e5-4c9226fe6e7a",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Environment: Google Colab\n",
            "üìÇ Data Path: /content/\n",
            "üì¶ Output Path: /content/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from IPython import get_ipython\n",
        "from typing import Optional\n",
        "\n",
        "def configure_environment_paths():\n",
        "    \"\"\"Detect environment and configure paths\"\"\"\n",
        "    try:\n",
        "        if \"google.colab\" in sys.modules:\n",
        "            print(\"‚úÖ Environment: Google Colab\")\n",
        "            base_data_path = \"/content/\"\n",
        "            base_output_path = \"/content/\"\n",
        "            environment_name = \"colab\"\n",
        "        elif 'kaggle_secrets' in sys.modules:\n",
        "            print(\"‚úÖ Environment: Kaggle\")\n",
        "            base_data_path = \"/kaggle/input/\"\n",
        "            base_output_path = \"/kaggle/working/\"\n",
        "            environment_name = \"kaggle\"\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Environment: Local/Unknown\")\n",
        "            base_data_path = \"./data/\"\n",
        "            base_output_path = \"./output/\"\n",
        "            environment_name = \"local\"\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è Non-interactive session. Using local paths.\")\n",
        "        base_data_path = \"./data/\"\n",
        "        base_output_path = \"./output/\"\n",
        "        environment_name = \"local\"\n",
        "\n",
        "    os.makedirs(base_output_path, exist_ok=True)\n",
        "    print(f\"üìÇ Data Path: {base_data_path}\")\n",
        "    print(f\"üì¶ Output Path: {base_output_path}\")\n",
        "    return base_data_path, base_output_path, environment_name\n",
        "def load_secret(key_name: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Loads a secret key from the appropriate environment (Colab, Kaggle, or local env vars).\n",
        "\n",
        "    Args:\n",
        "        key_name (str): The name of the secret key to load (e.g., \"WANDB_API_KEY\", \"HF_TOKEN\").\n",
        "\n",
        "    Returns:\n",
        "        Optional[str]: The secret key value if found, otherwise None.\n",
        "    \"\"\"\n",
        "    env = ENV_NAME\n",
        "    secret_value = None\n",
        "\n",
        "    print(f\"Attempting to load secret '{key_name}' from '{env}' environment...\")\n",
        "\n",
        "    try:\n",
        "        if env == \"colab\":\n",
        "            from google.colab import userdata\n",
        "            secret_value = userdata.get(key_name)\n",
        "        elif env == \"kaggle\":\n",
        "            from kaggle_secrets import UserSecretsClient\n",
        "            user_secrets = UserSecretsClient()\n",
        "            secret_value = user_secrets.get_secret(key_name)\n",
        "        else: # Local environment\n",
        "            secret_value = os.getenv(key_name)\n",
        "\n",
        "        if not secret_value:\n",
        "            print(f\"‚ö†Ô∏è Secret '{key_name}' not found in the {env} environment.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"‚úÖ Successfully loaded secret '{key_name}'.\")\n",
        "        return secret_value\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred while loading secret '{key_name}': {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "INPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET_mqyek9bwj",
        "outputId": "7e80d9bf-3c1a-422d-c1bd-c41cfc427d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 41ms\u001b[0m\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 0.16ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m116 packages\u001b[0m \u001b[2min 274ms\u001b[0m\u001b[0m\n",
            "\u001b[2K  \u001b[31m√ó\u001b[0m Failed to build `tokenizers==0.13.3`\n",
            "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mThe build backend returned an error\n",
            "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mCall to `setuptools.build_meta.build_wheel` failed (exit status: 1)\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[31m[stdout]\u001b[39m\n",
            "\u001b[31m      \u001b[0mrunning bdist_wheel\n",
            "\u001b[31m      \u001b[0mrunning build\n",
            "\u001b[31m      \u001b[0mrunning build_py\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/byte_level_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_unigram.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/char_level_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/base_tokenizer.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/bert_wordpiece.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer-styles.css ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mrunning build_ext\n",
            "\u001b[31m      \u001b[0mrunning build_rust\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
            "\u001b[31m      \u001b[0m/root/.cache/uv/builds-v0/.tmpsp86wS/lib/python3.12/site-packages/setuptools/dist.py:759:\n",
            "\u001b[31m      \u001b[0mSetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "\u001b[31m      \u001b[0m!!\n",
            "\n",
            "\u001b[31m      \u001b[0m\n",
            "\u001b[31m      \u001b[0m********************************************************************************\n",
            "\u001b[31m      \u001b[0m        Please consider removing the following classifiers in favor of a\n",
            "\u001b[31m      \u001b[0mSPDX license expression:\n",
            "\n",
            "\u001b[31m      \u001b[0m        License :: OSI Approved :: Apache Software License\n",
            "\n",
            "\u001b[31m      \u001b[0m        See\n",
            "\u001b[31m      \u001b[0mhttps://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license\n",
            "\u001b[31m      \u001b[0mfor details.\n",
            "\u001b[31m      \u001b[0m\n",
            "\u001b[31m      \u001b[0m********************************************************************************\n",
            "\n",
            "\u001b[31m      \u001b[0m!!\n",
            "\u001b[31m      \u001b[0m  self._finalize_license_expression()\n",
            "\u001b[31m      \u001b[0merror: can't find Rust compiler\n",
            "\n",
            "\u001b[31m      \u001b[0mIf you are using an outdated pip version, it is possible a prebuilt\n",
            "\u001b[31m      \u001b[0mwheel is available for this package but pip is not able to install from\n",
            "\u001b[31m      \u001b[0mit. Installing from the wheel would avoid the need for a Rust compiler.\n",
            "\n",
            "\u001b[31m      \u001b[0mTo update pip, run:\n",
            "\n",
            "\u001b[31m      \u001b[0m    pip install --upgrade pip\n",
            "\n",
            "\u001b[31m      \u001b[0mand then retry package installation.\n",
            "\n",
            "\u001b[31m      \u001b[0mIf you did intend to build this package from source, try installing\n",
            "\u001b[31m      \u001b[0ma Rust compiler from your system package manager and ensure it is\n",
            "\u001b[31m      \u001b[0mon the PATH during installation. Alternatively, rustup (available at\n",
            "\u001b[31m      \u001b[0mhttps://rustup.rs) is the recommended way to download and update the\n",
            "\u001b[31m      \u001b[0mRust compiler toolchain.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This usually indicates a problem with the package or the build\n",
            "\u001b[31m      \u001b[0menvironment.\n",
            "\u001b[36m  help: \u001b[0m`\u001b[36mtokenizers\u001b[39m` (\u001b[36mv0.13.3\u001b[39m) was included because `\u001b[36mtransformers\u001b[39m` (\u001b[36mv4.33.1\u001b[39m)\n",
            "        depends on `\u001b[36mtokenizers\u001b[39m`\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 85ms\u001b[0m\u001b[0m\n",
            "/content\n",
            "\n",
            "‚¨áÔ∏è Installing PyTorch 1.13 (Required for this model)...\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 245ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.1\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K  \u001b[31m√ó\u001b[0m No solution found when resolving dependencies:\n",
            "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mBecause torch==1.13.1+cu117 has no wheels with a matching Python ABI\n",
            "\u001b[31m      \u001b[0mtag (e.g., `\u001b[36mcp312\u001b[39m`) and you require torch==1.13.1+cu117, we can conclude\n",
            "\u001b[31m      \u001b[0mthat your requirements are unsatisfiable.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m `\u001b[36mtorch\u001b[39m` was found on \u001b[36mhttps://download.pytorch.org/whl/cu117\u001b[39m, but\n",
            "\u001b[31m      \u001b[0mnot at the requested version (\u001b[36mtorch==1.13.1+cu117\u001b[39m). A compatible version\n",
            "\u001b[31m      \u001b[0mmay be available on a subsequent index (e.g., \u001b[36mhttps://pypi.org/simple\u001b[39m).\n",
            "\u001b[31m      \u001b[0mBy default, uv will only consider versions that are published on the\n",
            "\u001b[31m      \u001b[0mfirst index that contains a given package, to avoid dependency confusion\n",
            "\u001b[31m      \u001b[0mattacks. If all indexes are equally trusted, use `\u001b[32m--index-strategy\n",
            "\u001b[31m      \u001b[0munsafe-best-match\u001b[39m` to consider all versions from all indexes, regardless\n",
            "\u001b[31m      \u001b[0mof the order in which they were defined.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m You require \u001b[36mCPython 3.12\u001b[39m (`\u001b[36mcp312\u001b[39m`), but we only found wheels for\n",
            "\u001b[31m      \u001b[0m`\u001b[36mtorch\u001b[39m` (\u001b[36mv1.13.1+cu117\u001b[39m) with the following Python ABI tags: `\u001b[36mcp37m\u001b[39m`,\n",
            "\u001b[31m      \u001b[0m`\u001b[36mcp38\u001b[39m`, `\u001b[36mcp39\u001b[39m`, `\u001b[36mcp310\u001b[39m`, `\u001b[36mcp311\u001b[39m`\n",
            "\n",
            "‚¨áÔ∏è Installing Dependencies (Manually fixed)...\n",
            "  \u001b[31m√ó\u001b[0m No solution found when resolving dependencies:\n",
            "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mBecause torch==1.13.1 has no wheels with a matching Python ABI tag\n",
            "\u001b[31m      \u001b[0m(e.g., `\u001b[36mcp312\u001b[39m`) and xformers==0.0.16 depends on torch==1.13.1, we can\n",
            "\u001b[31m      \u001b[0mconclude that xformers==0.0.16 cannot be used.\n",
            "\u001b[31m      \u001b[0mAnd because you require xformers==0.0.16, we can conclude that your\n",
            "\u001b[31m      \u001b[0mrequirements are unsatisfiable.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m You require \u001b[36mCPython 3.12\u001b[39m (`\u001b[36mcp312\u001b[39m`), but we only found wheels for\n",
            "\u001b[31m      \u001b[0m`\u001b[36mtorch\u001b[39m` (\u001b[36mv1.13.1\u001b[39m) with the following Python ABI tags: `\u001b[36mcp37m\u001b[39m`, `\u001b[36mcp38\u001b[39m`,\n",
            "\u001b[31m      \u001b[0m`\u001b[36mcp39\u001b[39m`, `\u001b[36mcp310\u001b[39m`, `\u001b[36mcp311\u001b[39m`\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m47 packages\u001b[0m \u001b[2min 28ms\u001b[0m\u001b[0m\n",
            "\u001b[2K  \u001b[31m√ó\u001b[0m Failed to build `tokenizers==0.13.3`\n",
            "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mThe build backend returned an error\n",
            "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mCall to `setuptools.build_meta.build_wheel` failed (exit status: 1)\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[31m[stdout]\u001b[39m\n",
            "\u001b[31m      \u001b[0mrunning bdist_wheel\n",
            "\u001b[31m      \u001b[0mrunning build\n",
            "\u001b[31m      \u001b[0mrunning build_py\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/byte_level_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_unigram.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/char_level_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/base_tokenizer.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/bert_wordpiece.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer-styles.css ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mrunning build_ext\n",
            "\u001b[31m      \u001b[0mrunning build_rust\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
            "\u001b[31m      \u001b[0m/root/.cache/uv/builds-v0/.tmpyflKzK/lib/python3.12/site-packages/setuptools/dist.py:759:\n",
            "\u001b[31m      \u001b[0mSetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "\u001b[31m      \u001b[0m!!\n",
            "\n",
            "\u001b[31m      \u001b[0m\n",
            "\u001b[31m      \u001b[0m********************************************************************************\n",
            "\u001b[31m      \u001b[0m        Please consider removing the following classifiers in favor of a\n",
            "\u001b[31m      \u001b[0mSPDX license expression:\n",
            "\n",
            "\u001b[31m      \u001b[0m        License :: OSI Approved :: Apache Software License\n",
            "\n",
            "\u001b[31m      \u001b[0m        See\n",
            "\u001b[31m      \u001b[0mhttps://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license\n",
            "\u001b[31m      \u001b[0mfor details.\n",
            "\u001b[31m      \u001b[0m\n",
            "\u001b[31m      \u001b[0m********************************************************************************\n",
            "\n",
            "\u001b[31m      \u001b[0m!!\n",
            "\u001b[31m      \u001b[0m  self._finalize_license_expression()\n",
            "\u001b[31m      \u001b[0merror: can't find Rust compiler\n",
            "\n",
            "\u001b[31m      \u001b[0mIf you are using an outdated pip version, it is possible a prebuilt\n",
            "\u001b[31m      \u001b[0mwheel is available for this package but pip is not able to install from\n",
            "\u001b[31m      \u001b[0mit. Installing from the wheel would avoid the need for a Rust compiler.\n",
            "\n",
            "\u001b[31m      \u001b[0mTo update pip, run:\n",
            "\n",
            "\u001b[31m      \u001b[0m    pip install --upgrade pip\n",
            "\n",
            "\u001b[31m      \u001b[0mand then retry package installation.\n",
            "\n",
            "\u001b[31m      \u001b[0mIf you did intend to build this package from source, try installing\n",
            "\u001b[31m      \u001b[0ma Rust compiler from your system package manager and ensure it is\n",
            "\u001b[31m      \u001b[0mon the PATH during installation. Alternatively, rustup (available at\n",
            "\u001b[31m      \u001b[0mhttps://rustup.rs) is the recommended way to download and update the\n",
            "\u001b[31m      \u001b[0mRust compiler toolchain.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This usually indicates a problem with the package or the build\n",
            "\u001b[31m      \u001b[0menvironment.\n",
            "\u001b[36m  help: \u001b[0m`\u001b[36mtokenizers\u001b[39m` (\u001b[36mv0.13.3\u001b[39m) was included because `\u001b[36mtransformers\u001b[39m` (\u001b[36mv4.33.1\u001b[39m)\n",
            "        depends on `\u001b[36mtokenizers\u001b[39m`\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m91 packages\u001b[0m \u001b[2min 55ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 195ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.1\u001b[0m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe package `typer==0.20.0` does not have an extra named `all`\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m38 packages\u001b[0m \u001b[2min 29ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.1\u001b[0m\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 255 kB in 1s (234 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "dos2unix is already the newest version (7.4.2-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "\n",
            "‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\n"
          ]
        }
      ],
      "source": [
        "!uv pip install --upgrade pip\n",
        "!uv pip install -r FontDiffusion/requirements.txt\n",
        "!uv pip install gdown\n",
        "# 3. Install PyTorch 1.13\n",
        "%cd {OUTPUT_PATH}\n",
        "print(\"\\n‚¨áÔ∏è Installing PyTorch 1.13 (Required for this model)...\")\n",
        "# Force reinstall torch 1.13 to match the model's training environment\n",
        "!uv pip uninstall torch torchvision\n",
        "!uv pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "\n",
        "# 4. Install other dependencies\n",
        "print(\"\\n‚¨áÔ∏è Installing Dependencies (Manually fixed)...\")\n",
        "# Install xformers compatible with Torch 1.13\n",
        "!uv pip install xformers==0.0.16 -q\n",
        "\n",
        "# Install original dependencies\n",
        "!uv pip install transformers==4.33.1 accelerate==0.23.0 diffusers==0.22.0\n",
        "!uv pip install gradio==4.8.0 pyyaml pygame opencv-python info-nce-pytorch kornia\n",
        "# -----------------------------------------------------------------\n",
        "!uv pip install lpips scikit-image pytorch-fid\n",
        "!sudo apt-get update && sudo apt-get install dos2unix\n",
        "print(\"\\n‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-28T10:29:50.900515Z",
          "iopub.status.busy": "2025-12-28T10:29:50.900151Z",
          "iopub.status.idle": "2025-12-28T10:30:11.203927Z",
          "shell.execute_reply": "2025-12-28T10:30:11.203305Z",
          "shell.execute_reply.started": "2025-12-28T10:29:50.900494Z"
        },
        "id": "9PsLgUs0cYmO",
        "trusted": true,
        "outputId": "1383ffc5-8861-4179-e7e3-f3e65f36e7b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "%cd {OUTPUT_PATH}\n",
        "if not os.path.exists(\"ckpt\"):\n",
        "  url = \"https://drive.google.com/drive/folders/12hfuZ9MQvXqcteNuz7JQ2B_mUcTr-5jZ\"\n",
        "  gdown.download_folder(url, quiet=True, use_cookies=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-28T10:30:11.206149Z",
          "iopub.status.busy": "2025-12-28T10:30:11.205802Z",
          "iopub.status.idle": "2025-12-28T10:30:11.212595Z",
          "shell.execute_reply": "2025-12-28T10:30:11.211927Z",
          "shell.execute_reply.started": "2025-12-28T10:30:11.206130Z"
        },
        "id": "ecfc18e0",
        "outputId": "5b2a2c51-0b96-43f2-b47b-e586d05e4c9b",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No .zip files found in /content/.\n"
          ]
        }
      ],
      "source": [
        "# @title Unzipping all archived files\n",
        "import os\n",
        "import glob\n",
        "from zipfile import ZipFile\n",
        "\n",
        "zip_file_paths = glob.glob(os.path.join(INPUT_PATH, '*.zip'))\n",
        "\n",
        "if not zip_file_paths:\n",
        "    print(f'No .zip files found in {INPUT_PATH}.')\n",
        "else:\n",
        "    for zip_file_path in zip_file_paths:\n",
        "        if os.path.exists(zip_file_path):\n",
        "            print(f'Unzipping {zip_file_path}...')\n",
        "            !unzip -o {zip_file_path} -d ./\n",
        "            print(f'Unzipping of {zip_file_path} complete.')\n",
        "        else:\n",
        "            print(f'Error: The file {zip_file_path} was not found (post-glob check).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-28T10:30:11.213846Z",
          "iopub.status.busy": "2025-12-28T10:30:11.213524Z",
          "iopub.status.idle": "2025-12-28T10:30:11.248712Z",
          "shell.execute_reply": "2025-12-28T10:30:11.247563Z",
          "shell.execute_reply.started": "2025-12-28T10:30:11.213822Z"
        },
        "id": "JBflCTABxlF4",
        "outputId": "3b3a6816-9d56-47b9-91f3-b930e9483bf5",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ckpt\n",
            "\n",
            "‚úÖ All weights found! You can proceed to the next step.\n"
          ]
        }
      ],
      "source": [
        "# @title Checking checkpoint files (.pth)\n",
        "import os\n",
        "import time\n",
        "\n",
        "CHECKPOINT_DIR = os.path.join(INPUT_PATH, \"ckpt\")\n",
        "print(CHECKPOINT_DIR)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "required_files = [\"unet.pth\", \"content_encoder.pth\", \"style_encoder.pth\"]\n",
        "while True:\n",
        "    missing = [f for f in required_files if not os.path.exists(f\"{CHECKPOINT_DIR}/{f}\")]\n",
        "    if not missing:\n",
        "        print(\"\\n‚úÖ All weights found! You can proceed to the next step.\")\n",
        "        break\n",
        "    else:\n",
        "        print(f\"Waiting for files... Missing: {missing}\")\n",
        "        print(\"Upload them to the 'ckpt' folder now.\")\n",
        "        time.sleep(10) # Checks every 10 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-28T10:30:42.953132Z",
          "iopub.status.busy": "2025-12-28T10:30:42.952259Z",
          "iopub.status.idle": "2025-12-28T10:30:43.328221Z",
          "shell.execute_reply": "2025-12-28T10:30:43.326792Z",
          "shell.execute_reply.started": "2025-12-28T10:30:42.953098Z"
        },
        "id": "Mx5uS5WQaqdn",
        "outputId": "38f8c891-fed8-47d7-e543-b960c32ae855",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Demonstrating function with a dummy CSV file ---\n",
            "Created a dummy CSV file at: /content/dummy_data.csv\n",
            "Successfully converted '/content/dummy_data.csv' to '/content/dummy_chars.txt', with one character per line.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def convert_csv_to_chars_txt(input_csv_path: str, output_txt_path: str, column_name: str = 'word'):\n",
        "    \"\"\"\n",
        "    Reads a CSV file, extracts text from a specified column, and writes each character\n",
        "    to a new line in a plain text file.\n",
        "\n",
        "    Args:\n",
        "        input_csv_path (str): The full path to the input CSV file.\n",
        "        output_txt_path (str): The full path for the output text file.\n",
        "        column_name (str): The name of the column in the CSV file containing the text.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(input_csv_path):\n",
        "        print(f\"Error: Input CSV file not found at '{input_csv_path}'. Please ensure the file is uploaded.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(input_csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file '{input_csv_path}': {e}\")\n",
        "        return\n",
        "\n",
        "    if column_name not in df.columns:\n",
        "        print(f\"Error: Column '{column_name}' not found in the CSV file '{input_csv_path}'.\")\n",
        "        return\n",
        "\n",
        "    all_characters = []\n",
        "    # Ensure the column values are treated as strings before iterating over them\n",
        "    for item in df[column_name].astype(str).dropna().tolist():\n",
        "        for char in item:\n",
        "            all_characters.append(char)\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)\n",
        "\n",
        "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(all_characters))\n",
        "    print(f\"Successfully converted '{input_csv_path}' to '{output_txt_path}', with one character per line.\")\n",
        "\n",
        "# --- Example Usage (demonstration with a dummy file) ---\n",
        "# As the original file 'Ds_300_ChuNom_TuTao.csv' was not found in the previous execution,\n",
        "# let's create a dummy file to demonstrate the function's usage.\n",
        "print(\"\\n--- Demonstrating function with a dummy CSV file ---\")\n",
        "dummy_csv_path = os.path.join(INPUT_PATH, \"dummy_data.csv\")\n",
        "dummy_output_txt_path = os.path.join(OUTPUT_PATH, \"dummy_chars.txt\")\n",
        "\n",
        "# Create a dummy CSV file\n",
        "dummy_data = {'word': ['hello', 'world', 'python']}\n",
        "pd.DataFrame(dummy_data).to_csv(dummy_csv_path, index=False)\n",
        "print(f\"Created a dummy CSV file at: {dummy_csv_path}\")\n",
        "\n",
        "convert_csv_to_chars_txt(dummy_csv_path, dummy_output_txt_path)\n",
        "\n",
        "# --- How to use with your actual file ---\n",
        "# Uncomment the lines below and replace 'your_actual_file.csv' and 'your_output.txt'\n",
        "# with the correct paths for your use case.\n",
        "#\n",
        "# original_csv_file = os.path.join(INPUT_PATH, \"Ds_300_ChuNom_TuTao.csv\") # Or the full path to your CSV\n",
        "# original_output_txt = os.path.join(OUTPUT_PATH, \"nom_tu_tao.txt\") # Or your desired output path\n",
        "# convert_csv_to_chars_txt(original_csv_file, original_output_txt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "lg9xYThrFLCV",
        "outputId": "289c8f0d-6561-4464-a970-820110e0d4e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset from dzungpham/font-diffusion-generated-data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DatasetNotFoundError",
          "evalue": "Dataset 'dzungpham/font-diffusion-generated-data' doesn't exist on the Hub or cannot be accessed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3262260766.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# This downloads the data to a local cache directory (e.g., /root/.cache/huggingface/datasets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# and returns a Dataset object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmy_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Dataset loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1393\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fix_for_backward_compatible_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFilesNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                     raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m                 ) from e\n\u001b[1;32m    979\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 api.hf_hub_download(\n",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'dzungpham/font-diffusion-generated-data' doesn't exist on the Hub or cannot be accessed."
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Replace with your Hugging Face username and the repo name you chose\n",
        "repo_name = \"dzungpham/font-diffusion-generated-data\"\n",
        "print(f\"Downloading dataset from {repo_name}...\")\n",
        "\n",
        "# This downloads the data to a local cache directory (e.g., /root/.cache/huggingface/datasets)\n",
        "# and returns a Dataset object.\n",
        "my_dataset = load_dataset(repo_name, split=\"train\")\n",
        "\n",
        "print(\"‚úÖ Dataset loaded.\")\n",
        "print(my_dataset) # You can inspect the dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-28T10:42:25.672117Z",
          "iopub.status.busy": "2025-12-28T10:42:25.671647Z",
          "iopub.status.idle": "2025-12-28T10:42:25.799293Z",
          "shell.execute_reply": "2025-12-28T10:42:25.798554Z",
          "shell.execute_reply.started": "2025-12-28T10:42:25.672076Z"
        },
        "id": "gma02BZvhx8I",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%cd {OUTPUT_PATH}\n",
        "!python FontDiffusion/sample_batch.py \\\n",
        "    --characters \"FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\" \\\n",
        "    --start_line 1 \\\n",
        "    --end_line 50 \\\n",
        "    --style_images \"FontDiffusion/styles_images\" \\\n",
        "    --ckpt_dir \"ckpt/\" \\\n",
        "    --ttf_path \"FontDiffusion/fonts\" \\\n",
        "    --output_dir \"my_dataset\" \\\n",
        "    --resume_from \"my_dataset/results_checkpoint.json\" \\\n",
        "    --batch_size 24 \\\n",
        "    --save_interval 5 \\\n",
        "    --channels_last \\\n",
        "    --num_inference_steps 20 \\\n",
        "    --guidance_scale 7.5 \\\n",
        "    --seed 42 \\\n",
        "    --compile \\\n",
        "    --enable_xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch FontDiffusion/train.py \\\n",
        "    --seed=123 \\\n",
        "    --experience_name=\"FontDiffuser_training_phase_1\" \\\n",
        "    --data_root=\"my_dataset\" \\\n",
        "    --output_dir=\"outputs/FontDiffuser\" \\\n",
        "    --report_to=\"wandb\" \\\n",
        "    --resolution=96 \\\n",
        "    --style_image_size=96 \\\n",
        "    --content_image_size=96 \\\n",
        "    --content_encoder_downsample_size=3 \\\n",
        "    --channel_attn=True \\\n",
        "    --content_start_channel=64 \\\n",
        "    --style_start_channel=64 \\\n",
        "    --train_batch_size=16 \\\n",
        "    --perceptual_coefficient=0.03 \\\n",
        "    --offset_coefficient=0.7 \\\n",
        "    --max_train_steps=440000 \\\n",
        "    --ckpt_interval=40000 \\\n",
        "    --gradient_accumulation_steps=1 \\\n",
        "    --log_interval=50 \\\n",
        "    --learning_rate=1e-5 \\\n",
        "    --lr_scheduler=\"linear\" \\\n",
        "    --lr_warmup_steps=10000 \\\n",
        "    --drop_prob=0.1 \\\n",
        "    --mixed_precision=\"no\""
      ],
      "metadata": {
        "id": "FxxJ9qy4KIZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-12-28T10:30:43.330470Z",
          "iopub.status.idle": "2025-12-28T10:30:43.330877Z",
          "shell.execute_reply": "2025-12-28T10:30:43.330691Z",
          "shell.execute_reply.started": "2025-12-28T10:30:43.330672Z"
        },
        "id": "kTz9WZ9ylBZx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "def find_result_folders(base_path: Path) -> List[Path]:\n",
        "    return [p for p in base_path.glob(\"*dataset\") if p.is_dir()]\n",
        "\n",
        "def zip_folder(folder_path: Path, output_base_path: Path) -> bool:\n",
        "    folder_name = folder_path.name\n",
        "    zip_path = output_base_path / f\"{folder_name}.zip\"\n",
        "    try:\n",
        "        print(f\"   -> Zipping folder: {folder_name}...\")\n",
        "        with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for file_path in folder_path.rglob(\"*\"):\n",
        "                if file_path.is_file():\n",
        "                    arcname = file_path.relative_to(folder_path.parent)\n",
        "                    zipf.write(file_path, arcname)\n",
        "        print(f\"   ‚úÖ Created ZIP: {zip_path.name}\")\n",
        "        return True\n",
        "    except Exception as exc:\n",
        "        print(f\"   ‚ùå Failed to zip {folder_name}: {exc}\")\n",
        "        return False\n",
        "\n",
        "def zip_stats_results_folders(output_base_path: str) -> None:\n",
        "    base = Path(output_base_path)\n",
        "    base.mkdir(parents=True, exist_ok=True)\n",
        "    result_folders = find_result_folders(base)\n",
        "\n",
        "    if not result_folders:\n",
        "        print(f\"‚ö†Ô∏è No folders matching '*dataset' found in '{output_base_path}'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"üîç Found {len(result_folders)} result folder(s) to zip.\")\n",
        "    successful = sum(1 for folder in result_folders if zip_folder(folder, base))\n",
        "\n",
        "    print(f\"\\n‚úÖ DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n",
        "        if not output_root:\n",
        "            raise ValueError(\"OUTPUT_PATH not defined\")\n",
        "\n",
        "        zip_stats_results_folders(\"/content\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-12-28T10:30:43.331996Z",
          "iopub.status.idle": "2025-12-28T10:30:43.332314Z",
          "shell.execute_reply": "2025-12-28T10:30:43.332159Z",
          "shell.execute_reply.started": "2025-12-28T10:30:43.332143Z"
        },
        "id": "SIH9c0l-mRqB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "\n",
        "# Import Hugging Face libraries\n",
        "from datasets import load_dataset, Dataset, Features, Value, Image\n",
        "\n",
        "def generate_metadata_from_results(output_dir: Path, results_filename: str = \"results.json\") -> Path:\n",
        "    \"\"\"\n",
        "    Generates a metadata.jsonl file assuming the generation script was run\n",
        "    from the parent directory of `output_dir`.\n",
        "\n",
        "    This is designed for a common Colab/Kaggle workflow where scripts are run\n",
        "    from `/content/` and outputs are saved to `/content/my_dataset/`.\n",
        "\n",
        "    Args:\n",
        "        output_dir (Path): The root directory of the generated output\n",
        "                           (e.g., Path(\"/content/my_dataset\")).\n",
        "        results_filename (str): The name of the JSON file containing generation logs.\n",
        "\n",
        "    Returns:\n",
        "        Path: The path to the newly created metadata.jsonl file.\n",
        "    \"\"\"\n",
        "    results_file = output_dir / results_filename\n",
        "    metadata_file = output_dir / \"metadata.jsonl\"\n",
        "\n",
        "    # The directory from which the original script was run (e.g., /content/)\n",
        "    execution_context_dir = output_dir.parent\n",
        "\n",
        "    print(f\"Reading generation data from: {results_file}\")\n",
        "    if not results_file.is_file():\n",
        "        raise FileNotFoundError(f\"The results file was not found at {results_file}\")\n",
        "\n",
        "    with results_file.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        results_data = json.load(f)\n",
        "\n",
        "    print(f\"Generating metadata file at: {metadata_file}\")\n",
        "    records_written = 0\n",
        "    with metadata_file.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for gen_info in results_data.get(\"generations\", []):\n",
        "            try:\n",
        "                # Path from JSON, e.g., \"my_dataset/TargetImage.png/...\"\n",
        "                path_from_json = gen_info[\"output_path\"]\n",
        "\n",
        "                # --- THIS IS THE KEY LOGIC ---\n",
        "                # 1. Reconstruct the full, absolute path.\n",
        "                #    Combines the execution context with the relative path from the file.\n",
        "                #    e.g., Path(\"/content\") + \"my_dataset/...\" -> Path(\"/content/my_dataset/...\")\n",
        "                absolute_path = (execution_context_dir / path_from_json).resolve()\n",
        "\n",
        "                # 2. Make the path relative to the dataset's root directory.\n",
        "                #    This makes the final metadata portable.\n",
        "                #    e.g., Path(\"/content/my_dataset/...\").relative_to(Path(\"/content/my_dataset\"))\n",
        "                #          -> \"TargetImage.png/...\"\n",
        "                relative_path_for_metadata = absolute_path.relative_to(output_dir)\n",
        "                # ---------------------------\n",
        "\n",
        "                metadata_entry = {\n",
        "                    \"file_name\": str(relative_path_for_metadata),\n",
        "                    \"character\": gen_info.get(\"character\", \"unknown\"),\n",
        "                    \"style\": gen_info.get(\"style\", \"unknown\"),\n",
        "                    \"font\": gen_info.get(\"font\", \"unknown\"),\n",
        "                }\n",
        "                f.write(json.dumps(metadata_entry) + \"\\n\")\n",
        "                records_written += 1\n",
        "            except (KeyError, ValueError) as e:\n",
        "                print(f\"Skipping a record due to an error: {e}\")\n",
        "\n",
        "    print(f\"‚úÖ Metadata generation complete. {records_written} records written.\")\n",
        "    return metadata_file\n",
        "\n",
        "# Note: The `load_dataset_from_metadata` function you provided in the last\n",
        "# prompt does not need to be changed. It is already robust and will work\n",
        "# perfectly with the output of this new generation function.\n",
        "\n",
        "\n",
        "def load_dataset_from_metadata(metadata_path: Path, base_data_dir: Path) -> Dataset:\n",
        "    \"\"\"\n",
        "    Loads a Hugging Face Dataset using a metadata.jsonl file.\n",
        "\n",
        "    This function reads the metadata, resolves the relative image paths,\n",
        "    and loads the images into a structured Dataset object.\n",
        "\n",
        "    Args:\n",
        "        metadata_path (Path): Path to the generated metadata.jsonl file.\n",
        "        base_data_dir (Path): The root directory where the image files are located.\n",
        "                              This is used to resolve the relative paths in the metadata.\n",
        "\n",
        "    Returns:\n",
        "        Dataset: The loaded and structured Hugging Face Dataset.\n",
        "    \"\"\"\n",
        "    print(f\"Loading dataset using metadata: {metadata_path}\")\n",
        "    if not metadata_path.is_file():\n",
        "        raise FileNotFoundError(f\"The metadata file was not found at {metadata_path}\")\n",
        "\n",
        "    # 1. Load the JSON data first. This will create a dataset with a 'file_name' column.\n",
        "    dataset = load_dataset(\"json\", data_files=str(metadata_path), split=\"train\")\n",
        "\n",
        "    # 2. Define a function to resolve the relative file_name to a full path for loading.\n",
        "    #    The `datasets.Image()` feature needs a complete path to open the file.\n",
        "    def resolve_image_path(example: Dict) -> Dict:\n",
        "        # Use pathlib's `/` operator for clean path joining\n",
        "        example[\"image\"] = str(base_data_dir / example[\"file_name\"])\n",
        "        return example\n",
        "\n",
        "    print(\"Resolving image paths...\")\n",
        "    dataset = dataset.map(resolve_image_path)\n",
        "\n",
        "    # 3. Cast the 'image' column (which now contains full paths) to the Image feature type.\n",
        "    #    This tells the library to actually load the pixels from the paths.\n",
        "    print(\"Casting paths to images...\")\n",
        "    dataset = dataset.cast_column(\"image\", Image())\n",
        "\n",
        "    print(\"‚úÖ Dataset loaded successfully with image data.\")\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfAXWsvdQ_iv"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from datasets import load_dataset, Dataset, Features, Value, Image # Ensure all imports\n",
        "import json # Ensure json is imported\n",
        "\n",
        "# (Assume you have already defined `load_dataset_from_metadata` from the previous prompt)\n",
        "\n",
        "# --- Step 1: Define your paths based on the Colab environment ---\n",
        "# This makes the code clean and easy to read.\n",
        "ROOT_PATH = Path(\"/content/\")\n",
        "OUTPUT_DIR_NAME = \"my_dataset\"\n",
        "FULL_OUTPUT_PATH = ROOT_PATH / OUTPUT_DIR_NAME\n",
        "\n",
        "# --- Step 2: Run your generation script from the root path ---\n",
        "# (This is a placeholder for your actual generation command)\n",
        "#\n",
        "# !python FontDiffusion/sample_batch.py \\\n",
        "#     --output_dir {OUTPUT_DIR_NAME} \\\n",
        "#     ... other args ...\n",
        "#\n",
        "# This will create the directory /content/my_dataset/ and populate it.\n",
        "\n",
        "# --- Step 3: Generate the metadata file ---\n",
        "# The function now correctly understands the path structure.\n",
        "try:\n",
        "    print(f\"Starting metadata generation for directory: {FULL_OUTPUT_PATH}\")\n",
        "    generated_metadata_path = generate_metadata_from_results(\n",
        "        output_dir=FULL_OUTPUT_PATH\n",
        "    )\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå ERROR: {e}\")\n",
        "    print(\"Please ensure your generation script has run successfully and created a results.json file.\")\n",
        "\n",
        "# --- Step 4: Load the structured dataset using the metadata ---\n",
        "# This part is unchanged and works as intended.\n",
        "try:\n",
        "    my_structured_dataset = load_dataset_from_metadata(\n",
        "        metadata_path=generated_metadata_path,\n",
        "        base_data_dir=FULL_OUTPUT_PATH\n",
        "    )\n",
        "\n",
        "    # --- Step 5: Verify and use your dataset ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"  ‚úÖ DATASET LOADED SUCCESSFULLY\")\n",
        "    print(\"=\"*50)\n",
        "    print(my_structured_dataset)\n",
        "\n",
        "    print(\"\\n--- Example Record ---\")\n",
        "    if len(my_structured_dataset) > 0:\n",
        "        example = my_structured_dataset[0]\n",
        "        print(f\"Character: {example['character']}\")\n",
        "        print(f\"Style: {example['style']}\")\n",
        "        print(f\"Font: {example['font']}\")\n",
        "        print(\"Image object:\", example['image'])\n",
        "\n",
        "        # In Colab/Jupyter, this will render the image directly in the output\n",
        "        display(example['image'])\n",
        "    else:\n",
        "        print(\"Dataset is empty. Check for errors during metadata generation.\")\n",
        "\n",
        "except (NameError, FileNotFoundError) as e:\n",
        "    print(f\"‚ùå ERROR: Could not load the dataset. Did the metadata generation fail? Details: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpj8EL50PA7q"
      },
      "outputs": [],
      "source": [
        "# Install the library if you haven't already\n",
        "from huggingface_hub import HfApi, notebook_login\n",
        "\n",
        "# 1. Login to Hugging Face\n",
        "# This will use the token from your Kaggle/Colab secrets\n",
        "notebook_login()\n",
        "\n",
        "# 2. Define your local path and the repository ID on the Hub\n",
        "# This is the directory containing ContentImage/, TargetImage/, etc.\n",
        "local_output_dir = Path(OUTPUT_PATH) / \"my_dataset\"\n",
        "repo_id = \"dzungpham/font-diffusion-generated-data\" # Choose a name for your repo\n",
        "\n",
        "# 3. Create the repository and upload the folder\n",
        "api = HfApi()\n",
        "\n",
        "print(f\"Creating repository '{repo_id}' on the Hub...\")\n",
        "api.create_repo(\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"dataset\", # Can be 'dataset' or 'model'\n",
        "    private=True,      # Set to False if you want it public\n",
        "    exist_ok=True      # Don't fail if it already exists\n",
        ")\n",
        "\n",
        "print(f\"Uploading folder '{local_output_dir}' to '{repo_id}'...\")\n",
        "# This command will recursively upload everything, preserving the structure.\n",
        "api.upload_folder(\n",
        "    folder_path=local_output_dir,\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Upload complete! Your file tree is now on the Hub.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31192,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}