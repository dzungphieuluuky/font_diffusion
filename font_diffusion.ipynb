{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a95a46ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-01T15:45:44.144713Z",
     "iopub.status.busy": "2026-01-01T15:45:44.144043Z",
     "iopub.status.idle": "2026-01-01T15:45:55.637574Z",
     "shell.execute_reply": "2026-01-01T15:45:55.636887Z",
     "shell.execute_reply.started": "2026-01-01T15:45:44.144684Z"
    },
    "id": "a95a46ef",
    "outputId": "d76d28cd-6292-42bf-fffa-a8c7efb86ed0",
    "papermill": {
     "duration": 12.857369,
     "end_time": "2025-12-30T18:53:35.066181",
     "exception": false,
     "start_time": "2025-12-30T18:53:22.208812",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'FontDiffusion'...\n",
      "remote: Enumerating objects: 20502, done.\u001b[K\n",
      "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
      "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
      "remote: Total 20502 (delta 44), reused 48 (delta 21), pack-reused 20426 (from 2)\u001b[K\n",
      "Receiving objects: 100% (20502/20502), 277.67 MiB | 39.41 MiB/s, done.\n",
      "Resolving deltas: 100% (890/890), done.\n",
      "Updating files: 100% (137/137), done.\n"
     ]
    }
   ],
   "source": [
    "# @title Environment Setup\n",
    "import os\n",
    "import sys\n",
    "if 'MPLBACKEND' in os.environ:\n",
    "    del os.environ['MPLBACKEND']\n",
    "    print(\"MPLBACKEND environment variable cleared.\")\n",
    "\n",
    "# 2. Clone the repository\n",
    "!rm -rf FontDiffusion\n",
    "!git clone https://github.com/dzungphieuluuky/FontDiffusion.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd8666",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cdd8666",
    "outputId": "8834f4e4-fc28-455c-a66c-d15b00de080a",
    "papermill": {
     "duration": 0.019157,
     "end_time": "2025-12-30T18:53:35.092303",
     "exception": false,
     "start_time": "2025-12-30T18:53:35.073146",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from IPython import get_ipython\n",
    "from typing import Optional\n",
    "\n",
    "def configure_environment_paths():\n",
    "    try:\n",
    "        if \"google.colab\" in str(get_ipython()):\n",
    "            print(\"âœ… Environment: Google Colab\")\n",
    "            base_data_path = \"/content/\"\n",
    "            base_output_path = \"/content/\"\n",
    "            environment_name = \"colab\"\n",
    "        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
    "            print(\"âœ… Environment: Kaggle\")\n",
    "            base_data_path = \"/kaggle/input/\"\n",
    "            base_output_path = \"/kaggle/working/\"\n",
    "            environment_name = \"kaggle\"\n",
    "        else:\n",
    "            print(\"âš ï¸ Environment: Local/Unknown\")\n",
    "            base_data_path = \"./data/\"\n",
    "            base_output_path = \"./output/\"\n",
    "            environment_name = \"local\"\n",
    "    except NameError:\n",
    "        print(\"âš ï¸ Non-interactive session. Using local paths.\")\n",
    "        base_data_path = \"./data/\"\n",
    "        base_output_path = \"./output/\"\n",
    "        environment_name = \"local\"\n",
    "    os.makedirs(base_output_path, exist_ok=True)\n",
    "    print(f\"ðŸ“‚ Data Path: {base_data_path}\")\n",
    "    print(f\"ðŸ“¦ Output Path: {base_output_path}\")\n",
    "    return base_data_path, base_output_path, environment_name\n",
    "\n",
    "def load_secret(key_name: str) -> Optional[str]:\n",
    "    env = ENV_NAME\n",
    "    secret_value = None\n",
    "    print(f\"Attempting to load secret '{key_name}' from '{env}' environment...\")\n",
    "    try:\n",
    "        if env == \"colab\":\n",
    "            from google.colab import userdata\n",
    "            secret_value = userdata.get(key_name)\n",
    "        elif env == \"kaggle\":\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            user_secrets = UserSecretsClient()\n",
    "            secret_value = user_secrets.get_secret(key_name)\n",
    "        else:\n",
    "            secret_value = os.getenv(key_name)\n",
    "        if not secret_value:\n",
    "            print(f\"âš ï¸ Secret '{key_name}' not found in the {env} environment.\")\n",
    "            return None\n",
    "        print(f\"âœ… Successfully loaded secret '{key_name}'.\")\n",
    "        return secret_value\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ An error occurred while loading secret '{key_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "def print_system_info():\n",
    "    print(\"\\nðŸ”§ System Information\")\n",
    "    print(f\"Python version: {sys.version.split()[0]}\")\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"CUDA version: {torch.version.cuda}\")\n",
    "            print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        else:\n",
    "            print(\"CUDA not available\")\n",
    "    except ImportError:\n",
    "        print(\"PyTorch not installed\")\n",
    "    finally:\n",
    "      !nvidia-smi\n",
    "\n",
    "INPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()\n",
    "is_kaggle = (\"kaggle\" in ENV_NAME)\n",
    "is_colab = not is_kaggle\n",
    "print_system_info()\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_key = load_secret(\"WANDB_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN = load_secret('HF_TOKEN')\n",
    "\n",
    "# Now, these libraries will log in automatically\n",
    "import wandb\n",
    "import huggingface_hub\n",
    "\n",
    "wandb.login() \n",
    "huggingface_hub.login(token=os.environ[\"HF_TOKEN\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b4150",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a73b4150",
    "outputId": "97db2cec-8e2d-438b-e5f8-38df08b7f59e",
    "papermill": {
     "duration": 61.239828,
     "end_time": "2025-12-30T18:54:36.338205",
     "exception": false,
     "start_time": "2025-12-30T18:53:35.098377",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!uv pip install --upgrade pip\n",
    "# 3. Install PyTorch 1.13\n",
    "%cd {OUTPUT_PATH}\n",
    "# Force reinstall torch 1.13 to match the model's training environment\n",
    "# !uv pip uninstall torch torchvision\n",
    "# !uv pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "!uv pip install torch==2.9 torchvision\n",
    "# 4. Install other dependencies\n",
    "\n",
    "print(\"\\nâ¬‡ï¸ Installing Dependencies (Manually fixed)...\")\n",
    "# Install xformers compatible with Torch 1.13\n",
    "!uv pip install xformers==0.0.16 -q\n",
    "\n",
    "# Install original dependencies\n",
    "!uv pip install transformers==4.33.1 accelerate==0.23.0 diffusers==0.22.0\n",
    "!uv pip install gradio==4.8.0 pyyaml pygame opencv-python info-nce-pytorch kornia\n",
    "# -----------------------------------------------------------------\n",
    "!uv pip install lpips scikit-image pytorch-fid\n",
    "!sudo apt-get update && sudo apt-get install dos2unix\n",
    "!uv pip install gdown tqdm\n",
    "!uv pip install wandb\n",
    "!uv pip install --upgrade pyarrow datasets\n",
    "print(\"\\nâœ… Environment setup complete. You can now proceed to Block 2 (Inference).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd517dfe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd517dfe",
    "outputId": "d83605e9-f5dc-4862-d1c9-b138a96ca47a",
    "papermill": {
     "duration": 12.524295,
     "end_time": "2025-12-30T18:54:48.878013",
     "exception": false,
     "start_time": "2025-12-30T18:54:36.353718",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# KAGGLE CELL #1: Download checkpoint\n",
    "if is_colab:\n",
    "  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\n",
    "else:\n",
    "  !uv pip install --upgrade \"huggingface-hub==0.25.2\" \"protobuf<5.0.0\" \"numpy<2.0.0\"\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "os.chdir(OUTPUT_PATH)\n",
    "# Download from Hub\n",
    "if not os.path.exists(\"ckpt\") or not list(Path(\"ckpt\").glob(\"*.safetensors\")):\n",
    "    print(\"ðŸ“¥ Downloading checkpoint from Hugging Face Hub...\\n\")\n",
    "    from huggingface_hub import snapshot_download\n",
    "    snapshot_download(\n",
    "        repo_id=\"dzungpham/font-diffusion-weights\",\n",
    "        local_dir=\"ckpt\",\n",
    "        allow_patterns=\"*.safetensors\",\n",
    "        force_download=False\n",
    "    )\n",
    "    print(\"\\nâœ… Download complete!\")\n",
    "else:\n",
    "    print(\"âœ… Checkpoint already downloaded\")\n",
    "# Verify\n",
    "print(\"\\nðŸ“‚ Files in ckpt/:\")\n",
    "for file in os.listdir(\"ckpt\"):\n",
    "    if file.endswith(\".safetensors\"):\n",
    "        size = os.path.getsize(f\"ckpt/{file}\") / (1024**2)\n",
    "        print(f\"  âœ“ {file} ({size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e8ea2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "767e8ea2",
    "outputId": "20185e27-e772-4823-e6bc-d9bd6d0b39a1",
    "papermill": {
     "duration": 0.023805,
     "end_time": "2025-12-30T18:54:48.917163",
     "exception": false,
     "start_time": "2025-12-30T18:54:48.893358",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Unzipping all archived files\n",
    "import os\n",
    "import glob\n",
    "from zipfile import ZipFile\n",
    "\n",
    "zip_file_paths = glob.glob(os.path.join(INPUT_PATH, '*.zip'))\n",
    "\n",
    "if not zip_file_paths:\n",
    "    print(f'No .zip files found in {INPUT_PATH}.')\n",
    "else:\n",
    "    for zip_file_path in zip_file_paths:\n",
    "        if os.path.exists(zip_file_path):\n",
    "            print(f'Unzipping {zip_file_path}...')\n",
    "            !unzip -o {zip_file_path} -d ./\n",
    "            print(f'Unzipping of {zip_file_path} complete.')\n",
    "        else:\n",
    "            print(f'Error: The file {zip_file_path} was not found (post-glob check).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51941368",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51941368",
    "outputId": "2a2c352c-968a-4e4d-b4cc-88a02c7eb788",
    "papermill": {
     "duration": 1.62157,
     "end_time": "2025-12-30T18:54:50.594793",
     "exception": false,
     "start_time": "2025-12-30T18:54:48.973223",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "def convert_csv_to_chars_txt(input_csv_path: str, output_txt_path: str, column_name: str = 'word'):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, extracts text from a specified column, and writes each character\n",
    "    to a new line in a plain text file.\n",
    "    Args:\n",
    "        input_csv_path (str): The full path to the input CSV file.\n",
    "        output_txt_path (str): The full path for the output text file.\n",
    "        column_name (str): The name of the column in the CSV file containing the text.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_csv_path):\n",
    "        print(f\"Error: Input CSV file not found at '{input_csv_path}'. Please ensure the file is uploaded.\")\n",
    "        return\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file '{input_csv_path}': {e}\")\n",
    "        return\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found in the CSV file '{input_csv_path}'.\")\n",
    "        return\n",
    "    all_characters = []\n",
    "    for item in df[column_name].astype(str).dropna().tolist():\n",
    "        for char in item:\n",
    "            all_characters.append(char)\n",
    "    os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)\n",
    "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(all_characters))\n",
    "    print(f\"Successfully converted '{input_csv_path}' to '{output_txt_path}', with one character per line.\")\n",
    "print(\"\\n--- Demonstrating function with a dummy CSV file ---\")\n",
    "dummy_csv_path = os.path.join(OUTPUT_PATH, \"dummy_data.csv\")\n",
    "dummy_output_txt_path = os.path.join(OUTPUT_PATH, \"dummy_chars.txt\")\n",
    "dummy_data = {'word': ['hello', 'world', 'python']}\n",
    "pd.DataFrame(dummy_data).to_csv(dummy_csv_path, index=False)\n",
    "print(f\"Created a dummy CSV file at: {dummy_csv_path}\")\n",
    "convert_csv_to_chars_txt(dummy_csv_path, dummy_output_txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4cf20b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f4cf20b",
    "outputId": "335f4192-47e7-451a-e14f-e0bd69fbdfc9",
    "papermill": {
     "duration": 0.140282,
     "end_time": "2025-12-30T18:54:50.749810",
     "exception": false,
     "start_time": "2025-12-30T18:54:50.609528",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Model files:\")\n",
    "!ls -larth {OUTPUT_PATH}/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cff682",
   "metadata": {
    "id": "92cff682",
    "papermill": {
     "duration": 0.104394,
     "end_time": "2025-12-30T18:54:50.869230",
     "exception": false,
     "start_time": "2025-12-30T18:54:50.764836",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd {OUTPUT_PATH}\n",
    "# ==========================================\n",
    "# EXPORT / DOWNLOAD DATASET COMMANDS\n",
    "# ==========================================\n",
    "HF_USERNAME = \"dzungpham\"\n",
    "# Train Split\n",
    "!python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "  --output_dir \"my_dataset/train_original\" \\\n",
    "  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "  --split \"train_original\" \\\n",
    "  --token HF_TOKEN\n",
    "\n",
    "# !python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "#   --output_dir \"my_dataset/train\" \\\n",
    "#   --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "#   --split \"train\" \\\n",
    "#   --token HF_TOKEN\n",
    "# # Validation: Unseen Both\n",
    "# !python FontDiffusion/export_hf_dataset_to_disk.py \\\n",
    "#   --output_dir \"my_dataset/val\" \\\n",
    "#   --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n",
    "#   --split \"val\" \\\n",
    "#   --token HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9c1d6-dd60-479c-92c4-2f653e4d48fd",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Fonts currently in fonts/ folder\")\n",
    "!ls -lt FontDiffusion/fonts\n",
    "print(\"Styles in style_images/ folder\")\n",
    "!ls -l FontDiffusion/styles_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29deed1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-01T16:03:18.565170Z",
     "iopub.status.busy": "2026-01-01T16:03:18.564864Z"
    },
    "id": "29deed1d",
    "outputId": "749b50d0-75e3-4d36-e509-919188feb64c",
    "papermill": {
     "duration": 10.53661,
     "end_time": "2025-12-30T18:55:01.421093",
     "exception": false,
     "start_time": "2025-12-30T18:54:50.884483",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m14 packages\u001b[0m \u001b[2min 79ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2mAudited \u001b[1m14 packages\u001b[0m \u001b[2min 0.18ms\u001b[0m\u001b[0m\n",
      "/kaggle/working\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.11/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2026-01-01 16:03:27.970851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767283407.993711    1819 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767283408.001084    1819 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "2026-01-01 16:03:33,362 | INFO | \n",
      "============================================================\n",
      "2026-01-01 16:03:33,362 | INFO | FONTDIFFUSER SYNTHESIS DATA GENERATION MAGIC\n",
      "2026-01-01 16:03:33,362 | INFO | ============================================================\n",
      "2026-01-01 16:03:33,364 | INFO | ðŸ“– Loading characters from file: FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\n",
      "2026-01-01 16:03:33,365 | INFO |    Lines 500 to 550 (total file: 10174 lines)\n",
      "2026-01-01 16:03:33,365 | INFO |    Processing 51 lines...\n",
      "ðŸ“– Reading character file: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 51/51 [00:00<00:00, 472206.41it/s]\u001b[0m\n",
      "2026-01-01 16:03:33,366 | INFO | âœ… Successfully loaded 51 single characters.\n",
      "2026-01-01 16:03:33,366 | INFO | \n",
      "ðŸ“‚ Loading 15 style images from directory...\n",
      "âœ“ Verifying style images: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 15/15 [00:00<00:00, 88862.37it/s]\u001b[0m\n",
      "2026-01-01 16:03:33,367 | INFO | \n",
      "Initializing font manager...\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "2026-01-01 16:03:33,494 | INFO | âœ“ Loaded font: NomNaTong-Regular\n",
      "2026-01-01 16:03:33,495 | INFO | âœ“ Loaded 1 fonts.\n",
      "2026-01-01 16:03:33,495 | INFO | \n",
      "ðŸ“Š Configuration:\n",
      "2026-01-01 16:03:33,495 | INFO |   Dataset split: train_original\n",
      "2026-01-01 16:03:33,495 | INFO |   Characters: 51 (lines 500-550)\n",
      "2026-01-01 16:03:33,495 | INFO |   Styles: 15\n",
      "2026-01-01 16:03:33,495 | INFO |   Output Directory: my_dataset/train_original\n",
      "2026-01-01 16:03:33,495 | INFO |   Checkpoint Directory: ckpt/\n",
      "2026-01-01 16:03:33,495 | INFO |   Device: cuda\n",
      "2026-01-01 16:03:33,495 | INFO |   Batch Size: 35\n",
      "2026-01-01 16:03:33,495 | INFO |   Results checkpoint path: my_dataset/train_original/results_checkpoint.json\n",
      "2026-01-01 16:03:33,844 | INFO | âœ“ Loaded checkpoint: 7485 unique generations\n",
      "2026-01-01 16:03:33,844 | INFO |   Total raw entries: 7485\n",
      "2026-01-01 16:03:33,848 | INFO | \n",
      "Loading FontDiffuser pipeline...\n",
      "Loading FontDiffuser pipeline...\n",
      "Load the down block  DownBlock2D\n",
      "Load the down block  MCADownBlock2D\n",
      "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
      "Load the down block  MCADownBlock2D\n",
      "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
      "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
      "Load the down block  DownBlock2D\n",
      "Load the up block  UpBlock2D\n",
      "Load the up block  StyleRSIUpBlock2D\n",
      "Load the up block  StyleRSIUpBlock2D\n",
      "Load the up block  UpBlock2D\n",
      "Param count for Ds initialized parameters: 20591296\n",
      "Get CG-GAN Style Encoder!\n",
      "Param count for Ds initialized parameters: 1187008\n",
      "Get CG-GAN Content Encoder!\n",
      "âœ“ Loaded model state_dict successfully\n",
      "Converting to channels-last memory format...\n",
      "âœ“ Model moved to device\n",
      "âœ“ Loaded training DDPM scheduler successfully\n",
      "âœ“ Loaded DPM-Solver pipeline successfully\n",
      "2026-01-01 16:03:35,359 | INFO | ðŸ”§ Compiling model components with torch.compile...\n",
      "/kaggle/working/FontDiffusion/sample_batch.py:1564: FutureWarning: Accessing config attribute `unet` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'unet' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.unet'.\n",
      "  if hasattr(pipe.model, \"unet\"):\n",
      "/kaggle/working/FontDiffusion/sample_batch.py:1565: FutureWarning: Accessing config attribute `unet` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'unet' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.unet'.\n",
      "  pipe.model.unet = torch.compile(pipe.model.unet)\n",
      "/kaggle/working/FontDiffusion/sample_batch.py:1566: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.style_encoder'.\n",
      "  if hasattr(pipe.model, \"style_encoder\"):\n",
      "/kaggle/working/FontDiffusion/sample_batch.py:1567: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.style_encoder'.\n",
      "  pipe.model.style_encoder = torch.compile(pipe.model.style_encoder)\n",
      "/kaggle/working/FontDiffusion/sample_batch.py:1568: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n",
      "  if hasattr(pipe.model, \"content_encoder\"):\n",
      "/kaggle/working/FontDiffusion/sample_batch.py:1570: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n",
      "  pipe.model.content_encoder\n",
      "2026-01-01 16:03:36,766 | INFO | âœ“ Compilation complete.\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Loading model from: /usr/local/lib/python3.11/dist-packages/lpips/weights/v0.1/alex.pth\n",
      "2026-01-01 16:03:37,434 | INFO | ============================================================\n",
      "2026-01-01 16:03:37,434 | INFO |                  GENERATING CONTENT IMAGES                  \n",
      "2026-01-01 16:03:37,434 | INFO | ============================================================\n",
      "2026-01-01 16:03:37,434 | INFO | \n",
      "============================================================\n",
      "2026-01-01 16:03:37,435 | INFO | Generating Content Images\n",
      "2026-01-01 16:03:37,435 | INFO | Using 1 fonts\n",
      "2026-01-01 16:03:37,435 | INFO | Characters: 51\n",
      "2026-01-01 16:03:37,435 | INFO | ============================================================\n",
      "2026-01-01 16:03:37,531 | INFO |   âœ“ Saved content image for 'ð§¶' at my_dataset/train_original/ContentImage/U+27076_ð§¶_4204e0f3.png. \n",
      "2026-01-01 16:03:37,596 | INFO |   âœ“ Saved content image for 'ð¢£§' at my_dataset/train_original/ContentImage/U+228E7_ð¢£§_faa6bc72.png. \n",
      "2026-01-01 16:03:37,662 | INFO |   âœ“ Saved content image for 'ð¢«Š' at my_dataset/train_original/ContentImage/U+22ACA_ð¢«Š_fdd9ddfa.png. \n",
      "2026-01-01 16:03:37,730 | INFO |   âœ“ Saved content image for 'ð¤¼¸' at my_dataset/train_original/ContentImage/U+24F38_ð¤¼¸_9f6709ad.png. \n",
      "2026-01-01 16:03:37,794 | INFO |   âœ“ Saved content image for 'ð¢€²' at my_dataset/train_original/ContentImage/U+22032_ð¢€²_16330dfd.png. \n",
      "2026-01-01 16:03:37,859 | INFO |   âœ“ Saved content image for 'ð£‰¹' at my_dataset/train_original/ContentImage/U+23279_ð£‰¹_7640a887.png. \n",
      "2026-01-01 16:03:37,923 | INFO |   âœ“ Saved content image for 'ð ¨¡' at my_dataset/train_original/ContentImage/U+20A21_ð ¨¡_2c692b93.png. \n",
      "2026-01-01 16:03:37,988 | INFO |   âœ“ Saved content image for 'ð¡®¤' at my_dataset/train_original/ContentImage/U+21BA4_ð¡®¤_dc41b0f6.png. \n",
      "2026-01-01 16:03:38,053 | INFO |   âœ“ Saved content image for 'ð¤¹˜' at my_dataset/train_original/ContentImage/U+24E58_ð¤¹˜_470e0f75.png. \n",
      "2026-01-01 16:03:38,118 | INFO |   âœ“ Saved content image for 'ð¤¶‘' at my_dataset/train_original/ContentImage/U+24D91_ð¤¶‘_e756ab92.png. \n",
      "2026-01-01 16:03:38,191 | INFO |   âœ“ Saved content image for 'ðª†§' at my_dataset/train_original/ContentImage/U+2A1A7_ðª†§_eef802ce.png. \n",
      "2026-01-01 16:03:38,266 | INFO |   âœ“ Saved content image for 'ð©½¸' at my_dataset/train_original/ContentImage/U+29F78_ð©½¸_e374ea9b.png. \n",
      "2026-01-01 16:03:38,349 | INFO |   âœ“ Saved content image for 'ð¡´¯' at my_dataset/train_original/ContentImage/U+21D2F_ð¡´¯_d2b0ec94.png. \n",
      "2026-01-01 16:03:38,411 | INFO |   âœ“ Saved content image for 'ð§¹…' at my_dataset/train_original/ContentImage/U+27E45_ð§¹…_d5690cfd.png. \n",
      "2026-01-01 16:03:38,473 | INFO |   âœ“ Saved content image for 'ð©µœ' at my_dataset/train_original/ContentImage/U+29D5C_ð©µœ_659ab384.png. \n",
      "2026-01-01 16:03:38,536 | INFO |   âœ“ Saved content image for 'æ´¡' at my_dataset/train_original/ContentImage/U+6D21_æ´¡_725610f7.png. \n",
      "2026-01-01 16:03:38,600 | INFO |   âœ“ Saved content image for 'ð¥Š£' at my_dataset/train_original/ContentImage/U+252A3_ð¥Š£_a1d05e4f.png. \n",
      "2026-01-01 16:03:38,665 | INFO |   âœ“ Saved content image for 'ð¡‘' at my_dataset/train_original/ContentImage/U+2145D_ð¡‘_0eb723e1.png. \n",
      "2026-01-01 16:03:38,733 | INFO |   âœ“ Saved content image for 'ð¢´‡' at my_dataset/train_original/ContentImage/U+22D07_ð¢´‡_2157a435.png. \n",
      "2026-01-01 16:03:38,797 | INFO |   âœ“ Saved content image for 'æ¤' at my_dataset/train_original/ContentImage/U+6364_æ¤_fc088daa.png. \n",
      "2026-01-01 16:03:38,862 | INFO |   âœ“ Saved content image for 'ð£™©' at my_dataset/train_original/ContentImage/U+23669_ð£™©_22c27f95.png. \n",
      "2026-01-01 16:03:38,931 | INFO |   âœ“ Saved content image for 'ð¨¢Ÿ' at my_dataset/train_original/ContentImage/U+2889F_ð¨¢Ÿ_c6baf354.png. \n",
      "2026-01-01 16:03:39,020 | INFO |   âœ“ Saved content image for 'ð¡±¸' at my_dataset/train_original/ContentImage/U+21C78_ð¡±¸_667ed7aa.png. \n",
      "2026-01-01 16:03:39,083 | INFO |   âœ“ Saved content image for 'ð§ºƒ' at my_dataset/train_original/ContentImage/U+27E83_ð§ºƒ_7ddd6d4f.png. \n",
      "2026-01-01 16:03:39,146 | INFO |   âœ“ Saved content image for 'ð¤ž¼' at my_dataset/train_original/ContentImage/U+247BC_ð¤ž¼_fa0308c8.png. \n",
      "2026-01-01 16:03:39,208 | INFO |   âœ“ Saved content image for 'ð«…®' at my_dataset/train_original/ContentImage/U+2B16E_ð«…®_de267cc2.png. \n",
      "2026-01-01 16:03:39,272 | INFO |   âœ“ Saved content image for 'å”' at my_dataset/train_original/ContentImage/U+5414_å”_687d5dfd.png. \n",
      "2026-01-01 16:03:39,336 | INFO |   âœ“ Saved content image for 'ð¢¶»' at my_dataset/train_original/ContentImage/U+22DBB_ð¢¶»_582670fb.png. \n",
      "2026-01-01 16:03:39,402 | INFO |   âœ“ Saved content image for 'ð§º€' at my_dataset/train_original/ContentImage/U+27E80_ð§º€_cd530be0.png. \n",
      "2026-01-01 16:03:39,467 | INFO |   âœ“ Saved content image for 'ð¨€Ž' at my_dataset/train_original/ContentImage/U+2800E_ð¨€Ž_2af7ad78.png. \n",
      "2026-01-01 16:03:39,532 | INFO |   âœ“ Saved content image for 'ð¨”' at my_dataset/train_original/ContentImage/U+2850D_ð¨”_4bb303ce.png. \n",
      "2026-01-01 16:03:39,602 | INFO |   âœ“ Saved content image for 'ð¨“¡' at my_dataset/train_original/ContentImage/U+284E1_ð¨“¡_8498f2ec.png. \n",
      "2026-01-01 16:03:39,691 | INFO |   âœ“ Saved content image for 'ð¥•„' at my_dataset/train_original/ContentImage/U+25544_ð¥•„_9bab944d.png. \n",
      "2026-01-01 16:03:39,756 | INFO |   âœ“ Saved content image for 'ð¡‹‚' at my_dataset/train_original/ContentImage/U+212C2_ð¡‹‚_668bb41f.png. \n",
      "2026-01-01 16:03:39,820 | INFO |   âœ“ Saved content image for 'ð¤½—' at my_dataset/train_original/ContentImage/U+24F57_ð¤½—_2c6c0d63.png. \n",
      "2026-01-01 16:03:39,882 | INFO |   âœ“ Saved content image for 'ð¥¯Œ' at my_dataset/train_original/ContentImage/U+25BCC_ð¥¯Œ_013aaca6.png. \n",
      "2026-01-01 16:03:39,944 | INFO |   âœ“ Saved content image for 'ð¥…ž' at my_dataset/train_original/ContentImage/U+2515E_ð¥…ž_d458d589.png. \n",
      "2026-01-01 16:03:40,006 | INFO |   âœ“ Saved content image for 'ð§ˆ' at my_dataset/train_original/ContentImage/U+27348_ð§ˆ_10d88f2f.png. \n",
      "2026-01-01 16:03:40,068 | INFO |   âœ“ Saved content image for 'ð¦ ˜' at my_dataset/train_original/ContentImage/U+26818_ð¦ ˜_1fe8d8ae.png. \n",
      "2026-01-01 16:03:40,131 | INFO |   âœ“ Saved content image for 'ð¤› ' at my_dataset/train_original/ContentImage/U+246E0_ð¤› _0c7ae691.png. \n",
      "2026-01-01 16:03:40,194 | INFO |   âœ“ Saved content image for 'ð§¤' at my_dataset/train_original/ContentImage/U+27901_ð§¤_ec1b1f65.png. \n",
      "2026-01-01 16:03:40,259 | INFO |   âœ“ Saved content image for 'ð¢²«' at my_dataset/train_original/ContentImage/U+22CAB_ð¢²«_2ec3f9cc.png. \n",
      "2026-01-01 16:03:40,349 | INFO |   âœ“ Saved content image for 'ð©™‹' at my_dataset/train_original/ContentImage/U+2964B_ð©™‹_6c4330a1.png. \n",
      "2026-01-01 16:03:40,413 | INFO |   âœ“ Saved content image for 'æˆ' at my_dataset/train_original/ContentImage/U+6408_æˆ_ab8621d6.png. \n",
      "2026-01-01 16:03:40,479 | INFO |   âœ“ Saved content image for 'ð¢´' at my_dataset/train_original/ContentImage/U+22D10_ð¢´_062f41cc.png. \n",
      "2026-01-01 16:03:40,542 | INFO |   âœ“ Saved content image for 'ð ½‹' at my_dataset/train_original/ContentImage/U+20F4B_ð ½‹_3a5b7bcf.png. \n",
      "2026-01-01 16:03:40,605 | INFO |   âœ“ Saved content image for 'ð ¸›' at my_dataset/train_original/ContentImage/U+20E1B_ð ¸›_b49cbab7.png. \n",
      "2026-01-01 16:03:40,668 | INFO |   âœ“ Saved content image for 'å«§' at my_dataset/train_original/ContentImage/U+5AE7_å«§_1e6a1c5b.png. \n",
      "2026-01-01 16:03:40,732 | INFO |   âœ“ Saved content image for 'ð¡²¤' at my_dataset/train_original/ContentImage/U+21CA4_ð¡²¤_ddf87397.png. \n",
      "2026-01-01 16:03:40,795 | INFO |   âœ“ Saved content image for 'ð¨†¢' at my_dataset/train_original/ContentImage/U+281A2_ð¨†¢_1222b7e6.png. \n",
      "2026-01-01 16:03:40,858 | INFO |   âœ“ Saved content image for 'ð¢š·' at my_dataset/train_original/ContentImage/U+226B7_ð¢š·_32602c0a.png. \n",
      "ðŸ“¸ Generating content images: 100%|\u001b[35mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 51/51 [00:03<00:00, 14.90it/s]\u001b[0m\n",
      "2026-01-01 16:03:40,858 | INFO | âœ“ Generated 51 content images\n",
      "2026-01-01 16:03:40,858 | INFO | ============================================================\n",
      "2026-01-01 16:03:40,861 | INFO | \n",
      "============================================================\n",
      "2026-01-01 16:03:40,861 | INFO |                    BATCH IMAGE GENERATION                   \n",
      "2026-01-01 16:03:40,861 | INFO | ============================================================\n",
      "2026-01-01 16:03:40,861 | INFO | Fonts:                1\n",
      "2026-01-01 16:03:40,861 | INFO | Styles:               15\n",
      "2026-01-01 16:03:40,861 | INFO | Characters (input):   51\n",
      "2026-01-01 16:03:40,861 | INFO | Characters (content): 51\n",
      "2026-01-01 16:03:40,861 | INFO | Batch size:           35\n",
      "2026-01-01 16:03:40,861 | INFO | Previously generated: 7485 unique pairs\n",
      "2026-01-01 16:03:40,861 | INFO | Unique chars seen:    549\n",
      "2026-01-01 16:03:40,861 | INFO | Unique styles used:   15\n",
      "2026-01-01 16:03:40,861 | INFO | ============================================================\n",
      "\n",
      "2026-01-01 16:03:40,861 | INFO | Using font: NomNaTong-Regular\n",
      "2026-01-01 16:03:40,862 | INFO | ============================================================\n",
      "\n",
      "2026-01-01 16:03:40,862 | INFO |   ðŸ”„ 1: Generating 50/51 new images            \n",
      "ðŸŽ¨ Generating styles:   0%|                              | 0/15 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if is_colab:\n",
    "  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\n",
    "else:\n",
    "  !uv pip install --upgrade \"huggingface-hub==0.25.2\" \"protobuf<5.0.0\" \"numpy<2.0.0\"\n",
    "%cd {OUTPUT_PATH}\n",
    "!accelerate launch --num_processes 1 \\\n",
    "    FontDiffusion/sample_batch.py \\\n",
    "    --characters \"FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\" \\\n",
    "    --style_images \"FontDiffusion/styles_images\" \\\n",
    "    --ckpt_dir \"ckpt/\" \\\n",
    "    --ttf_path \"FontDiffusion/fonts/NomNaTong-Regular.otf\" \\\n",
    "    --output_dir \"my_dataset/train_original\" \\\n",
    "    --num_inference_steps 20 \\\n",
    "    --guidance_scale 7.5 \\\n",
    "    --start_line 500 \\\n",
    "    --end_line 550 \\\n",
    "    --batch_size 35 \\\n",
    "    --save_interval 1 \\\n",
    "    --channels_last \\\n",
    "    --seed 42 \\\n",
    "    --compile \\\n",
    "    --enable_xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "997103e5-221c-40a1-a0d1-83a93e1030f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:49:26.184343Z",
     "iopub.status.busy": "2026-01-01T15:49:26.183659Z",
     "iopub.status.idle": "2026-01-01T15:49:26.442833Z",
     "shell.execute_reply": "2026-01-01T15:49:26.441774Z",
     "shell.execute_reply.started": "2026-01-01T15:49:26.184313Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "7485\n"
     ]
    }
   ],
   "source": [
    "!find my_dataset/train_original/ContentImage -type f | wc -l\n",
    "!find my_dataset/train_original/TargetImage -type f | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d3b79-f33c-41c6-be25-e80b2c1165a5",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !ls -lt my_dataset/train_original/ContentImage/*3594*\n",
    "!ls -l my_dataset/train_original/TargetImage/*1*\n",
    "# !ls -lt my_dataset/train_original/TargetImage/*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9250a14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-01T15:46:24.186667Z",
     "iopub.status.busy": "2026-01-01T15:46:24.186020Z",
     "iopub.status.idle": "2026-01-01T15:46:26.241487Z",
     "shell.execute_reply": "2026-01-01T15:46:26.240779Z",
     "shell.execute_reply.started": "2026-01-01T15:46:24.186631Z"
    },
    "id": "f9250a14",
    "outputId": "0f834d09-da00-4aa4-f486-6e70981b4137",
    "papermill": {
     "duration": 0.236541,
     "end_time": "2025-12-30T18:55:01.673705",
     "exception": false,
     "start_time": "2025-12-30T18:55:01.437164",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-01 15:46:24,423 | INFO | âœ“ Using source directory: my_dataset/train_original\n",
      "2026-01-01 15:46:24,428 | INFO | \n",
      "======================================================================\n",
      "2026-01-01 15:46:24,428 | INFO | FONTDIFFUSION VALIDATION SPLIT CREATOR\n",
      "2026-01-01 15:46:24,428 | INFO | ======================================================================\n",
      "2026-01-01 15:46:24,428 | INFO | \n",
      "======================================================================\n",
      "2026-01-01 15:46:24,428 | INFO | ANALYZING TRAINING DATA\n",
      "2026-01-01 15:46:24,428 | INFO | ======================================================================\n",
      "2026-01-01 15:46:24,428 | INFO | \n",
      "ðŸ” Scanning content images...\n",
      "Content images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [00:00<00:00, 413955.24img/s]\n",
      "2026-01-01 15:46:24,432 | INFO |   âœ“ Found 499 content images\n",
      "2026-01-01 15:46:24,432 | INFO | \n",
      "ðŸ” Scanning target images...\n",
      "Styles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 389.00style/s]\n",
      "2026-01-01 15:46:24,471 | INFO |   âœ“ Found 7305 valid target images\n",
      "2026-01-01 15:46:24,471 | INFO | \n",
      "âš ï¸  PARSE ERROR DIAGNOSTICS:\n",
      "2026-01-01 15:46:24,471 | INFO |   Total parse errors: 180\n",
      "2026-01-01 15:46:24,471 | INFO | \n",
      "  First 10 unparseable files:\n",
      "2026-01-01 15:46:24,471 | INFO |     Folder: 1\n",
      "2026-01-01 15:46:24,471 | INFO |     File:   U+F041E_1_f1ade516.png\n",
      "2026-01-01 15:46:24,471 | INFO |     Parts:  ['U+F041E', '1', 'f1ade516'] (count: 3)\n",
      "2026-01-01 15:46:24,471 | INFO |     Folder: 1\n",
      "2026-01-01 15:46:24,471 | INFO |     File:   U+F0239_1_b09e6d97.png\n",
      "2026-01-01 15:46:24,471 | INFO |     Parts:  ['U+F0239', '1', 'b09e6d97'] (count: 3)\n",
      "2026-01-01 15:46:24,471 | INFO |     Folder: 1\n",
      "2026-01-01 15:46:24,471 | INFO |     File:   U+F01D5_1_953d66a4.png\n",
      "2026-01-01 15:46:24,472 | INFO |     Parts:  ['U+F01D5', '1', '953d66a4'] (count: 3)\n",
      "2026-01-01 15:46:24,472 | INFO |     Folder: 1\n",
      "2026-01-01 15:46:24,472 | INFO |     File:   U+F0199_1_b324bcb5.png\n",
      "2026-01-01 15:46:24,472 | INFO |     Parts:  ['U+F0199', '1', 'b324bcb5'] (count: 3)\n",
      "2026-01-01 15:46:24,472 | INFO |     Folder: 1\n",
      "2026-01-01 15:46:24,472 | INFO |     File:   U+F03BE_1_b2f70c79.png\n",
      "2026-01-01 15:46:24,472 | INFO |     Parts:  ['U+F03BE', '1', 'b2f70c79'] (count: 3)\n",
      "2026-01-01 15:46:24,472 | INFO |     Folder: 1\n",
      "2026-01-01 15:46:24,472 | INFO |     File:   U+F03D0_1_91c25ad5.png\n",
      "2026-01-01 15:46:24,472 | INFO |     Parts:  ['U+F03D0', '1', '91c25ad5'] (count: 3)\n",
      "2026-01-01 15:46:24,472 | INFO |     Folder: 1\n",
      "2026-01-01 15:46:24,472 | INFO |     File:   U+F04F1_1_838495e1.png\n",
      "2026-01-01 15:46:24,472 | INFO |     Parts:  ['U+F04F1', '1', '838495e1'] (count: 3)\n",
      "2026-01-01 15:46:24,472 | INFO |     Folder: 1\n",
      "2026-01-01 15:46:24,472 | INFO |     File:   U+F0425_1_729ff986.png\n",
      "2026-01-01 15:46:24,472 | INFO |     Parts:  ['U+F0425', '1', '729ff986'] (count: 3)\n",
      "2026-01-01 15:46:24,472 | INFO |     Folder: 1\n",
      "2026-01-01 15:46:24,472 | INFO |     File:   U+F0586_1_52d88662.png\n",
      "2026-01-01 15:46:24,472 | INFO |     Parts:  ['U+F0586', '1', '52d88662'] (count: 3)\n",
      "2026-01-01 15:46:24,472 | INFO |     Folder: 1\n",
      "2026-01-01 15:46:24,472 | INFO |     File:   U+F0598_1_cfd80750.png\n",
      "2026-01-01 15:46:24,472 | INFO |     Parts:  ['U+F0598', '1', 'cfd80750'] (count: 3)\n",
      "2026-01-01 15:46:24,472 | INFO |     ... and 170 more\n",
      "2026-01-01 15:46:24,472 | INFO | \n",
      "ðŸ” Validating content â†” target pairs...\n",
      "Validating pairs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7305/7305 [00:00<00:00, 2413500.65pair/s]\n",
      "2026-01-01 15:46:24,477 | INFO | \n",
      "======================================================================\n",
      "2026-01-01 15:46:24,477 | INFO | ðŸ“Š DATA ANALYSIS SUMMARY\n",
      "2026-01-01 15:46:24,477 | INFO | ======================================================================\n",
      "2026-01-01 15:46:24,477 | INFO | Content images found:        499\n",
      "2026-01-01 15:46:24,477 | INFO | Target images scanned:       7,305\n",
      "2026-01-01 15:46:24,477 | INFO |   â”œâ”€ Parse errors:          180\n",
      "2026-01-01 15:46:24,477 | INFO |   â””â”€ Style mismatches:      0\n",
      "2026-01-01 15:46:24,477 | INFO | Target images after filter:  7,305\n",
      "2026-01-01 15:46:24,477 | INFO | Missing content images:      0\n",
      "2026-01-01 15:46:24,477 | INFO | Final valid pairs:           7,305\n",
      "2026-01-01 15:46:24,477 | INFO | ======================================================================\n",
      "2026-01-01 15:46:24,477 | INFO | \n",
      "âš ï¸  IMAGE LOSS BREAKDOWN:\n",
      "2026-01-01 15:46:24,477 | INFO |   Total scanned:          7,305\n",
      "2026-01-01 15:46:24,478 | INFO |   Lost to parse errors:   180 (2.46%)\n",
      "2026-01-01 15:46:24,478 | INFO |   Lost to style mismatch: 0 (0.00%)\n",
      "2026-01-01 15:46:24,478 | INFO |   Lost to missing content:0 (0.00%)\n",
      "2026-01-01 15:46:24,478 | INFO |   Total lost:             180 (2.46%)\n",
      "2026-01-01 15:46:24,478 | INFO |   Usable for split:       7,305 (100.00%)\n",
      "2026-01-01 15:46:24,478 | INFO | \n",
      "======================================================================\n",
      "2026-01-01 15:46:24,478 | INFO | CREATING TRAIN/VAL SPLITS (random char & style)\n",
      "2026-01-01 15:46:24,478 | INFO | ======================================================================\n",
      "2026-01-01 15:46:24,479 | INFO | \n",
      "ðŸ“Š Split Statistics:\n",
      "2026-01-01 15:46:24,479 | INFO |   Total chars: 499 â†’ train: 400, val: 99\n",
      "2026-01-01 15:46:24,479 | INFO |   Total styles: 15 â†’ train: 12, val: 3\n",
      "2026-01-01 15:46:24,479 | INFO | \n",
      "  train:\n",
      "2026-01-01 15:46:24,479 | INFO |     Chars: 400\n",
      "2026-01-01 15:46:24,479 | INFO |     Styles: 12\n",
      "2026-01-01 15:46:24,479 | INFO | \n",
      "  val:\n",
      "2026-01-01 15:46:24,479 | INFO |     Chars: 99\n",
      "2026-01-01 15:46:24,479 | INFO |     Styles: 3\n",
      "2026-01-01 15:46:24,479 | INFO | \n",
      "ðŸ“ CREATING TRAIN SPLIT...\n",
      "2026-01-01 15:46:24,479 | INFO | \n",
      "  ðŸ“¥ Copying content images for train...\n",
      "2026-01-01 15:46:24,581 | INFO |   ðŸ“¥ Copying target images for train...        \n",
      "2026-01-01 15:46:25,865 | INFO |   âœ“ train: 400 content, 4,680 target (skipped: 0)\n",
      "2026-01-01 15:46:25,865 | INFO | \n",
      "  ðŸ“‹ Filtering checkpoint for train...\n",
      "2026-01-01 15:46:25,955 | INFO |     âœ“ Saved: 4,680/7,485 generations           \n",
      "2026-01-01 15:46:25,957 | INFO | \n",
      "ðŸ“ CREATING VAL SPLIT...\n",
      "2026-01-01 15:46:25,958 | INFO | \n",
      "  ðŸ“¥ Copying content images for val...\n",
      "2026-01-01 15:46:25,977 | INFO |   ðŸ“¥ Copying target images for val...          \n",
      "2026-01-01 15:46:26,048 | INFO |   âœ“ val: 99 content, 291 target (skipped: 0)   \n",
      "2026-01-01 15:46:26,049 | INFO | \n",
      "  ðŸ“‹ Filtering checkpoint for val...\n",
      "2026-01-01 15:46:26,102 | INFO |     âœ“ Saved: 291/7,485 generations             \n",
      "2026-01-01 15:46:26,104 | INFO | \n",
      "âœ“ Saved split metadata to my_dataset/split_info.json\n",
      "2026-01-01 15:46:26,105 | INFO | \n",
      "======================================================================\n",
      "2026-01-01 15:46:26,105 | INFO | âœ“ SPLIT CREATION COMPLETE\n",
      "2026-01-01 15:46:26,105 | INFO | ======================================================================\n",
      "2026-01-01 15:46:26,105 | INFO | \n",
      "âœ… Created:\n",
      "2026-01-01 15:46:26,105 | INFO |   ðŸ“ train/\n",
      "2026-01-01 15:46:26,105 | INFO |     â”œâ”€â”€ ContentImage/ (training chars)\n",
      "2026-01-01 15:46:26,105 | INFO |     â”œâ”€â”€ TargetImage/ (training styles)\n",
      "2026-01-01 15:46:26,105 | INFO |     â””â”€â”€ results_checkpoint.json (filtered)\n",
      "2026-01-01 15:46:26,105 | INFO |   ðŸ“ val/\n",
      "2026-01-01 15:46:26,105 | INFO |     â”œâ”€â”€ ContentImage/ (validation chars)\n",
      "2026-01-01 15:46:26,105 | INFO |     â”œâ”€â”€ TargetImage/ (validation styles)\n",
      "2026-01-01 15:46:26,105 | INFO |     â””â”€â”€ results_checkpoint.json (filtered)\n",
      "2026-01-01 15:46:26,105 | INFO | \n",
      "ðŸ’¡ Guarantees:\n",
      "2026-01-01 15:46:26,105 | INFO |   âœ“ Every target has matching content\n",
      "2026-01-01 15:46:26,105 | INFO |   âœ“ Checkpoint contains only relevant generations\n",
      "2026-01-01 15:46:26,105 | INFO |   âœ“ Train and val are completely disjoint\n"
     ]
    }
   ],
   "source": [
    "!python FontDiffusion/create_validation_split.py \\\n",
    "  --data_root my_dataset \\\n",
    "  --val_ratio 0.2 \\\n",
    "  --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79508d80-fac1-4318-9174-a32613a557e3",
   "metadata": {
    "id": "79508d80-fac1-4318-9174-a32613a557e3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!uv pip install --upgrade pyarrow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vRL8QovYCvLY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vRL8QovYCvLY",
    "outputId": "08301c52-4ae1-4268-c516-2ff8bd834783",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "HF_USERNAME = \"dzungpham\"\n",
    "!python FontDiffusion/create_hf_dataset.py \\\n",
    "  --data_dir \"my_dataset/train_original\" \\\n",
    "  --repo_id dzungpham/font-diffusion-generated-data \\\n",
    "  --split \"train_original\" \\\n",
    "  --token {HF_TOKEN}\n",
    "\n",
    "# # Train Split\n",
    "# !python FontDiffusion/create_hf_dataset.py \\\n",
    "#   --data_dir \"my_dataset/train\" \\\n",
    "#   --repo_id dzungpham/font-diffusion-generated-data \\\n",
    "#   --split \"train\" \\\n",
    "#   --token {HF_TOKEN}\n",
    "\n",
    "# # Train Split\n",
    "# !python FontDiffusion/create_hf_dataset.py \\\n",
    "#   --data_dir \"my_dataset/val\" \\\n",
    "#   --repo_id dzungpham/font-diffusion-generated-data \\\n",
    "#   --split \"val\" \\\n",
    "#   --token {HF_TOKEN}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87caab2",
   "metadata": {
    "id": "a87caab2",
    "papermill": {
     "duration": 1.992585,
     "end_time": "2025-12-30T18:55:24.769269",
     "exception": false,
     "start_time": "2025-12-30T18:55:22.776684",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81a2db-1f85-4801-a96e-f0bda7f3f315",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r my_dataset/train_original/ my_dataset/train/\n",
    "!ls -l my_dataset/train/*/*/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267634e8",
   "metadata": {
    "id": "267634e8",
    "papermill": {
     "duration": 0.021927,
     "end_time": "2025-12-30T18:55:24.807644",
     "exception": false,
     "start_time": "2025-12-30T18:55:24.785717",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TRAINING PHASE 1\n",
    "if is_colab:\n",
    "  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\n",
    "else:\n",
    "  !uv pip install --upgrade \"huggingface-hub==0.25.2\"\n",
    "import wandb\n",
    "# Run the training script with the corrected flag syntax\n",
    "!accelerate launch FontDiffusion/my_train.py \\\n",
    "    --seed=123 \\\n",
    "    --experience_name=FontDiffuser_training_phase_1 \\\n",
    "    --phase_1_ckpt_dir=\"ckpt\" \\\n",
    "    --data_root=my_dataset \\\n",
    "    --output_dir=outputs/FontDiffuser \\\n",
    "    --report_to=wandb \\\n",
    "\n",
    "    --resolution=96 \\\n",
    "    --style_image_size=96 \\\n",
    "    --content_image_size=96 \\\n",
    "    --content_encoder_downsample_size=3 \\\n",
    "    --channel_attn=True \\\n",
    "    --content_start_channel=64 \\\n",
    "    --style_start_channel=64 \\\n",
    "      \n",
    "    --train_batch_size=8 \\\n",
    "    --gradient_accumulation_steps=1 \\\n",
    "    --perceptual_coefficient=0.03 \\\n",
    "    --offset_coefficient=0.7 \\\n",
    "\n",
    "    --max_train_steps=200 \\\n",
    "    --ckpt_interval=100 \\\n",
    "    --val_interval=10000 \\\n",
    "    --log_interval=50 \\\n",
    "\n",
    "    --learning_rate=1e-4 \\\n",
    "    --lr_scheduler=linear \\\n",
    "    --lr_warmup_steps=10000 \\\n",
    "    --drop_prob=0.1 \\\n",
    "    --mixed_precision=no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a3326-6bfa-4fba-bbcc-e2e7be996c7f",
   "metadata": {
    "id": "cb2a3326-6bfa-4fba-bbcc-e2e7be996c7f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls -lr outputs/FontDiffuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8136e",
   "metadata": {
    "id": "97f8136e",
    "papermill": {
     "duration": 0.022471,
     "end_time": "2025-12-30T18:55:24.845778",
     "exception": false,
     "start_time": "2025-12-30T18:55:24.823307",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TRAINING PHASE 2\n",
    "!wandb login\n",
    "!python FontDiffusion/my_train.py \\\n",
    "    --seed=123 \\\n",
    "    --experience_name=\"FontDiffuser_training_phase_2\" \\\n",
    "    --data_root=\"my_dataset\" \\\n",
    "    --output_dir=\"outputs/FontDiffuser\" \\\n",
    "    --report_to=\"wandb\" \\\n",
    "    --phase_2 \\\n",
    "    --phase_1_ckpt_dir=\"outputs/FontDiffuser/global_step_2000\" \\\n",
    "    --scr_ckpt_path=\"ckpt/scr_210000.pth\" \\\n",
    "    --sc_coefficient=0.05 \\\n",
    "    --num_neg=13 \\\n",
    "    --resolution=96 \\\n",
    "    --style_image_size=96 \\\n",
    "    --content_image_size=96 \\\n",
    "    --content_encoder_downsample_size=3 \\\n",
    "    --channel_attn=True \\\n",
    "    --content_start_channel=64 \\\n",
    "    --style_start_channel=64 \\\n",
    "    --train_batch_size=8 \\\n",
    "    --perceptual_coefficient=0.03 \\\n",
    "    --offset_coefficient=0.4 \\\n",
    "    --max_train_steps=100 \\\n",
    "    --ckpt_interval=50 \\\n",
    "    --gradient_accumulation_steps=2 \\\n",
    "    --log_interval=50 \\\n",
    "    --learning_rate=1e-5 \\\n",
    "    --lr_scheduler=\"constant\" \\\n",
    "    --lr_warmup_steps=1000 \\\n",
    "    --drop_prob=0.1 \\\n",
    "    --mixed_precision=\"no\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c45e2f",
   "metadata": {
    "id": "88c45e2f",
    "papermill": {
     "duration": 0.217876,
     "end_time": "2025-12-30T18:55:25.079820",
     "exception": false,
     "start_time": "2025-12-30T18:55:24.861944",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python FontDiffusion/pth2safetensors.py \\\n",
    "    --weights_dir \"ckpt\" \\\n",
    "    --repo_id \"dzungpham/font-diffusion-weights\" \\\n",
    "    --token \"{HF_TOKEN}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868b20b",
   "metadata": {
    "id": "5868b20b",
    "papermill": {
     "duration": 0.031197,
     "end_time": "2025-12-30T18:55:25.126961",
     "exception": false,
     "start_time": "2025-12-30T18:55:25.095764",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "def find_result_folders(base_path: Path, pattern_name: str) -> List[Path]:\n",
    "    return [p for p in base_path.glob(pattern_name) if p.is_dir()]\n",
    "\n",
    "def zip_folder(folder_path: Path, output_base_path: Path) -> bool:\n",
    "    folder_name = folder_path.name\n",
    "    zip_path = output_base_path / f\"{folder_name}.zip\"\n",
    "    try:\n",
    "        print(f\"   -> Zipping folder: {folder_name}...\")\n",
    "        with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for file_path in folder_path.rglob(\"*\"):\n",
    "                if file_path.is_file():\n",
    "                    arcname = file_path.relative_to(folder_path.parent)\n",
    "                    zipf.write(file_path, arcname)\n",
    "        print(f\"   âœ… Created ZIP: {zip_path.name}\")\n",
    "        return True\n",
    "    except Exception as exc:\n",
    "        print(f\"   âŒ Failed to zip {folder_name}: {exc}\")\n",
    "        return False\n",
    "\n",
    "def zip_stats_results_folders(output_base_path: str, pattern_name: str) -> None:\n",
    "    base = Path(output_base_path)\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "    result_folders = find_result_folders(base, pattern_name)\n",
    "    if not result_folders:\n",
    "        print(f\"âš ï¸ No folders matching '*dataset' found in '{output_base_path}'.\")\n",
    "        return\n",
    "    print(f\"ðŸ” Found {len(result_folders)} result folder(s) to zip.\")\n",
    "    successful = sum(1 for folder in result_folders if zip_folder(folder, base))\n",
    "    print(f\"\\nâœ… DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n",
    "        if not output_root:\n",
    "            raise ValueError(\"OUTPUT_PATH not defined\")\n",
    "        zip_stats_results_folders(\n",
    "            output_base_path=OUTPUT_PATH,\n",
    "            pattern_name=\"my_dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 127.636509,
   "end_time": "2025-12-30T18:55:25.961447",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-30T18:53:18.324938",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
