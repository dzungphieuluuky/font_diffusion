{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":127.636509,"end_time":"2025-12-30T18:55:25.961447","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-30T18:53:18.324938","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a95a46ef","cell_type":"code","source":"# @title Environment Setup\nimport os\nimport sys\nif 'MPLBACKEND' in os.environ:\n    del os.environ['MPLBACKEND']\n    print(\"MPLBACKEND environment variable cleared.\")\n\n# 2. Clone the repository\n!rm -rf FontDiffusion\n!git clone https://github.com/dzungphieuluuky/FontDiffusion.git","metadata":{"id":"a95a46ef","outputId":"d76d28cd-6292-42bf-fffa-a8c7efb86ed0","papermill":{"duration":12.857369,"end_time":"2025-12-30T18:53:35.066181","exception":false,"start_time":"2025-12-30T18:53:22.208812","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"id":"9cdd8666","cell_type":"code","source":"import os\nimport sys\nfrom IPython import get_ipython\nfrom typing import Optional\n\ndef configure_environment_paths():\n    try:\n        if \"google.colab\" in str(get_ipython()):\n            print(\"‚úÖ Environment: Google Colab\")\n            base_data_path = \"/content/\"\n            base_output_path = \"/content/\"\n            environment_name = \"colab\"\n        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n            print(\"‚úÖ Environment: Kaggle\")\n            base_data_path = \"/kaggle/input/\"\n            base_output_path = \"/kaggle/working/\"\n            environment_name = \"kaggle\"\n        else:\n            print(\"‚ö†Ô∏è Environment: Local/Unknown\")\n            base_data_path = \"./data/\"\n            base_output_path = \"./output/\"\n            environment_name = \"local\"\n    except NameError:\n        print(\"‚ö†Ô∏è Non-interactive session. Using local paths.\")\n        base_data_path = \"./data/\"\n        base_output_path = \"./output/\"\n        environment_name = \"local\"\n    os.makedirs(base_output_path, exist_ok=True)\n    print(f\"üìÇ Data Path: {base_data_path}\")\n    print(f\"üì¶ Output Path: {base_output_path}\")\n    return base_data_path, base_output_path, environment_name\n\ndef load_secret(key_name: str) -> Optional[str]:\n    env = ENV_NAME\n    secret_value = None\n    print(f\"Attempting to load secret '{key_name}' from '{env}' environment...\")\n    try:\n        if env == \"colab\":\n            from google.colab import userdata\n            secret_value = userdata.get(key_name)\n        elif env == \"kaggle\":\n            from kaggle_secrets import UserSecretsClient\n            user_secrets = UserSecretsClient()\n            secret_value = user_secrets.get_secret(key_name)\n        else:\n            secret_value = os.getenv(key_name)\n        if not secret_value:\n            print(f\"‚ö†Ô∏è Secret '{key_name}' not found in the {env} environment.\")\n            return None\n        print(f\"‚úÖ Successfully loaded secret '{key_name}'.\")\n        return secret_value\n    except Exception as e:\n        print(f\"‚ùå An error occurred while loading secret '{key_name}': {e}\")\n        return None\n\ndef print_system_info():\n    print(\"\\nüîß System Information\")\n    print(f\"Python version: {sys.version.split()[0]}\")\n    try:\n        import torch\n        print(f\"PyTorch version: {torch.__version__}\")\n        if torch.cuda.is_available():\n            print(f\"CUDA version: {torch.version.cuda}\")\n            print(f\"GPU count: {torch.cuda.device_count()}\")\n            for i in range(torch.cuda.device_count()):\n                print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n        else:\n            print(\"CUDA not available\")\n    except ImportError:\n        print(\"PyTorch not installed\")\n    finally:\n      !nvidia-smi\n\nINPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()\nis_kaggle = (\"kaggle\" in ENV_NAME)\nis_colab = not is_kaggle\nprint_system_info()\n\nos.environ[\"WANDB_API_KEY\"] = wandb_key = load_secret(\"WANDB_API_KEY\")\nos.environ[\"HF_TOKEN\"] = HF_TOKEN = load_secret('HF_TOKEN')\n\n# Now, these libraries will log in automatically\nimport wandb\nimport huggingface_hub\n\nwandb.login() \nhuggingface_hub.login(token=os.environ[\"HF_TOKEN\"]) ","metadata":{"id":"9cdd8666","outputId":"8834f4e4-fc28-455c-a66c-d15b00de080a","papermill":{"duration":0.019157,"end_time":"2025-12-30T18:53:35.092303","exception":false,"start_time":"2025-12-30T18:53:35.073146","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"id":"a73b4150","cell_type":"code","source":"!uv pip install --upgrade pip\n# 3. Install PyTorch 1.13\n%cd {OUTPUT_PATH}\n# Force reinstall torch 1.13 to match the model's training environment\n# !uv pip uninstall torch torchvision\n# !uv pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n!uv pip install torch==2.9 torchvision\n# 4. Install other dependencies\n\nprint(\"\\n‚¨áÔ∏è Installing Dependencies (Manually fixed)...\")\n# Install xformers compatible with Torch 1.13\n!uv pip install xformers==0.0.16 -q\n\n# Install original dependencies\n!uv pip install transformers==4.33.1 accelerate==0.23.0 diffusers==0.22.0\n!uv pip install gradio==4.8.0 pyyaml pygame opencv-python info-nce-pytorch kornia\n# -----------------------------------------------------------------\n!uv pip install lpips scikit-image pytorch-fid\n!sudo apt-get update && sudo apt-get install dos2unix\n!uv pip install gdown tqdm\n!uv pip install wandb\n!uv pip install --upgrade pyarrow datasets\nprint(\"\\n‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\")","metadata":{"id":"a73b4150","outputId":"97db2cec-8e2d-438b-e5f8-38df08b7f59e","papermill":{"duration":61.239828,"end_time":"2025-12-30T18:54:36.338205","exception":false,"start_time":"2025-12-30T18:53:35.098377","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"id":"bd517dfe","cell_type":"code","source":"# KAGGLE CELL #1: Download checkpoint\nif is_colab:\n  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\nelse:\n  !uv pip install --upgrade \"huggingface-hub==0.25.2\" \"protobuf<5.0.0\" \"numpy<2.0.0\"\nimport os\nimport sys\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nos.chdir(OUTPUT_PATH)\n# Download from Hub\nif not os.path.exists(\"ckpt\") or not list(Path(\"ckpt\").glob(\"*.safetensors\")):\n    print(\"üì• Downloading checkpoint from Hugging Face Hub...\\n\")\n    from huggingface_hub import snapshot_download\n    snapshot_download(\n        repo_id=\"dzungpham/font-diffusion-weights\",\n        local_dir=\"ckpt\",\n        allow_patterns=\"*.safetensors\",\n        force_download=False\n    )\n    print(\"\\n‚úÖ Download complete!\")\nelse:\n    print(\"‚úÖ Checkpoint already downloaded\")\n# Verify\nprint(\"\\nüìÇ Files in ckpt/:\")\nfor file in os.listdir(\"ckpt\"):\n    if file.endswith(\".safetensors\"):\n        size = os.path.getsize(f\"ckpt/{file}\") / (1024**2)\n        print(f\"  ‚úì {file} ({size:.2f} MB)\")","metadata":{"id":"bd517dfe","outputId":"d83605e9-f5dc-4862-d1c9-b138a96ca47a","papermill":{"duration":12.524295,"end_time":"2025-12-30T18:54:48.878013","exception":false,"start_time":"2025-12-30T18:54:36.353718","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"id":"767e8ea2","cell_type":"code","source":"# @title Unzipping all archived files\nimport os\nimport glob\nfrom zipfile import ZipFile\n\nzip_file_paths = glob.glob(os.path.join(INPUT_PATH, '*.zip'))\n\nif not zip_file_paths:\n    print(f'No .zip files found in {INPUT_PATH}.')\nelse:\n    for zip_file_path in zip_file_paths:\n        if os.path.exists(zip_file_path):\n            print(f'Unzipping {zip_file_path}...')\n            !unzip -o {zip_file_path} -d ./\n            print(f'Unzipping of {zip_file_path} complete.')\n        else:\n            print(f'Error: The file {zip_file_path} was not found (post-glob check).')","metadata":{"id":"767e8ea2","outputId":"20185e27-e772-4823-e6bc-d9bd6d0b39a1","papermill":{"duration":0.023805,"end_time":"2025-12-30T18:54:48.917163","exception":false,"start_time":"2025-12-30T18:54:48.893358","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"id":"51941368","cell_type":"code","source":"import pandas as pd\nimport os\ndef convert_csv_to_chars_txt(input_csv_path: str, output_txt_path: str, column_name: str = 'word'):\n    \"\"\"\n    Reads a CSV file, extracts text from a specified column, and writes each character\n    to a new line in a plain text file.\n    Args:\n        input_csv_path (str): The full path to the input CSV file.\n        output_txt_path (str): The full path for the output text file.\n        column_name (str): The name of the column in the CSV file containing the text.\n    \"\"\"\n    if not os.path.exists(input_csv_path):\n        print(f\"Error: Input CSV file not found at '{input_csv_path}'. Please ensure the file is uploaded.\")\n        return\n    try:\n        df = pd.read_csv(input_csv_path)\n    except Exception as e:\n        print(f\"Error reading CSV file '{input_csv_path}': {e}\")\n        return\n    if column_name not in df.columns:\n        print(f\"Error: Column '{column_name}' not found in the CSV file '{input_csv_path}'.\")\n        return\n    all_characters = []\n    for item in df[column_name].astype(str).dropna().tolist():\n        for char in item:\n            all_characters.append(char)\n    os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)\n    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(all_characters))\n    print(f\"Successfully converted '{input_csv_path}' to '{output_txt_path}', with one character per line.\")\nprint(\"\\n--- Demonstrating function with a dummy CSV file ---\")\ndummy_csv_path = os.path.join(OUTPUT_PATH, \"dummy_data.csv\")\ndummy_output_txt_path = os.path.join(OUTPUT_PATH, \"dummy_chars.txt\")\ndummy_data = {'word': ['hello', 'world', 'python']}\npd.DataFrame(dummy_data).to_csv(dummy_csv_path, index=False)\nprint(f\"Created a dummy CSV file at: {dummy_csv_path}\")\nconvert_csv_to_chars_txt(dummy_csv_path, dummy_output_txt_path)","metadata":{"id":"51941368","outputId":"2a2c352c-968a-4e4d-b4cc-88a02c7eb788","papermill":{"duration":1.62157,"end_time":"2025-12-30T18:54:50.594793","exception":false,"start_time":"2025-12-30T18:54:48.973223","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"id":"4f4cf20b","cell_type":"code","source":"print(\"Model files:\")\n!ls -larth {OUTPUT_PATH}/ckpt","metadata":{"id":"4f4cf20b","outputId":"335f4192-47e7-451a-e14f-e0bd69fbdfc9","papermill":{"duration":0.140282,"end_time":"2025-12-30T18:54:50.749810","exception":false,"start_time":"2025-12-30T18:54:50.609528","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"id":"92cff682","cell_type":"code","source":"%cd {OUTPUT_PATH}\n# ==========================================\n# EXPORT / DOWNLOAD DATASET COMMANDS\n# ==========================================\nHF_USERNAME = \"dzungpham\"\n# Train Split\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/train_original\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"train_original\" \\\n  --token HF_TOKEN\n\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/train\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"train\" \\\n  --token HF_TOKEN\n# Validation: Unseen Both\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/val\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"val\" \\\n  --token HF_TOKEN","metadata":{"id":"92cff682","papermill":{"duration":0.104394,"end_time":"2025-12-30T18:54:50.869230","exception":false,"start_time":"2025-12-30T18:54:50.764836","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"6db9c1d6-dd60-479c-92c4-2f653e4d48fd","cell_type":"code","source":"print(\"Fonts currently in fonts/ folder\")\n!ls -lt FontDiffusion/fonts\nprint(\"Styles in style_images/ folder\")\n!ls -l FontDiffusion/styles_images","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"eb05a6b7-6003-4377-bbd2-103bff55303b","cell_type":"code","source":"import json\nfrom pathlib import Path\nfrom typing import Set\n\ndef remove_unparseable_from_checkpoint(checkpoint_path: str, unparseable_txt_path: str) -> None:\n    \"\"\"\n    Removes generations referencing unparseable files from a results_checkpoint.json file.\n\n    Args:\n        checkpoint_path (str): Path to results_checkpoint.json.\n        unparseable_txt_path (str): Path to unparseable_files.txt (absolute paths, one per line).\n    \"\"\"\n    # Load unparseable file names into a set\n    with open(unparseable_txt_path, \"r\", encoding=\"utf-8\") as f:\n        unparseable_files: Set[str] = {Path(line.strip()).name for line in f if line.strip()}\n\n    # Load checkpoint JSON\n    with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n        results = json.load(f)\n\n    generations = results.get(\"generations\", [])\n    original_count = len(generations)\n\n    # Filter out generations whose target_image_path's filename is in unparseable_files\n    filtered_generations = [\n        gen for gen in generations\n        if os.path.join(\"kaggle/working/my_dataset/train_original\", gen.get(\"target_image_path\", \"\")) not in unparseable_files\n    ]\n\n    removed_count = original_count - len(filtered_generations)\n    results[\"generations\"] = filtered_generations\n\n    # Save updated checkpoint\n    with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=2, ensure_ascii=False)\n\n    print(f\"Removed {removed_count} generations from {checkpoint_path}.\")\n\n# Example usage:\nremove_unparseable_from_checkpoint(\n    \"my_dataset/train_original/results_checkpoint.json\",\n    \"my_dataset/unparseable_files.txt\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b32b36ce-f246-4f86-b9f1-2ac844c7bec8","cell_type":"code","source":"!cat my_dataset/unparseable_files.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"29deed1d","cell_type":"code","source":"if is_colab:\n  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\nelse:\n  !uv pip install --upgrade \"huggingface-hub==0.25.2\" \"protobuf<5.0.0\" \"numpy<2.0.0\"\n%cd {OUTPUT_PATH}\n!accelerate launch --num_processes 1 \\\n    FontDiffusion/sample_batch.py \\\n    --characters \"FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\" \\\n    --style_images \"FontDiffusion/styles_images\" \\\n    --ckpt_dir \"ckpt/\" \\\n    --ttf_path \"FontDiffusion/fonts/NomNaTong-Regular.otf\" \\\n    --output_dir \"my_dataset/train_original\" \\\n    --num_inference_steps 20 \\\n    --guidance_scale 7.5 \\\n    --start_line 1 \\\n    --end_line 500 \\\n    --batch_size 35 \\\n    --save_interval 1 \\\n    --channels_last \\\n    --seed 42 \\\n    --compile \\\n    --enable_xformers\n\nimport logging\nlogging.info(\"Updating Dataset after generation...\")\nHF_USERNAME = \"dzungpham\"\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/train_original\" \\\n  --repo_id dzungpham/font-diffusion-generated-data \\\n  --split \"train_original\" \\\n  --token {HF_TOKEN}\n","metadata":{"id":"29deed1d","outputId":"749b50d0-75e3-4d36-e509-919188feb64c","papermill":{"duration":10.53661,"end_time":"2025-12-30T18:55:01.421093","exception":false,"start_time":"2025-12-30T18:54:50.884483","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"id":"997103e5-221c-40a1-a0d1-83a93e1030f7","cell_type":"code","source":"!find my_dataset/train_original/ContentImage -type f | wc -l\n!find my_dataset/train_original/TargetImage -type f | wc -l","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"619d3b79-f33c-41c6-be25-e80b2c1165a5","cell_type":"code","source":"# !ls -lt my_dataset/train_original/ContentImage/*3594*\n!ls -l my_dataset/train_original/TargetImage/*1*\n# !ls -lt my_dataset/train_original/TargetImage/*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"97c4a6a0-dba9-46be-b6df-d79cce0df689","cell_type":"code","source":"import re\nfrom pathlib import Path\n\n# Your valid pattern\nexpected_pattern = r\"U\\+[0-9A-F]{4,5}.*_[0-9a-f]{8}\\.png\"\n\n# Define the root directory ('.' for current directory)\nroot_dir = Path('./my_dataset/train_original/TargetImage')\n\n# .rglob('*') finds every file recursively\nfor path in root_dir.rglob('*'):\n    # Process only files (ignore directories)\n    if path.is_file():\n        # Check if the FILENAME (path.name) matches the regex\n        if not re.match(expected_pattern, path.name):\n            try:\n                print(f\"Deleting invalid file: {path}\")\n                # path.unlink() # This deletes the file\n            except Exception as e:\n                print(f\"Error deleting {path}: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f9250a14","cell_type":"code","source":"!python FontDiffusion/create_validation_split.py \\\n  --data_root my_dataset \\\n  --val_ratio 0.2 \\\n  --seed 42","metadata":{"id":"f9250a14","outputId":"0f834d09-da00-4aa4-f486-6e70981b4137","papermill":{"duration":0.236541,"end_time":"2025-12-30T18:55:01.673705","exception":false,"start_time":"2025-12-30T18:55:01.437164","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"id":"79508d80-fac1-4318-9174-a32613a557e3","cell_type":"code","source":"!uv pip install --upgrade pyarrow datasets","metadata":{"trusted":true,"id":"79508d80-fac1-4318-9174-a32613a557e3"},"outputs":[],"execution_count":null},{"id":"48f97e84-cd8c-49a9-86bd-fce456be56a4","cell_type":"code","source":"# remove_unparseable_files.py\n\nwith open(\"my_dataset/unparseable_files.txt\", \"r\", encoding=\"utf-8\") as f:\n    paths = [line.strip() for line in f if line.strip()]\n\nimport os\n\nfor path in paths:\n    try:\n        if os.path.exists(path):\n            os.remove(path)\n            print(f\"Deleted: {path}\")\n        else:\n            print(f\"Not found: {path}\")\n    except Exception as e:\n        print(f\"Error deleting {path}: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"vRL8QovYCvLY","cell_type":"code","source":"HF_USERNAME = \"dzungpham\"\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/train_original\" \\\n  --repo_id dzungpham/font-diffusion-generated-data \\\n  --split \"train_original\" \\\n  --token {HF_TOKEN}\n\n# Train Split\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/train\" \\\n  --repo_id dzungpham/font-diffusion-generated-data \\\n  --split \"train\" \\\n  --token {HF_TOKEN}\n\n# Train Split\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/val\" \\\n  --repo_id dzungpham/font-diffusion-generated-data \\\n  --split \"val\" \\\n  --token {HF_TOKEN}\n","metadata":{"id":"vRL8QovYCvLY","outputId":"08301c52-4ae1-4268-c516-2ff8bd834783","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"outputs":[],"execution_count":null},{"id":"a87caab2","cell_type":"code","source":"import torch, gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"id":"a87caab2","papermill":{"duration":1.992585,"end_time":"2025-12-30T18:55:24.769269","exception":false,"start_time":"2025-12-30T18:55:22.776684","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"eb81a2db-1f85-4801-a96e-f0bda7f3f315","cell_type":"code","source":"!cp -r my_dataset/train_original/ my_dataset/train/\n!ls -l my_dataset/train/*/*/*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9c80e5ac-214f-4c4f-ab3e-99d0b083d527","cell_type":"code","source":"%%writefile rename_to_simple_format.py\n\"\"\"\nRename files from hash-based format to simple character+style format\nOLD: U+XXXX_[char]_hash.png ‚Üí NEW: char.png\nOLD: U+XXXX_[char]_style_hash.png ‚Üí NEW: style+char.png\n\"\"\"\n\nimport os\nimport json\nfrom pathlib import Path\nfrom typing import Dict, Tuple, Optional\nfrom collections import defaultdict\nfrom tqdm import tqdm\n\n\ndef parse_content_filename(filename: str) -> Optional[str]:\n    \"\"\"\n    Parse content filename to extract character\n    Format: U+XXXX_[char]_hash.png or U+XXXX_hash.png\n    Returns: character or None\n    \"\"\"\n    if not filename.endswith(\".png\"):\n        return None\n\n    stem = filename[:-4]\n    parts = stem.split(\"_\")\n\n    if len(parts) < 2:\n        return None\n\n    codepoint = parts[0]\n    if not codepoint.startswith(\"U+\"):\n        return None\n\n    try:\n        char_code = int(codepoint.replace(\"U+\", \"\"), 16)\n        char = chr(char_code)\n        return char\n    except (ValueError, OverflowError):\n        return None\n\n\ndef parse_target_filename(filename: str) -> Optional[Tuple[str, str]]:\n    \"\"\"\n    Parse target filename to extract character and style\n    Format: U+XXXX_[char]_style_hash.png or U+XXXX_style_hash.png\n\n    ‚úÖ CORRECTED: Uses hash validation + filename structure to determine format\n    \n    Returns:\n        (character, style) tuple or None if parsing fails\n    \"\"\"\n    if not filename.endswith(\".png\"):\n        return None\n\n    stem = filename[:-4]  # Remove .png\n    parts = stem.split(\"_\")\n\n    if len(parts) < 3:\n        return None\n\n    # First part should be codepoint\n    codepoint = parts[0]\n    if not codepoint.startswith(\"U+\"):\n        return None\n\n    try:\n        # Decode character from codepoint\n        char_code = int(codepoint.replace(\"U+\", \"\"), 16)\n        char = chr(char_code)\n\n        # Hash is ALWAYS the last part (must be exactly 8 hex characters)\n        hash_val = parts[-1]\n\n        # ‚úÖ STRICT validation: hash must be exactly 8 hex chars\n        if len(hash_val) != 8 or not all(\n            c in \"0123456789abcdef\" for c in hash_val.lower()\n        ):\n            return None\n\n        # Now work with parts[1:-1] (everything between codepoint and hash)\n        middle_parts = parts[1:-1]\n        \n        if len(middle_parts) == 0:\n            return None\n        \n        # If only 1 middle part: it's the style (char was omitted)\n        if len(middle_parts) == 1:\n            style = middle_parts[0]\n            return char, style\n        \n        # If 2+ middle parts: first is char, rest is style\n        # (character was included in filename)\n        style = \"_\".join(middle_parts[1:])\n        \n        if not style:\n            # If style is empty after joining, something is wrong\n            return None\n        \n        return char, style\n\n    except (ValueError, OverflowError, IndexError):\n        return None\ndef rename_content_images(content_dir: str) -> Dict[str, str]:\n    \"\"\"\n    Rename content images from U+XXXX_[char]_hash.png to char.png\n    \n    Args:\n        content_dir: Path to ContentImage directory\n    \n    Returns:\n        Mapping of old_name -> new_name\n    \"\"\"\n    content_path = Path(content_dir)\n    \n    if not content_path.exists():\n        raise FileNotFoundError(f\"ContentImage directory not found: {content_dir}\")\n\n    print(f\"\\nüîÑ Renaming content images in {content_dir}...\")\n    \n    png_files = sorted(content_path.glob(\"*.png\"))\n    \n    if not png_files:\n        print(f\"  ‚ö†Ô∏è  No PNG files found\")\n        return {}\n\n    rename_map = {}\n    failed = []\n    duplicates = defaultdict(list)\n\n    for old_file in tqdm(png_files, desc=\"  Processing content\", ncols=100, unit=\"file\"):\n        char = parse_content_filename(old_file.name)\n        \n        if char is None:\n            failed.append(old_file.name)\n            tqdm.write(f\"    ‚ö†Ô∏è  Failed to parse: {old_file.name}\")\n            continue\n\n        # New filename: just the character\n        new_filename = f\"{char}.png\"\n        new_path = content_path / new_filename\n\n        # Track duplicates\n        if new_path.exists() and str(old_file) != str(new_path):\n            duplicates[new_filename].append((old_file.name, new_filename))\n            tqdm.write(f\"    ‚ö†Ô∏è  Duplicate target: {old_file.name} ‚Üí {new_filename}\")\n            continue\n\n        # Rename file\n        try:\n            if old_file != new_path:\n                old_file.rename(new_path)\n                rename_map[old_file.name] = new_filename\n        except Exception as e:\n            tqdm.write(f\"    ‚ùå Error renaming {old_file.name}: {e}\")\n            failed.append(old_file.name)\n\n    print(f\"  ‚úì Renamed: {len(rename_map)}\")\n    if failed:\n        print(f\"  ‚ö†Ô∏è  Failed: {len(failed)}\")\n    if duplicates:\n        print(f\"  ‚ö†Ô∏è  Duplicates: {len(duplicates)}\")\n\n    return rename_map\n\n\ndef rename_target_images(target_dir: str) -> Dict[Tuple[str, str], Tuple[str, str]]:\n    \"\"\"\n    Rename target images from U+XXXX_[char]_style_hash.png to style+char.png\n    \n    Args:\n        target_dir: Path to TargetImage directory\n    \n    Returns:\n        Mapping of (old_style, old_name) -> (new_style, new_name)\n    \"\"\"\n    target_path = Path(target_dir)\n    \n    if not target_path.exists():\n        raise FileNotFoundError(f\"TargetImage directory not found: {target_dir}\")\n\n    print(f\"\\nüîÑ Renaming target images in {target_dir}...\")\n    \n    style_dirs = sorted([d for d in target_path.iterdir() if d.is_dir()])\n    \n    if not style_dirs:\n        print(f\"  ‚ö†Ô∏è  No style directories found\")\n        return {}\n\n    rename_map = {}\n    failed = []\n    duplicates = defaultdict(list)\n\n    for style_dir in tqdm(style_dirs, desc=\"  Processing styles\", ncols=100, unit=\"style\"):\n        style_name = style_dir.name\n        png_files = sorted(style_dir.glob(\"*.png\"))\n\n        if not png_files:\n            continue\n\n        for old_file in png_files:\n            parsed = parse_target_filename(old_file.name)\n            \n            if parsed is None:\n                failed.append((style_name, old_file.name))\n                tqdm.write(f\"    ‚ö†Ô∏è  Failed to parse: {style_name}/{old_file.name}\")\n                continue\n\n            char, parsed_style = parsed\n\n            # Validate style matches directory\n            if parsed_style != style_name:\n                failed.append((style_name, old_file.name))\n                tqdm.write(\n                    f\"    ‚ö†Ô∏è  Style mismatch: {style_name}/{old_file.name} \"\n                    f\"(extracted: {parsed_style})\"\n                )\n                continue\n\n            # New filename: style+char\n            new_filename = f\"{style_name}+{char}.png\"\n            new_path = style_dir / new_filename\n\n            # Track duplicates\n            if new_path.exists() and str(old_file) != str(new_path):\n                duplicates[(style_name, new_filename)].append(old_file.name)\n                tqdm.write(f\"    ‚ö†Ô∏è  Duplicate: {old_file.name} ‚Üí {new_filename}\")\n                continue\n\n            # Rename file\n            try:\n                if old_file != new_path:\n                    old_file.rename(new_path)\n                    rename_map[(style_name, old_file.name)] = (style_name, new_filename)\n            except Exception as e:\n                tqdm.write(f\"    ‚ùå Error renaming {old_file.name}: {e}\")\n                failed.append((style_name, old_file.name))\n\n    print(f\"  ‚úì Renamed: {len(rename_map)}\")\n    if failed:\n        print(f\"  ‚ö†Ô∏è  Failed: {len(failed)}\")\n    if duplicates:\n        print(f\"  ‚ö†Ô∏è  Duplicates: {len(duplicates)}\")\n\n    return rename_map\n\n\ndef update_checkpoint_filenames(\n    checkpoint_path: str,\n    content_rename_map: Dict[str, str],\n    target_rename_map: Dict[Tuple[str, str], Tuple[str, str]],\n) -> None:\n    \"\"\"\n    Update results_checkpoint.json with new filenames\n    \n    Args:\n        checkpoint_path: Path to results_checkpoint.json\n        content_rename_map: Mapping of old ‚Üí new content filenames\n        target_rename_map: Mapping of (style, old) ‚Üí (style, new) target filenames\n    \"\"\"\n    checkpoint_path = Path(checkpoint_path)\n    \n    if not checkpoint_path.exists():\n        print(f\"\\n‚ö†Ô∏è  Checkpoint not found: {checkpoint_path}\")\n        return\n\n    print(f\"\\nüìù Updating {checkpoint_path.name}...\")\n\n    with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n        checkpoint = json.load(f)\n\n    generations = checkpoint.get(\"generations\", [])\n    updated = 0\n    failed = 0\n\n    for gen in tqdm(generations, desc=\"  Updating\", ncols=100, unit=\"gen\"):\n        char = gen.get(\"character\")\n        style = gen.get(\"style\")\n        old_content_path = gen.get(\"content_image_path\", \"\")\n        old_target_path = gen.get(\"target_image_path\", \"\")\n\n        # Update content image path\n        old_content_filename = Path(old_content_path).name\n        if old_content_filename in content_rename_map:\n            new_content_filename = content_rename_map[old_content_filename]\n            gen[\"content_image_path\"] = f\"ContentImage/{new_content_filename}\"\n            gen[\"content_filename\"] = new_content_filename\n            updated += 1\n        else:\n            failed += 1\n\n        # Update target image path\n        old_target_filename = Path(old_target_path).name\n        key = (style, old_target_filename)\n        if key in target_rename_map:\n            new_style, new_target_filename = target_rename_map[key]\n            gen[\"target_image_path\"] = f\"TargetImage/{new_style}/{new_target_filename}\"\n            gen[\"target_filename\"] = new_target_filename\n            updated += 1\n        else:\n            failed += 1\n\n    # Save updated checkpoint\n    with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(checkpoint, f, indent=2, ensure_ascii=False)\n\n    print(f\"  ‚úì Updated: {updated} entries\")\n    if failed > 0:\n        print(f\"  ‚ö†Ô∏è  Failed: {failed} entries\")\n\n\ndef rename_dataset(data_root: str, split: str = \"train\") -> None:\n    \"\"\"\n    Rename all files in a dataset split from hash-based to simple format\n    \n    Args:\n        data_root: Root data directory\n        split: Dataset split (default: \"train\")\n    \"\"\"\n    split_dir = Path(data_root) / split\n    content_dir = split_dir / \"ContentImage\"\n    target_dir = split_dir / \"TargetImage\"\n    checkpoint_path = split_dir / \"results_checkpoint.json\"\n\n    print(\"\\n\" + \"=\" * 70)\n    print(f\"RENAMING DATASET: {split}\")\n    print(\"=\" * 70)\n    print(f\"Root: {data_root}\")\n    print(f\"Split: {split}\")\n    print(f\"  Content: {content_dir}\")\n    print(f\"  Target: {target_dir}\")\n    print(f\"  Checkpoint: {checkpoint_path}\")\n    print(\"=\" * 70)\n\n    # Rename content images\n    content_rename_map = rename_content_images(str(content_dir))\n\n    # Rename target images\n    target_rename_map = rename_target_images(str(target_dir))\n\n    # Update checkpoint\n    if checkpoint_path.exists():\n        update_checkpoint_filenames(\n            str(checkpoint_path),\n            content_rename_map,\n            target_rename_map,\n        )\n\n    print(\"\\n\" + \"=\" * 70)\n    print(\"‚úÖ RENAMING COMPLETE\")\n    print(\"=\" * 70)\n    print(f\"Content images renamed: {len(content_rename_map)}\")\n    print(f\"Target images renamed: {len(target_rename_map)}\")\n    print(\"=\" * 70 + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser(\n        description=\"Rename dataset files from hash-based to simple format\"\n    )\n    parser.add_argument(\n        \"--data_root\", type=str, required=True, help=\"Root data directory\"\n    )\n    parser.add_argument(\n        \"--split\", type=str, default=\"train\", help=\"Dataset split to rename\"\n    )\n\n    args = parser.parse_args()\n\n    try:\n        rename_dataset(args.data_root, args.split)\n    except Exception as e:\n        print(f\"\\n‚ùå Error: {e}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1cbf05ee-191f-4d20-a223-812270f17c1d","cell_type":"code","source":"!python rename_to_simple_format.py \\\n    --data_root my_dataset \\\n    --split train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"267634e8","cell_type":"code","source":"# TRAINING PHASE 1\nif is_colab:\n  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\nelse:\n  !uv pip install --upgrade \"huggingface-hub==0.25.2\"\nimport wandb\n# Run the training script with the corrected flag syntax\n!accelerate launch FontDiffusion/my_train.py \\\n    --seed=123 \\\n    --experience_name=FontDiffuser_training_phase_1 \\\n    --phase_1_ckpt_dir=\"ckpt\" \\\n    --data_root=my_dataset \\\n    --output_dir=outputs/FontDiffuser \\\n    --report_to=wandb \\\n    \\\n    --resolution=96 \\\n    --style_image_size=96 \\\n    --content_image_size=96 \\\n    --content_encoder_downsample_size=3 \\\n    --channel_attn=True \\\n    --content_start_channel=64 \\\n    --style_start_channel=64 \\\n    --train_batch_size=8 \\\n    --gradient_accumulation_steps=1 \\\n    --perceptual_coefficient=0.03 \\\n    --offset_coefficient=0.7 \\\n    \\\n    --max_train_steps=200 \\\n    --ckpt_interval=100 \\\n    --val_interval=100 \\\n    --log_interval=50 \\\n    \\\n    --learning_rate=1e-4 \\\n    --lr_scheduler=linear \\\n    --lr_warmup_steps=10000 \\\n    --drop_prob=0.1 \\\n    --mixed_precision=no","metadata":{"id":"267634e8","papermill":{"duration":0.021927,"end_time":"2025-12-30T18:55:24.807644","exception":false,"start_time":"2025-12-30T18:55:24.785717","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"cb2a3326-6bfa-4fba-bbcc-e2e7be996c7f","cell_type":"code","source":"!ls -lr outputs/FontDiffuser","metadata":{"trusted":true,"id":"cb2a3326-6bfa-4fba-bbcc-e2e7be996c7f"},"outputs":[],"execution_count":null},{"id":"97f8136e","cell_type":"code","source":"# TRAINING PHASE 2\n!wandb login\n!python FontDiffusion/my_train.py \\\n    --seed=123 \\\n    --experience_name=\"FontDiffuser_training_phase_2\" \\\n    --data_root=\"my_dataset\" \\\n    --output_dir=\"outputs/FontDiffuser\" \\\n    --report_to=\"wandb\" \\\n    --phase_2 \\\n    --phase_1_ckpt_dir=\"outputs/FontDiffuser/global_step_2000\" \\\n    --scr_ckpt_path=\"ckpt/scr_210000.pth\" \\\n    --sc_coefficient=0.05 \\\n    --num_neg=13 \\\n    --resolution=96 \\\n    --style_image_size=96 \\\n    --content_image_size=96 \\\n    --content_encoder_downsample_size=3 \\\n    --channel_attn=True \\\n    --content_start_channel=64 \\\n    --style_start_channel=64 \\\n    --train_batch_size=8 \\\n    --perceptual_coefficient=0.03 \\\n    --offset_coefficient=0.4 \\\n    --max_train_steps=100 \\\n    --ckpt_interval=50 \\\n    --gradient_accumulation_steps=2 \\\n    --log_interval=50 \\\n    --learning_rate=1e-5 \\\n    --lr_scheduler=\"constant\" \\\n    --lr_warmup_steps=1000 \\\n    --drop_prob=0.1 \\\n    --mixed_precision=\"no\"\n","metadata":{"id":"97f8136e","papermill":{"duration":0.022471,"end_time":"2025-12-30T18:55:24.845778","exception":false,"start_time":"2025-12-30T18:55:24.823307","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"88c45e2f","cell_type":"code","source":"!python FontDiffusion/pth2safetensors.py \\\n    --weights_dir \"ckpt\" \\\n    --repo_id \"dzungpham/font-diffusion-weights\" \\\n    --token \"{HF_TOKEN}\"","metadata":{"id":"88c45e2f","papermill":{"duration":0.217876,"end_time":"2025-12-30T18:55:25.079820","exception":false,"start_time":"2025-12-30T18:55:24.861944","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"5868b20b","cell_type":"code","source":"import os\nimport zipfile\nfrom pathlib import Path\nfrom typing import List\ndef find_result_folders(base_path: Path, pattern_name: str) -> List[Path]:\n    return [p for p in base_path.glob(pattern_name) if p.is_dir()]\n\ndef zip_folder(folder_path: Path, output_base_path: Path) -> bool:\n    folder_name = folder_path.name\n    zip_path = output_base_path / f\"{folder_name}.zip\"\n    try:\n        print(f\"   -> Zipping folder: {folder_name}...\")\n        with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zipf:\n            for file_path in folder_path.rglob(\"*\"):\n                if file_path.is_file():\n                    arcname = file_path.relative_to(folder_path.parent)\n                    zipf.write(file_path, arcname)\n        print(f\"   ‚úÖ Created ZIP: {zip_path.name}\")\n        return True\n    except Exception as exc:\n        print(f\"   ‚ùå Failed to zip {folder_name}: {exc}\")\n        return False\n\ndef zip_stats_results_folders(output_base_path: str, pattern_name: str) -> None:\n    base = Path(output_base_path)\n    base.mkdir(parents=True, exist_ok=True)\n    result_folders = find_result_folders(base, pattern_name)\n    if not result_folders:\n        print(f\"‚ö†Ô∏è No folders matching '*dataset' found in '{output_base_path}'.\")\n        return\n    print(f\"üîç Found {len(result_folders)} result folder(s) to zip.\")\n    successful = sum(1 for folder in result_folders if zip_folder(folder, base))\n    print(f\"\\n‚úÖ DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\")\n\nif __name__ == \"__main__\":\n    try:\n        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n        if not output_root:\n            raise ValueError(\"OUTPUT_PATH not defined\")\n        zip_stats_results_folders(\n            output_base_path=OUTPUT_PATH,\n            pattern_name=\"my_dataset\")\n    except Exception as e:\n        print(f\"‚ùå An error occurred: {e}\")","metadata":{"id":"5868b20b","papermill":{"duration":0.031197,"end_time":"2025-12-30T18:55:25.126961","exception":false,"start_time":"2025-12-30T18:55:25.095764","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}