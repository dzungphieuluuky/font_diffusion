{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# @title Environment Setup\nimport os\nimport sys\nif 'MPLBACKEND' in os.environ:\n    del os.environ['MPLBACKEND']\n    print(\"MPLBACKEND environment variable cleared.\")\n\n# 2. Clone the repository\n!rm -rf FontDiffusion\n!git clone https://github.com/dzungphieuluuky/FontDiffusion.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-30T13:29:07.563707Z","iopub.execute_input":"2025-12-30T13:29:07.563961Z","iopub.status.idle":"2025-12-30T13:29:20.472811Z","shell.execute_reply.started":"2025-12-30T13:29:07.563936Z","shell.execute_reply":"2025-12-30T13:29:20.472040Z"},"id":"BWFvN9XJxf9K","outputId":"399a7d0e-1ac5-4ef8-c2ab-c88139b580b1","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":["MPLBACKEND environment variable cleared.\n","Cloning into 'FontDiffusion'...\n","remote: Enumerating objects: 20086, done.\u001b[K\n","remote: Counting objects: 100% (4898/4898), done.\u001b[K\n","remote: Compressing objects: 100% (4877/4877), done.\u001b[K\n","remote: Total 20086 (delta 49), reused 4858 (delta 21), pack-reused 15188 (from 3)\u001b[K\n","Receiving objects: 100% (20086/20086), 277.16 MiB | 10.54 MiB/s, done.\n","Resolving deltas: 100% (612/612), done.\n","Updating files: 100% (136/136), done.\n"]}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom IPython import get_ipython\nfrom typing import Optional\n\ndef configure_environment_paths():\n    \"\"\"Detect environment and configure paths\"\"\"\n    try:\n        if \"google.colab\" in str(get_ipython()):\n            print(\"âœ… Environment: Google Colab\")\n            base_data_path = \"/content/\"\n            base_output_path = \"/content/\"\n            environment_name = \"colab\"\n        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n            print(\"âœ… Environment: Kaggle\")\n            base_data_path = \"/kaggle/input/\"\n            base_output_path = \"/kaggle/working/\"\n            environment_name = \"kaggle\"\n        else:\n            print(\"âš ï¸ Environment: Local/Unknown\")\n            base_data_path = \"./data/\"\n            base_output_path = \"./output/\"\n            environment_name = \"local\"\n    except NameError:\n        print(\"âš ï¸ Non-interactive session. Using local paths.\")\n        base_data_path = \"./data/\"\n        base_output_path = \"./output/\"\n        environment_name = \"local\"\n    os.makedirs(base_output_path, exist_ok=True)\n    print(f\"ðŸ“‚ Data Path: {base_data_path}\")\n    print(f\"ðŸ“¦ Output Path: {base_output_path}\")\n    return base_data_path, base_output_path, environment_name\ndef load_secret(key_name: str) -> Optional[str]:\n    \"\"\"\n    Loads a secret key from the appropriate environment (Colab, Kaggle, or local env vars).\n    Args:\n        key_name (str): The name of the secret key to load (e.g., \"WANDB_API_KEY\", \"HF_TOKEN\").\n    Returns:\n        Optional[str]: The secret key value if found, otherwise None.\n    \"\"\"\n    env = ENV_NAME\n    secret_value = None\n    print(f\"Attempting to load secret '{key_name}' from '{env}' environment...\")\n    try:\n        if env == \"colab\":\n            from google.colab import userdata\n            secret_value = userdata.get(key_name)\n        elif env == \"kaggle\":\n            from kaggle_secrets import UserSecretsClient\n            user_secrets = UserSecretsClient()\n            secret_value = user_secrets.get_secret(key_name)\n        else: # Local environment\n            secret_value = os.getenv(key_name)\n        if not secret_value:\n            print(f\"âš ï¸ Secret '{key_name}' not found in the {env} environment.\")\n            return None\n        print(f\"âœ… Successfully loaded secret '{key_name}'.\")\n        return secret_value\n    except Exception as e:\n        print(f\"âŒ An error occurred while loading secret '{key_name}': {e}\")\n        return None\nINPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-30T13:29:20.474600Z","iopub.execute_input":"2025-12-30T13:29:20.474902Z","iopub.status.idle":"2025-12-30T13:29:20.484461Z","shell.execute_reply.started":"2025-12-30T13:29:20.474877Z","shell.execute_reply":"2025-12-30T13:29:20.483825Z"},"id":"sxdyquWfaqdm","outputId":"9f97d0a4-22a8-4bd4-8857-43e4074b18aa","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Environment: Google Colab\n","ðŸ“‚ Data Path: /content/\n","ðŸ“¦ Output Path: /content/\n"]}],"execution_count":2},{"cell_type":"code","source":"!uv pip install --upgrade pip\n!uv pip install -r FontDiffusion/my_requirements.txt\n# 3. Install PyTorch 1.13\n%cd {OUTPUT_PATH}\n# Force reinstall torch 1.13 to match the model's training environment\n# !uv pip uninstall torch torchvision\n# !uv pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n!uv pip install torch torchvision\n# 4. Install other dependencies\nprint(\"\\nâ¬‡ï¸ Installing Dependencies (Manually fixed)...\")\n# Install xformers compatible with Torch 1.13\n!uv pip install xformers==0.0.16 -q\n\n# Install original dependencies\n!uv pip install transformers==4.33.1 accelerate==0.23.0 diffusers==0.22.0\n!uv pip install gradio==4.8.0 pyyaml pygame opencv-python info-nce-pytorch kornia\n# -----------------------------------------------------------------\n!uv pip install lpips scikit-image pytorch-fid\n!sudo apt-get update && sudo apt-get install dos2unix\n!uv pip install gdown\n!uv pip install wandb\nprint(\"\\nâœ… Environment setup complete. You can now proceed to Block 2 (Inference).\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ET_mqyek9bwj","outputId":"e9c23164-1fa6-4a3b-b89a-1b77dff4282e","trusted":true,"execution":{"iopub.status.busy":"2025-12-30T13:29:20.485216Z","iopub.execute_input":"2025-12-30T13:29:20.485399Z","iopub.status.idle":"2025-12-30T13:30:24.070148Z","shell.execute_reply.started":"2025-12-30T13:29:20.485383Z","shell.execute_reply":"2025-12-30T13:30:24.069186Z"}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 16ms\u001b[0m\u001b[0m\n","\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 0.14ms\u001b[0m\u001b[0m\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2K  \u001b[31mÃ—\u001b[0m No solution found when resolving dependencies:\n","\u001b[31m  â•°â”€â–¶ \u001b[0mBecause you require zipp==1.0.0 and zipp==3.19.2, we can conclude that\n","\u001b[31m      \u001b[0myour requirements are unsatisfiable.\n","/content\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 90ms\u001b[0m\u001b[0m\n","\n","â¬‡ï¸ Installing Dependencies (Manually fixed)...\n","  \u001b[31mÃ—\u001b[0m No solution found when resolving dependencies:\n","\u001b[31m  â•°â”€â–¶ \u001b[0mBecause torch==1.13.1 has no wheels with a matching Python ABI tag\n","\u001b[31m      \u001b[0m(e.g., `\u001b[36mcp312\u001b[39m`) and xformers==0.0.16 depends on torch==1.13.1, we can\n","\u001b[31m      \u001b[0mconclude that xformers==0.0.16 cannot be used.\n","\u001b[31m      \u001b[0mAnd because you require xformers==0.0.16, we can conclude that your\n","\u001b[31m      \u001b[0mrequirements are unsatisfiable.\n","\n","\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m You require \u001b[36mCPython 3.12\u001b[39m (`\u001b[36mcp312\u001b[39m`), but we only found wheels for\n","\u001b[31m      \u001b[0m`\u001b[36mtorch\u001b[39m` (\u001b[36mv1.13.1\u001b[39m) with the following Python ABI tags: `\u001b[36mcp37m\u001b[39m`, `\u001b[36mcp38\u001b[39m`,\n","\u001b[31m      \u001b[0m`\u001b[36mcp39\u001b[39m`, `\u001b[36mcp310\u001b[39m`, `\u001b[36mcp311\u001b[39m`\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2K\u001b[2mResolved \u001b[1m47 packages\u001b[0m \u001b[2min 28ms\u001b[0m\u001b[0m\n","\u001b[2K  \u001b[31mÃ—\u001b[0m Failed to build `tokenizers==0.13.3`\n","\u001b[31m  â”œâ”€â–¶ \u001b[0mThe build backend returned an error\n","\u001b[31m  â•°â”€â–¶ \u001b[0mCall to `setuptools.build_meta.build_wheel` failed (exit status: 1)\n","\n","\u001b[31m      \u001b[0m\u001b[31m[stdout]\u001b[39m\n","\u001b[31m      \u001b[0mrunning bdist_wheel\n","\u001b[31m      \u001b[0mrunning build\n","\u001b[31m      \u001b[0mrunning build_py\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/byte_level_bpe.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_unigram.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/char_level_bpe.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_bpe.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/base_tokenizer.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/bert_wordpiece.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer-styles.css ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n","\u001b[31m      \u001b[0mrunning build_ext\n","\u001b[31m      \u001b[0mrunning build_rust\n","\n","\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n","\u001b[31m      \u001b[0m/root/.cache/uv/builds-v0/.tmpYG7Ofi/lib/python3.12/site-packages/setuptools/dist.py:759:\n","\u001b[31m      \u001b[0mSetuptoolsDeprecationWarning: License classifiers are deprecated.\n","\u001b[31m      \u001b[0m!!\n","\n","\u001b[31m      \u001b[0m\n","\u001b[31m      \u001b[0m********************************************************************************\n","\u001b[31m      \u001b[0m        Please consider removing the following classifiers in favor of a\n","\u001b[31m      \u001b[0mSPDX license expression:\n","\n","\u001b[31m      \u001b[0m        License :: OSI Approved :: Apache Software License\n","\n","\u001b[31m      \u001b[0m        See\n","\u001b[31m      \u001b[0mhttps://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license\n","\u001b[31m      \u001b[0mfor details.\n","\u001b[31m      \u001b[0m\n","\u001b[31m      \u001b[0m********************************************************************************\n","\n","\u001b[31m      \u001b[0m!!\n","\u001b[31m      \u001b[0m  self._finalize_license_expression()\n","\u001b[31m      \u001b[0merror: can't find Rust compiler\n","\n","\u001b[31m      \u001b[0mIf you are using an outdated pip version, it is possible a prebuilt\n","\u001b[31m      \u001b[0mwheel is available for this package but pip is not able to install from\n","\u001b[31m      \u001b[0mit. Installing from the wheel would avoid the need for a Rust compiler.\n","\n","\u001b[31m      \u001b[0mTo update pip, run:\n","\n","\u001b[31m      \u001b[0m    pip install --upgrade pip\n","\n","\u001b[31m      \u001b[0mand then retry package installation.\n","\n","\u001b[31m      \u001b[0mIf you did intend to build this package from source, try installing\n","\u001b[31m      \u001b[0ma Rust compiler from your system package manager and ensure it is\n","\u001b[31m      \u001b[0mon the PATH during installation. Alternatively, rustup (available at\n","\u001b[31m      \u001b[0mhttps://rustup.rs) is the recommended way to download and update the\n","\u001b[31m      \u001b[0mRust compiler toolchain.\n","\n","\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This usually indicates a problem with the package or the build\n","\u001b[31m      \u001b[0menvironment.\n","\u001b[36m  help: \u001b[0m`\u001b[36mtokenizers\u001b[39m` (\u001b[36mv0.13.3\u001b[39m) was included because `\u001b[36mtransformers\u001b[39m` (\u001b[36mv4.33.1\u001b[39m)\n","        depends on `\u001b[36mtokenizers\u001b[39m`\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2mAudited \u001b[1m6 packages\u001b[0m \u001b[2min 95ms\u001b[0m\u001b[0m\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 95ms\u001b[0m\u001b[0m\n","Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","Hit:3 https://cli.github.com/packages stable InRelease\n","Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n","Hit:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","dos2unix is already the newest version (7.4.2-2).\n","0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 103ms\u001b[0m\u001b[0m\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 90ms\u001b[0m\u001b[0m\n","\n","âœ… Environment setup complete. You can now proceed to Block 2 (Inference).\n"]}],"execution_count":3},{"cell_type":"code","source":"import gdown\n%cd {OUTPUT_PATH}\nif not os.path.exists(\"ckpt\"):\n  url = \"https://drive.google.com/drive/folders/12hfuZ9MQvXqcteNuz7JQ2B_mUcTr-5jZ\"\n  gdown.download_folder(url, quiet=True, use_cookies=False)","metadata":{"execution":{"iopub.status.busy":"2025-12-30T13:30:24.071273Z","iopub.execute_input":"2025-12-30T13:30:24.071924Z","iopub.status.idle":"2025-12-30T13:30:34.410970Z","shell.execute_reply.started":"2025-12-30T13:30:24.071884Z","shell.execute_reply":"2025-12-30T13:30:34.410329Z"},"id":"9PsLgUs0cYmO","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a8f66673-b3e0-4160-930c-474e2056e4d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"execution_count":4},{"cell_type":"code","source":"# @title Unzipping all archived files\nimport os\nimport glob\nfrom zipfile import ZipFile\n\nzip_file_paths = glob.glob(os.path.join(INPUT_PATH, '*.zip'))\n\nif not zip_file_paths:\n    print(f'No .zip files found in {INPUT_PATH}.')\nelse:\n    for zip_file_path in zip_file_paths:\n        if os.path.exists(zip_file_path):\n            print(f'Unzipping {zip_file_path}...')\n            !unzip -o {zip_file_path} -d ./\n            print(f'Unzipping of {zip_file_path} complete.')\n        else:\n            print(f'Error: The file {zip_file_path} was not found (post-glob check).')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-30T13:30:34.412667Z","iopub.execute_input":"2025-12-30T13:30:34.413083Z","iopub.status.idle":"2025-12-30T13:30:34.420605Z","shell.execute_reply.started":"2025-12-30T13:30:34.413059Z","shell.execute_reply":"2025-12-30T13:30:34.419905Z"},"id":"ecfc18e0","outputId":"627c1125-c3ea-4b74-e96d-51f748213fe9","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":["No .zip files found in /content/.\n"]}],"execution_count":5},{"cell_type":"code","source":"# @title Checking checkpoint files (.pth)\nimport os\nimport time\n\nCHECKPOINT_DIR = os.path.join(OUTPUT_PATH, \"ckpt\")\nprint(CHECKPOINT_DIR)\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nrequired_files = [\"unet.pth\", \"content_encoder.pth\", \"style_encoder.pth\"]\nwhile True:\n    missing = [f for f in required_files if not os.path.exists(f\"{CHECKPOINT_DIR}/{f}\")]\n    if not missing:\n        print(\"\\nâœ… All weights found! You can proceed to the next step.\")\n        break\n    else:\n        print(f\"Waiting for files... Missing: {missing}\")\n        print(\"Upload them to the 'ckpt' folder now.\")\n        time.sleep(10) # Checks every 10 seconds","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-30T13:30:34.421291Z","iopub.execute_input":"2025-12-30T13:30:34.421499Z","iopub.status.idle":"2025-12-30T13:30:37.460593Z","shell.execute_reply.started":"2025-12-30T13:30:34.421482Z","shell.execute_reply":"2025-12-30T13:30:37.459805Z"},"id":"JBflCTABxlF4","outputId":"6b0a37a3-1e08-4eba-a1ce-99ba6da518a7","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/ckpt\n","\n","âœ… All weights found! You can proceed to the next step.\n"]}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport os\n\ndef convert_csv_to_chars_txt(input_csv_path: str, output_txt_path: str, column_name: str = 'word'):\n    \"\"\"\n    Reads a CSV file, extracts text from a specified column, and writes each character\n    to a new line in a plain text file.\n\n    Args:\n        input_csv_path (str): The full path to the input CSV file.\n        output_txt_path (str): The full path for the output text file.\n        column_name (str): The name of the column in the CSV file containing the text.\n    \"\"\"\n    if not os.path.exists(input_csv_path):\n        print(f\"Error: Input CSV file not found at '{input_csv_path}'. Please ensure the file is uploaded.\")\n        return\n\n    try:\n        df = pd.read_csv(input_csv_path)\n    except Exception as e:\n        print(f\"Error reading CSV file '{input_csv_path}': {e}\")\n        return\n\n    if column_name not in df.columns:\n        print(f\"Error: Column '{column_name}' not found in the CSV file '{input_csv_path}'.\")\n        return\n\n    all_characters = []\n    # Ensure the column values are treated as strings before iterating over them\n    for item in df[column_name].astype(str).dropna().tolist():\n        for char in item:\n            all_characters.append(char)\n\n    # Ensure output directory exists\n    os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)\n\n    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(all_characters))\n    print(f\"Successfully converted '{input_csv_path}' to '{output_txt_path}', with one character per line.\")\n\n# --- Example Usage (demonstration with a dummy file) ---\n# As the original file 'Ds_300_ChuNom_TuTao.csv' was not found in the previous execution,\n# let's create a dummy file to demonstrate the function's usage.\nprint(\"\\n--- Demonstrating function with a dummy CSV file ---\")\ndummy_csv_path = os.path.join(OUTPUT_PATH, \"dummy_data.csv\")\ndummy_output_txt_path = os.path.join(OUTPUT_PATH, \"dummy_chars.txt\")\n\n# Create a dummy CSV file\ndummy_data = {'word': ['hello', 'world', 'python']}\npd.DataFrame(dummy_data).to_csv(dummy_csv_path, index=False)\nprint(f\"Created a dummy CSV file at: {dummy_csv_path}\")\n\nconvert_csv_to_chars_txt(dummy_csv_path, dummy_output_txt_path)\n\n# --- How to use with your actual file ---\n# Uncomment the lines below and replace 'your_actual_file.csv' and 'your_output.txt'\n# with the correct paths for your use case.\n#\n# original_csv_file = os.path.join(INPUT_PATH, \"Ds_300_ChuNom_TuTao.csv\") # Or the full path to your CSV\n# original_output_txt = os.path.join(OUTPUT_PATH, \"nom_tu_tao.txt\") # Or your desired output path\n# convert_csv_to_chars_txt(original_csv_file, original_output_txt)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-30T13:30:37.462442Z","iopub.execute_input":"2025-12-30T13:30:37.462655Z","iopub.status.idle":"2025-12-30T13:30:37.808690Z","shell.execute_reply.started":"2025-12-30T13:30:37.462637Z","shell.execute_reply":"2025-12-30T13:30:37.807867Z"},"id":"Mx5uS5WQaqdn","outputId":"8937c2f6-2b2c-438f-d9b9-778a1410718d","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Demonstrating function with a dummy CSV file ---\n","Created a dummy CSV file at: /content/dummy_data.csv\n","Successfully converted '/content/dummy_data.csv' to '/content/dummy_chars.txt', with one character per line.\n"]}],"execution_count":7},{"cell_type":"code","source":"!ls -lart {OUTPUT_PATH}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T13:30:37.809525Z","iopub.execute_input":"2025-12-30T13:30:37.809771Z","iopub.status.idle":"2025-12-30T13:30:37.931890Z","shell.execute_reply.started":"2025-12-30T13:30:37.809751Z","shell.execute_reply":"2025-12-30T13:30:37.931045Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"Sxz63qgifNlV","outputId":"0709d787-f993-465e-91a4-a9d408161b06"},"outputs":[{"output_type":"stream","name":"stdout","text":["total 44\n","drwxr-xr-x  4 root root 4096 Dec  9 14:41 .config\n","drwxr-xr-x  1 root root 4096 Dec  9 14:42 sample_data\n","drwxr-xr-x  1 root root 4096 Dec 30 15:18 ..\n","drwxr-xr-x  2 root root 4096 Dec 30 15:24 ckpt\n","drwxr-xr-x  9 root root 4096 Dec 30 15:42 my_dataset\n","drwxr-xr-x  3 root root 4096 Dec 30 15:48 outputs\n","drwxr-xr-x  3 root root 4096 Dec 30 15:48 wandb\n","drwxr-xr-x  1 root root 4096 Dec 30 15:52 .\n","drwxr-xr-x 12 root root 4096 Dec 30 15:52 FontDiffusion\n","-rw-r--r--  1 root root   24 Dec 30 15:52 dummy_data.csv\n","-rw-r--r--  1 root root   31 Dec 30 15:52 dummy_chars.txt\n"]}],"execution_count":8},{"cell_type":"code","source":"%cd {OUTPUT_PATH}\nHF_TOKEN = load_secret(\"HF_TOKEN\")\nHF_USERNAME = \"dzungpham\"\n\n# ==========================================\n# EXPORT / DOWNLOAD DATASET COMMANDS\n# ==========================================\n\n# Train Split\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/train\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"train\" \\\n  --token HF_TOKEN\n\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/train_original\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"train_original\" \\\n  --token HF_TOKEN\n\n# Validation: Unseen Both\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/val_unseen_both\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"val_unseen_both\" \\\n  --token HF_TOKEN\n\n# Validation: Seen Style, Unseen Char\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/val_seen_style_unseen_char\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"val_seen_style_unseen_char\" \\\n  --token HF_TOKEN\n\n# Validation: Unseen Style, Seen Char\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/val_unseen_style_seen_char\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"val_unseen_style_seen_char\" \\\n  --token HF_TOKEN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T13:30:37.932960Z","iopub.execute_input":"2025-12-30T13:30:37.933299Z","iopub.status.idle":"2025-12-30T13:31:32.073307Z","shell.execute_reply.started":"2025-12-30T13:30:37.933257Z","shell.execute_reply":"2025-12-30T13:31:32.072418Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"MvEJIiH5fNlV","outputId":"76697de4-43e0-401a-b6f4-7537a1c6e103"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Attempting to load secret 'HF_TOKEN' from 'colab' environment...\n","âœ… Successfully loaded secret 'HF_TOKEN'.\n","\n","============================================================\n","FONTDIFFUSION DATASET EXPORTER\n","============================================================\n","âœ“ Created output directories:\n","  Root: my_dataset/train\n","  Content: my_dataset/train/ContentImage\n","  Target: my_dataset/train/TargetImage\n","\n","============================================================\n","LOADING DATASET\n","============================================================\n","\n","Loading from Hub: dzungpham/font-diffusion-generated-data\n","README.md: 1.30kB [00:00, 5.75MB/s]\n","data/train-00000-of-00001.parquet: 100% 25.7M/25.7M [00:04<00:00, 5.95MB/s]\n","data/val_unseen_both-00000-of-00001.parq(â€¦): 100% 307k/307k [00:01<00:00, 195kB/s]\n","data/val_seen_style_unseen_char-00000-of(â€¦): 100% 940k/940k [00:02<00:00, 448kB/s]\n","data/val_unseen_style_seen_char-00000-of(â€¦): 100% 6.88M/6.88M [00:02<00:00, 2.67MB/s]\n","data/train_original-00000-of-00001.parqu(â€¦): 100% 68.6M/68.6M [00:05<00:00, 11.5MB/s]\n","data/test-00000-of-00001.parquet: 100% 85.9k/85.9k [00:01<00:00, 61.6kB/s]\n","Generating train split: 100% 1650/1650 [00:00<00:00, 20342.68 examples/s]\n","Generating val split: 100% 24/24 [00:00<00:00, 6862.31 examples/s]\n","Generating val_unseen_both split: 100% 27/27 [00:00<00:00, 9038.73 examples/s]\n","Generating val_seen_style_unseen_char split: 100% 99/99 [00:00<00:00, 23381.73 examples/s]\n","Generating val_unseen_style_seen_char split: 100% 450/450 [00:00<00:00, 29130.71 examples/s]\n","Generating train_original split: 100% 4470/4470 [00:00<00:00, 23486.99 examples/s]\n","Generating test split: 100% 5/5 [00:00<00:00, 1214.90 examples/s]\n","âœ“ Loaded dataset with 1650 samples\n","  Columns: ['character', 'char_index', 'style', 'style_index', 'content_image', 'target_image', 'font']\n","\n","============================================================\n","EXPORTING DATASET TO DISK\n","============================================================\n","\n","Found:\n","  11 unique styles\n","  1 unique characters\n","  150 character indices\n","\n","âœ“ Created 11 style directories\n","\n","------------------------------------------------------------\n","Exporting content images...\n","------------------------------------------------------------\n","ðŸ–¼ï¸  Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1650/1650 [00:01<00:00, 1019.32it/s]\n","âœ“ Exported 150 content images\n","\n","------------------------------------------------------------\n","Exporting target images...\n","------------------------------------------------------------\n","ðŸŽ¨ Styles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1650/1650 [00:04<00:00, 411.39it/s]\n","âœ“ Exported 1650 target images\n","\n","------------------------------------------------------------\n","Saving metadata...\n","------------------------------------------------------------\n","âœ“ Saved metadata to my_dataset/train/results.json\n","  Total generations: 1650\n","  Styles: 11\n","  Characters: 1\n","\n","============================================================\n","EXPORT SUMMARY\n","============================================================\n","\n","âœ“ Export Statistics:\n","  Root: my_dataset/train\n","  Total samples: 1650\n","  Styles: 11\n","  Characters: 1\n","  Target images: 1650\n","\n","âœ“ Directory Structure:\n","  my_dataset/train/\n","  â”œâ”€â”€ ContentImage/ (11 images)\n","  â”œâ”€â”€ TargetImage/\n","  â”‚   â”œâ”€â”€ style10/ (150 images)\n","  â”‚   â”œâ”€â”€ style11/ (150 images)\n","  â”‚   â”œâ”€â”€ style12/ (150 images)\n","  â”‚   â”œâ”€â”€ ... (8 more styles)\n","  â””â”€â”€ results.json\n","\n","âœ“ Summary saved to my_dataset/train/export_summary.json\n","\n","âœ“ Export completed successfully!\n","\n","============================================================\n","FONTDIFFUSION DATASET EXPORTER\n","============================================================\n","âœ“ Created output directories:\n","  Root: my_dataset/train_original\n","  Content: my_dataset/train_original/ContentImage\n","  Target: my_dataset/train_original/TargetImage\n","\n","============================================================\n","LOADING DATASET\n","============================================================\n","\n","Loading from Hub: dzungpham/font-diffusion-generated-data\n","âœ“ Loaded dataset with 4470 samples\n","  Columns: ['character', 'char_index', 'style', 'style_index', 'content_image', 'target_image', 'font']\n","\n","============================================================\n","EXPORTING DATASET TO DISK\n","============================================================\n","\n","Found:\n","  15 unique styles\n","  1 unique characters\n","  298 character indices\n","\n","âœ“ Created 15 style directories\n","\n","------------------------------------------------------------\n","Exporting content images...\n","------------------------------------------------------------\n","ðŸ–¼ï¸  Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4470/4470 [00:03<00:00, 1172.55it/s]\n","âœ“ Exported 298 content images\n","\n","------------------------------------------------------------\n","Exporting target images...\n","------------------------------------------------------------\n","ðŸŽ¨ Styles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4470/4470 [00:11<00:00, 390.17it/s]\n","âœ“ Exported 4470 target images\n","\n","------------------------------------------------------------\n","Saving metadata...\n","------------------------------------------------------------\n","âœ“ Saved metadata to my_dataset/train_original/results.json\n","  Total generations: 4470\n","  Styles: 15\n","  Characters: 1\n","\n","============================================================\n","EXPORT SUMMARY\n","============================================================\n","\n","âœ“ Export Statistics:\n","  Root: my_dataset/train_original\n","  Total samples: 4470\n","  Styles: 15\n","  Characters: 1\n","  Target images: 4470\n","\n","âœ“ Directory Structure:\n","  my_dataset/train_original/\n","  â”œâ”€â”€ ContentImage/ (15 images)\n","  â”œâ”€â”€ TargetImage/\n","  â”‚   â”œâ”€â”€ style0/ (298 images)\n","  â”‚   â”œâ”€â”€ style1/ (298 images)\n","  â”‚   â”œâ”€â”€ style10/ (298 images)\n","  â”‚   â”œâ”€â”€ ... (12 more styles)\n","  â””â”€â”€ results.json\n","\n","âœ“ Summary saved to my_dataset/train_original/export_summary.json\n","\n","âœ“ Export completed successfully!\n","\n","============================================================\n","FONTDIFFUSION DATASET EXPORTER\n","============================================================\n","âœ“ Created output directories:\n","  Root: my_dataset/val_unseen_both\n","  Content: my_dataset/val_unseen_both/ContentImage\n","  Target: my_dataset/val_unseen_both/TargetImage\n","\n","============================================================\n","LOADING DATASET\n","============================================================\n","\n","Loading from Hub: dzungpham/font-diffusion-generated-data\n","âœ“ Loaded dataset with 27 samples\n","  Columns: ['character', 'char_index', 'style', 'style_index', 'content_image', 'target_image', 'font']\n","\n","============================================================\n","EXPORTING DATASET TO DISK\n","============================================================\n","\n","Found:\n","  3 unique styles\n","  1 unique characters\n","  9 character indices\n","\n","âœ“ Created 3 style directories\n","\n","------------------------------------------------------------\n","Exporting content images...\n","------------------------------------------------------------\n","ðŸ–¼ï¸  Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:00<00:00, 646.57it/s]\n","âœ“ Exported 9 content images\n","\n","------------------------------------------------------------\n","Exporting target images...\n","------------------------------------------------------------\n","ðŸŽ¨ Styles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:00<00:00, 455.42it/s]\n","âœ“ Exported 27 target images\n","\n","------------------------------------------------------------\n","Saving metadata...\n","------------------------------------------------------------\n","âœ“ Saved metadata to my_dataset/val_unseen_both/results.json\n","  Total generations: 27\n","  Styles: 3\n","  Characters: 1\n","\n","============================================================\n","EXPORT SUMMARY\n","============================================================\n","\n","âœ“ Export Statistics:\n","  Root: my_dataset/val_unseen_both\n","  Total samples: 27\n","  Styles: 3\n","  Characters: 1\n","  Target images: 27\n","\n","âœ“ Directory Structure:\n","  my_dataset/val_unseen_both/\n","  â”œâ”€â”€ ContentImage/ (0 images)\n","  â”œâ”€â”€ TargetImage/\n","  â”‚   â”œâ”€â”€ style0/ (9 images)\n","  â”‚   â”œâ”€â”€ style1/ (9 images)\n","  â”‚   â”œâ”€â”€ style6/ (9 images)\n","  â””â”€â”€ results.json\n","\n","âœ“ Summary saved to my_dataset/val_unseen_both/export_summary.json\n","\n","âœ“ Export completed successfully!\n","\n","============================================================\n","FONTDIFFUSION DATASET EXPORTER\n","============================================================\n","âœ“ Created output directories:\n","  Root: my_dataset/val_seen_style_unseen_char\n","  Content: my_dataset/val_seen_style_unseen_char/ContentImage\n","  Target: my_dataset/val_seen_style_unseen_char/TargetImage\n","\n","============================================================\n","LOADING DATASET\n","============================================================\n","\n","Loading from Hub: dzungpham/font-diffusion-generated-data\n","âœ“ Loaded dataset with 99 samples\n","  Columns: ['character', 'char_index', 'style', 'style_index', 'content_image', 'target_image', 'font']\n","\n","============================================================\n","EXPORTING DATASET TO DISK\n","============================================================\n","\n","Found:\n","  11 unique styles\n","  1 unique characters\n","  9 character indices\n","\n","âœ“ Created 11 style directories\n","\n","------------------------------------------------------------\n","Exporting content images...\n","------------------------------------------------------------\n","ðŸ–¼ï¸  Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:00<00:00, 1073.56it/s]\n","âœ“ Exported 9 content images\n","\n","------------------------------------------------------------\n","Exporting target images...\n","------------------------------------------------------------\n","ðŸŽ¨ Styles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:00<00:00, 408.46it/s]\n","âœ“ Exported 99 target images\n","\n","------------------------------------------------------------\n","Saving metadata...\n","------------------------------------------------------------\n","âœ“ Saved metadata to my_dataset/val_seen_style_unseen_char/results.json\n","  Total generations: 99\n","  Styles: 11\n","  Characters: 1\n","\n","============================================================\n","EXPORT SUMMARY\n","============================================================\n","\n","âœ“ Export Statistics:\n","  Root: my_dataset/val_seen_style_unseen_char\n","  Total samples: 99\n","  Styles: 11\n","  Characters: 1\n","  Target images: 99\n","\n","âœ“ Directory Structure:\n","  my_dataset/val_seen_style_unseen_char/\n","  â”œâ”€â”€ ContentImage/ (0 images)\n","  â”œâ”€â”€ TargetImage/\n","  â”‚   â”œâ”€â”€ style10/ (9 images)\n","  â”‚   â”œâ”€â”€ style11/ (9 images)\n","  â”‚   â”œâ”€â”€ style12/ (9 images)\n","  â”‚   â”œâ”€â”€ ... (8 more styles)\n","  â””â”€â”€ results.json\n","\n","âœ“ Summary saved to my_dataset/val_seen_style_unseen_char/export_summary.json\n","\n","âœ“ Export completed successfully!\n","\n","============================================================\n","FONTDIFFUSION DATASET EXPORTER\n","============================================================\n","âœ“ Created output directories:\n","  Root: my_dataset/val_unseen_style_seen_char\n","  Content: my_dataset/val_unseen_style_seen_char/ContentImage\n","  Target: my_dataset/val_unseen_style_seen_char/TargetImage\n","\n","============================================================\n","LOADING DATASET\n","============================================================\n","\n","Loading from Hub: dzungpham/font-diffusion-generated-data\n","âœ“ Loaded dataset with 450 samples\n","  Columns: ['character', 'char_index', 'style', 'style_index', 'content_image', 'target_image', 'font']\n","\n","============================================================\n","EXPORTING DATASET TO DISK\n","============================================================\n","\n","Found:\n","  3 unique styles\n","  1 unique characters\n","  150 character indices\n","\n","âœ“ Created 3 style directories\n","\n","------------------------------------------------------------\n","Exporting content images...\n","------------------------------------------------------------\n","ðŸ–¼ï¸  Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 450/450 [00:00<00:00, 794.65it/s]\n","âœ“ Exported 150 content images\n","\n","------------------------------------------------------------\n","Exporting target images...\n","------------------------------------------------------------\n","ðŸŽ¨ Styles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 450/450 [00:01<00:00, 448.36it/s]\n","âœ“ Exported 450 target images\n","\n","------------------------------------------------------------\n","Saving metadata...\n","------------------------------------------------------------\n","âœ“ Saved metadata to my_dataset/val_unseen_style_seen_char/results.json\n","  Total generations: 450\n","  Styles: 3\n","  Characters: 1\n","\n","============================================================\n","EXPORT SUMMARY\n","============================================================\n","\n","âœ“ Export Statistics:\n","  Root: my_dataset/val_unseen_style_seen_char\n","  Total samples: 450\n","  Styles: 3\n","  Characters: 1\n","  Target images: 450\n","\n","âœ“ Directory Structure:\n","  my_dataset/val_unseen_style_seen_char/\n","  â”œâ”€â”€ ContentImage/ (3 images)\n","  â”œâ”€â”€ TargetImage/\n","  â”‚   â”œâ”€â”€ style0/ (150 images)\n","  â”‚   â”œâ”€â”€ style1/ (150 images)\n","  â”‚   â”œâ”€â”€ style6/ (150 images)\n","  â””â”€â”€ results.json\n","\n","âœ“ Summary saved to my_dataset/val_unseen_style_seen_char/export_summary.json\n","\n","âœ“ Export completed successfully!\n"]}],"execution_count":9},{"cell_type":"code","source":"# !uv pip install --upgrade diffusers \"huggingface-hub>=0.15.1,<1.0\"\n# !uv pip install --upgrade accelerate peft\n\n# already change sample_batch file to save all data in train_original\n%cd {OUTPUT_PATH}\n!python FontDiffusion/sample_batch.py \\\n    --characters \"FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\" \\\n    --style_images \"FontDiffusion/styles_images\" \\\n    --ckpt_dir \"ckpt/\" \\\n    --ttf_path \"FontDiffusion/fonts\" \\\n    --output_dir \"my_dataset/train_original\" \\\n    --resume_from \"my_dataset/train_original/results.json\" \\\n    --num_inference_steps 20 \\\n    --guidance_scale 7.5 \\\n    --start_line 100 \\\n    --end_line 500 \\\n    --batch_size 24 \\\n    --save_interval 5 \\\n    --channels_last \\\n    --seed 42 \\\n    --compile \\\n    --enable_xformers","metadata":{"execution":{"iopub.status.busy":"2025-12-30T13:31:32.074442Z","iopub.execute_input":"2025-12-30T13:31:32.074718Z","iopub.status.idle":"2025-12-30T13:32:20.554434Z","shell.execute_reply.started":"2025-12-30T13:31:32.074679Z","shell.execute_reply":"2025-12-30T13:32:20.553491Z"},"id":"gma02BZvhx8I","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cfe090e4-086d-4eb6-e8e6-befe5601e7f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","/usr/local/lib/python3.12/dist-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n","  from pkg_resources import resource_stream, resource_exists\n","pygame 2.6.1 (SDL 2.28.4, Python 3.12.12)\n","Hello from the pygame community. https://www.pygame.org/contribute.html\n","\n","============================================================\n","FONTDIFFUSER STANDARD FORMAT GENERATION (Index-based)\n","============================================================\n","Loading characters from lines 100 to 500 (total: 10174 lines)\n","Successfully loaded 401 single characters.\n","\n","Initializing font manager...\n","\n","============================================================\n","Loading 15 fonts from directory...\n","============================================================\n","error: XDG_RUNTIME_DIR not set in the environment.\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HAN NOM A\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HAN NOM B\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: Han-Nom Kai 1.00\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: Han-Nom-Khai-Regular-300623\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: Han-nom Minh 1.42\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HanaMinA\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HanaMinA\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HanaMinB\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HanaMinB\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HanaMinC\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: NomNaTong-Regular\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: NomNaTong-Regular\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: NomNaTong-Regular2\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: NomNaTongLight\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: NomNaTongLight2\n","============================================================\n","Successfully loaded 12 fonts\n","\n","\n","ðŸ“Š Configuration:\n","  Number of Characters: 401 (lines 100-500)\n","  Number of Styles: 15\n","  Output Directory: my_dataset/train_original\n","âœ“ Loaded checkpoint from my_dataset/train_original/results.json\n","\n","Loading FontDiffuser pipeline...\n","Loading FontDiffuser pipeline...\n","Load the down block  DownBlock2D\n","Load the down block  MCADownBlock2D\n","The style_attention cross attention dim in Down Block 1 layer is 1024\n","The style_attention cross attention dim in Down Block 2 layer is 1024\n","Load the down block  MCADownBlock2D\n","The style_attention cross attention dim in Down Block 1 layer is 1024\n","The style_attention cross attention dim in Down Block 2 layer is 1024\n","Load the down block  DownBlock2D\n","Load the up block  UpBlock2D\n","Load the up block  StyleRSIUpBlock2D\n","Load the up block  StyleRSIUpBlock2D\n","Load the up block  UpBlock2D\n","Param count for Ds initialized parameters: 20591296\n","Get CG-GAN Style Encoder!\n","Param count for Ds initialized parameters: 1187008\n","Get CG-GAN Content Encoder!\n","âœ“ Loaded model state_dict successfully\n","Converting to channels-last memory format...\n","2025-12-30 17:07:16.984677: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1767114437.024804   31444 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1767114437.038871   31444 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1767114437.074257   31444 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1767114437.074290   31444 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1767114437.074299   31444 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1767114437.074305   31444 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-12-30 17:07:17.083674: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","âœ“ Model moved to device\n","âœ“ Loaded training DDPM scheduler successfully\n","âœ“ Loaded DPM-Solver pipeline successfully\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n","âœ“ Loaded results.json:\n","  Characters: 1\n","  Styles: 15\n","  Existing pairs: 4470\n","ðŸ“¥ Resuming from checkpoint: 4470 style-character pairs already processed\n","\n","============================================================\n","BATCH GENERATION (Index-based Tracking)\n","============================================================\n","Fonts: 12\n","Styles: 15\n","Number of Characters: 401\n","Batch size: 24\n","Inference steps: 20\n","Guidance scale: 7.5\n","Save interval: 5\n","Output directory: my_dataset/train_original\n","============================================================\n","ðŸŽ¨ Generating styles:   0%|                               | 0/15 [00:00<?, ?it/s, Processing style0]/content/FontDiffusion/src/model.py:88: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.style_encoder'.\n","  style_img_feature, _, style_residual_features = self.style_encoder(style_images)\n","/content/FontDiffusion/src/model.py:94: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n","  content_img_feture, content_residual_features = self.content_encoder(content_images)\n","/content/FontDiffusion/src/model.py:97: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n","  style_content_feature, style_content_res_features = self.content_encoder(style_images)\n","/content/FontDiffusion/src/model.py:102: FutureWarning: Accessing config attribute `unet` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'unet' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.unet'.\n","  out = self.unet(\n","  âœ“ style0 (HAN NOM B): 81 images in 127.19s (1.570s/img)\n","  âœ“ style0 (HAN NOM A): 14 images in 23.21s (1.658s/img)\n","  âœ“ style0 (Han-Nom Kai 1.00): 9 images in 15.17s (1.686s/img)\n","  âœ“ style1 (HAN NOM B): 81 images in 136.69s (1.688s/img)\n","  âœ“ style1 (HAN NOM A): 14 images in 24.28s (1.734s/img)\n","  âœ“ style1 (Han-Nom Kai 1.00): 9 images in 15.71s (1.746s/img)\n","  âœ“ style2 (HAN NOM B): 81 images in 140.15s (1.730s/img)\n","  âœ“ style2 (HAN NOM A): 14 images in 24.31s (1.737s/img)\n","  âœ“ style2 (Han-Nom Kai 1.00): 9 images in 15.89s (1.766s/img)\n","  âœ“ style3 (HAN NOM B): 81 images in 141.35s (1.745s/img)\n","  âœ“ style3 (HAN NOM A): 14 images in 24.56s (1.755s/img)\n","  âœ“ style3 (Han-Nom Kai 1.00): 9 images in 15.94s (1.771s/img)\n","  âœ“ style4 (HAN NOM B): 81 images in 141.03s (1.741s/img)\n","  âœ“ style4 (HAN NOM A): 14 images in 24.41s (1.743s/img)\n","  âœ“ style4 (Han-Nom Kai 1.00): 9 images in 15.90s (1.766s/img)\n","ðŸŽ¨ Generating styles:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 4/15 [15:21<33:55, 185.00s/it, Processing style4]  ðŸ’¾ Checkpoint saved: my_dataset/train_original/results_checkpoint.json\n","ðŸŽ¨ Generating styles:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 5/15 [15:21<31:07, 186.71s/it, Processing style5]"]}],"execution_count":null},{"cell_type":"code","source":"!python FontDiffusion/create_validation_split.py \\\n  --data_root my_dataset \\\n  --val_ratio 0.2 \\\n  --test_ratio 0.1 \\\n  --seed 42","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"XoppW2x5fNlW","outputId":"4d70a7a7-73ce-4500-b8c2-575fca704886"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","FONTDIFFUSION VALIDATION SPLIT CREATOR\n","============================================================\n","âœ“ Using source directory: my_dataset/train_original\n","\n","============================================================\n","CREATING DATA SPLITS\n","============================================================\n","\n","============================================================\n","ANALYZING TRAINING DATA\n","============================================================\n","\n","Found:\n","  Styles: 15 - ['style0', 'style1', 'style10', 'style11', 'style12', 'style13', 'style14', 'style2', 'style3', 'style4', 'style5', 'style6', 'style7', 'style8', 'style9']\n","  Characters: 701 - ['char0', 'char1', 'char10', 'char100', 'char101', 'char102', 'char103', 'char104', 'char105', 'char106']...\n","\n","============================================================\n","CREATING VALIDATION SCENARIOS\n","============================================================\n","\n","ðŸ“Š Split Statistics:\n","  Styles: 11 train + 3 val + 1 test\n","  Chars:  491 train + 140 val + 70 test\n","\n","ðŸ“‹ Validation Scenarios:\n","\n","  train:\n","    Description: Seen styles + Seen characters (training data)\n","    Styles: ['style8', 'style4', 'style13', 'style10', 'style9', 'style7', 'style14', 'style3', 'style2', 'style11', 'style12']\n","    Chars: ['char505', 'char324', 'char318', 'char61', 'char322']...\n","\n","  val_seen_style_unseen_char:\n","    Description: Seen styles + Unseen characters\n","    Styles: ['style8', 'style4', 'style13', 'style10', 'style9', 'style7', 'style14', 'style3', 'style2', 'style11', 'style12']\n","    Chars: ['char634', 'char159', 'char649', 'char340', 'char32']...\n","\n","  val_unseen_style_seen_char:\n","    Description: Unseen styles + Seen characters\n","    Styles: ['style6', 'style1', 'style0']\n","    Chars: ['char505', 'char324', 'char318', 'char61', 'char322']...\n","\n","  val_unseen_both:\n","    Description: Unseen styles + Unseen characters\n","    Styles: ['style6', 'style1', 'style0']\n","    Chars: ['char634', 'char159', 'char649', 'char340', 'char32']...\n","\n","  test:\n","    Description: Test set (hold-out)\n","    Styles: ['style5']\n","    Chars: ['char241', 'char256', 'char275', 'char164', 'char415']...\n","\n","ðŸ”§ Creating directory structure...\n","\n","ðŸ“ Train split:\n","  âœ“ Copied 2511 target images\n","  âœ“ Copied 202 content images\n","\n","ðŸ“ val_seen_style_unseen_char:\n","  âœ“ Copied 790 target images\n","  âœ“ Copied 65 content images\n","\n","ðŸ“ val_unseen_style_seen_char:\n","  âœ“ Copied 1184 target images\n","  âœ“ Copied 202 content images\n","\n","ðŸ“ val_unseen_both:\n","  âœ“ Copied 345 target images\n","  âœ“ Copied 65 content images\n","\n","ðŸ“ test:\n","  âœ“ Copied 32 target images\n","  âœ“ Copied 32 content images\n","\n","âœ“ Saved scenario metadata to my_dataset/validation_scenarios.json\n","\n","============================================================\n","âœ“ VALIDATION SPLIT CREATION COMPLETE\n","============================================================\n","\n","Created directories:\n","  ðŸ“ train/ - Original training data\n","  ðŸ“ val_seen_style_unseen_char/ - Test new characters\n","  ðŸ“ val_unseen_style_seen_char/ - Test new styles\n","  ðŸ“ val_unseen_both/ - Test full generalization\n","  ðŸ“ test/ - Hold-out test set\n"]}],"execution_count":15},{"cell_type":"code","source":"# --- RAW DATA (Before Splitting) ---\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/train_original\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"train_original\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# --- ORGANIZED SPLITS (After Splitting) ---\n\n# Train Split\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/train\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"train\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# Test Split\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/test\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"test\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# Validation: Unseen Both\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/val_unseen_both\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"val_unseen_both\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# Validation: Seen Style, Unseen Char\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/val_seen_style_unseen_char\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"val_seen_style_unseen_char\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# Validation: Unseen Style, Seen Char\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/val_unseen_style_seen_char\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"val_unseen_style_seen_char\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"v-a7paEbfNlW","outputId":"bb07b2e6-6adc-4e41-ce3c-38090877c726"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","FONTDIFFUSION DATASET CREATOR\n","============================================================\n","âœ“ Validated directory structure\n","  Content images: my_dataset/train_original/ContentImage\n","  Target images: my_dataset/train_original/TargetImage\n","\n","============================================================\n","BUILDING DATASET\n","============================================================\n","Traceback (most recent call last):\n","  File \"/content/FontDiffusion/create_hf_dataset.py\", line 317, in <module>\n","    dataset = create_and_push_dataset(\n","              ^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/FontDiffusion/create_hf_dataset.py\", line 279, in create_and_push_dataset\n","    dataset = builder.build_dataset()\n","              ^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/FontDiffusion/create_hf_dataset.py\", line 77, in build_dataset\n","    metadata = self._load_results_metadata()\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/FontDiffusion/create_hf_dataset.py\", line 56, in _load_results_metadata\n","    return json.load(f)\n","           ^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/json/__init__.py\", line 293, in load\n","    return loads(fp.read(),\n","           ^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/json/__init__.py\", line 346, in loads\n","    return _default_decoder.decode(s)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/json/decoder.py\", line 338, in decode\n","    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/json/decoder.py\", line 354, in raw_decode\n","    obj, end = self.scan_once(s, idx)\n","               ^^^^^^^^^^^^^^^^^^^^^^\n","json.decoder.JSONDecodeError: Unterminated string starting at: line 30324 column 7 (char 1048572)\n","\n","============================================================\n","FONTDIFFUSION DATASET CREATOR\n","============================================================\n","âœ“ Validated directory structure\n","  Content images: my_dataset/train/ContentImage\n","  Target images: my_dataset/train/TargetImage\n","\n","============================================================\n","BUILDING DATASET\n","============================================================\n","\n","Found 11 style directories\n","\n","Processing style-character pairs...\n","ðŸŽ¨ Styles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:02<00:00,  3.86it/s]\n","\n","âœ“ Loaded 2706 image pairs\n","\n","âœ“ Dataset created with 2706 samples\n","  Columns: ['character', 'char_index', 'style', 'style_index', 'content_image', 'target_image', 'font']\n","  Features: {'character': Value('string'), 'char_index': Value('int64'), 'style': Value('string'), 'style_index': Value('int64'), 'content_image': Image(mode=None, decode=True), 'target_image': Image(mode=None, decode=True), 'font': Value('string')}\n","\n","============================================================\n","PUSHING TO HUB\n","============================================================\n","\n","Repository: dzungpham/font-diffusion-generated-data\n","Split: train\n","Private: True\n","Uploading the dataset shards:   0% 0/1 [00:00<?, ? shards/s]\n","Map:   0% 0/2706 [00:00<?, ? examples/s]\u001b[A\n","Map: 100% 2706/2706 [00:00<00:00, 20523.51 examples/s]\n","\n","Creating parquet from Arrow format: 100% 28/28 [00:00<00:00, 593.60ba/s]\n","Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n","New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n","\n","                              :  63% 25.2M/40.0M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (0 / 1)      :  63% 25.2M/40.0M [00:01<00:00, 19.0MB/s,   ???B/s  ]\n","\n","Processing Files (0 / 1)      : 100% 40.0M/40.0M [00:01<00:00, 28.8MB/s, 74.1MB/s  ]\n","\n","                              : 100% 40.0M/40.0M [00:00<00:00, 37.1MB/s]\u001b[A\u001b[A\n","\n","                              : 100% 40.0M/40.0M [00:00<00:00, 24.8MB/s]\u001b[A\u001b[A\n","\n","                              : 100% 40.0M/40.0M [00:00<00:00, 18.6MB/s]\u001b[A\u001b[A\n","\n","                              : 100% 40.0M/40.0M [00:00<00:00, 14.9MB/s]\u001b[A\u001b[A\n","\n","                              : 100% 40.0M/40.0M [00:01<00:00, 12.4MB/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 40.0M/40.0M [00:02<00:00, 11.4MB/s, 10.6MB/s  ]\n","New Data Upload               : 100% 27.8k/27.8k [00:02<00:00, 10.2kB/s, 19.9kB/s  ]\u001b[A\n","\n","                              : 100% 40.0M/40.0M [00:01<00:00, 9.30MB/s]\u001b[A\u001b[A\n","\n","                              : 100% 40.0M/40.0M [00:01<00:00, 9.00MB/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 40.0M/40.0M [00:03<00:00, 12.8MB/s, 8.27MB/s  ]\n","New Data Upload               : 100% 27.8k/27.8k [00:03<00:00, 8.91kB/s, 15.5kB/s  ]\n","                              : 100% 40.0M/40.0M [00:01<00:00, 8.26MB/s]\n","Uploading the dataset shards: 100% 1/1 [00:04<00:00,  4.53s/ shards]\n","README.md: 1.30kB [00:00, 2.96MB/s]\n","\n","âœ“ Successfully pushed to Hub!\n","  Dataset URL: https://huggingface.co/datasets/dzungpham/font-diffusion-generated-data\n","\n","âœ“ Done!\n","\n","============================================================\n","FONTDIFFUSION DATASET CREATOR\n","============================================================\n","âœ“ Validated directory structure\n","  Content images: my_dataset/test/ContentImage\n","  Target images: my_dataset/test/TargetImage\n","\n","============================================================\n","BUILDING DATASET\n","============================================================\n","\n","Found 1 style directories\n","\n","Processing style-character pairs...\n","ðŸŽ¨ Styles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 47.92it/s]\n","\n","âœ“ Loaded 10 image pairs\n","\n","âœ“ Dataset created with 10 samples\n","  Columns: ['character', 'char_index', 'style', 'style_index', 'content_image', 'target_image', 'font']\n","  Features: {'character': Value('string'), 'char_index': Value('int64'), 'style': Value('string'), 'style_index': Value('int64'), 'content_image': Image(mode=None, decode=True), 'target_image': Image(mode=None, decode=True), 'font': Value('string')}\n","\n","============================================================\n","PUSHING TO HUB\n","============================================================\n","\n","Repository: dzungpham/font-diffusion-generated-data\n","Split: test\n","Private: True\n","Uploading the dataset shards:   0% 0/1 [00:00<?, ? shards/s]\n","Map: 100% 10/10 [00:00<00:00, 1581.20 examples/s]\n","\n","Creating parquet from Arrow format: 100% 1/1 [00:00<00:00, 904.33ba/s]\n","Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n","New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n","\n","                              : 100% 158k/158k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 158k/158k [00:00<00:00, 747kB/s,   ???B/s  ]\n","\n","                              : 100% 158k/158k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 158k/158k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 158k/158k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 158k/158k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 158k/158k [00:01<00:00, 156kB/s,  0.00B/s  ]\n","New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n","                              : 100% 158k/158k [00:00<?, ?B/s]\n","Uploading the dataset shards: 100% 1/1 [00:03<00:00,  3.62s/ shards]\n","README.md: 1.30kB [00:00, 5.98MB/s]\n","\n","âœ“ Successfully pushed to Hub!\n","  Dataset URL: https://huggingface.co/datasets/dzungpham/font-diffusion-generated-data\n","\n","âœ“ Done!\n","\n","============================================================\n","FONTDIFFUSION DATASET CREATOR\n","============================================================\n","âœ“ Validated directory structure\n","  Content images: my_dataset/val_unseen_both/ContentImage\n","  Target images: my_dataset/val_unseen_both/TargetImage\n","\n","============================================================\n","BUILDING DATASET\n","============================================================\n","\n","Found 3 style directories\n","\n","Processing style-character pairs...\n","ðŸŽ¨ Styles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 20.18it/s]\n","\n","âœ“ Loaded 129 image pairs\n","\n","âœ“ Dataset created with 129 samples\n","  Columns: ['character', 'char_index', 'style', 'style_index', 'content_image', 'target_image', 'font']\n","  Features: {'character': Value('string'), 'char_index': Value('int64'), 'style': Value('string'), 'style_index': Value('int64'), 'content_image': Image(mode=None, decode=True), 'target_image': Image(mode=None, decode=True), 'font': Value('string')}\n","\n","============================================================\n","PUSHING TO HUB\n","============================================================\n","\n","Repository: dzungpham/font-diffusion-generated-data\n","Split: val_unseen_both\n","Private: True\n","Uploading the dataset shards:   0% 0/1 [00:00<?, ? shards/s]\n","Map: 100% 129/129 [00:00<00:00, 13718.69 examples/s]\n","\n","Creating parquet from Arrow format: 100% 2/2 [00:00<00:00, 669.70ba/s]\n","Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n","New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n","\n","                              : 100% 1.54M/1.54M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 1.54M/1.54M [00:00<00:00, 6.70MB/s,   ???B/s  ]\n","\n","                              : 100% 1.54M/1.54M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 1.54M/1.54M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 1.54M/1.54M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 1.54M/1.54M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 1.54M/1.54M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 1.54M/1.54M [00:01<00:00, 1.25MB/s,  0.00B/s  ]\n","New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n","                              : 100% 1.54M/1.54M [00:01<?, ?B/s]\n","Uploading the dataset shards: 100% 1/1 [00:03<00:00,  3.11s/ shards]\n","README.md: 1.30kB [00:00, 6.47MB/s]\n","\n","âœ“ Successfully pushed to Hub!\n","  Dataset URL: https://huggingface.co/datasets/dzungpham/font-diffusion-generated-data\n","\n","âœ“ Done!\n","\n","============================================================\n","FONTDIFFUSION DATASET CREATOR\n","============================================================\n","âœ“ Validated directory structure\n","  Content images: my_dataset/val_seen_style_unseen_char/ContentImage\n","  Target images: my_dataset/val_seen_style_unseen_char/TargetImage\n","\n","============================================================\n","BUILDING DATASET\n","============================================================\n","\n","Found 11 style directories\n","\n","Processing style-character pairs...\n","ðŸŽ¨ Styles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 28.37it/s]\n","\n","âœ“ Loaded 473 image pairs\n","\n","âœ“ Dataset created with 473 samples\n","  Columns: ['character', 'char_index', 'style', 'style_index', 'content_image', 'target_image', 'font']\n","  Features: {'character': Value('string'), 'char_index': Value('int64'), 'style': Value('string'), 'style_index': Value('int64'), 'content_image': Image(mode=None, decode=True), 'target_image': Image(mode=None, decode=True), 'font': Value('string')}\n","\n","============================================================\n","PUSHING TO HUB\n","============================================================\n","\n","Repository: dzungpham/font-diffusion-generated-data\n","Split: val_seen_style_unseen_char\n","Private: True\n","Uploading the dataset shards:   0% 0/1 [00:00<?, ? shards/s]\n","Map: 100% 473/473 [00:00<00:00, 18912.17 examples/s]\n","\n","Creating parquet from Arrow format: 100% 5/5 [00:00<00:00, 476.47ba/s]\n","Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n","New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n","\n","                              : 100% 5.40M/5.40M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 5.40M/5.40M [00:00<00:00, 18.2MB/s,   ???B/s  ]\n","\n","                              : 100% 5.40M/5.40M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 5.40M/5.40M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 5.40M/5.40M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 5.40M/5.40M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 5.40M/5.40M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 5.40M/5.40M [00:01<00:00, 4.16MB/s,  0.00B/s  ]\n","New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n","                              : 100% 5.40M/5.40M [00:01<?, ?B/s]\n","Uploading the dataset shards: 100% 1/1 [00:02<00:00,  2.33s/ shards]\n","README.md: 1.30kB [00:00, 5.38MB/s]\n","\n","âœ“ Successfully pushed to Hub!\n","  Dataset URL: https://huggingface.co/datasets/dzungpham/font-diffusion-generated-data\n","\n","âœ“ Done!\n","\n","============================================================\n","FONTDIFFUSION DATASET CREATOR\n","============================================================\n","âœ“ Validated directory structure\n","  Content images: my_dataset/val_unseen_style_seen_char/ContentImage\n","  Target images: my_dataset/val_unseen_style_seen_char/TargetImage\n","\n","============================================================\n","BUILDING DATASET\n","============================================================\n","\n","Found 3 style directories\n","\n","Processing style-character pairs...\n","ðŸŽ¨ Styles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.00it/s]\n","\n","âœ“ Loaded 738 image pairs\n","\n","âœ“ Dataset created with 738 samples\n","  Columns: ['character', 'char_index', 'style', 'style_index', 'content_image', 'target_image', 'font']\n","  Features: {'character': Value('string'), 'char_index': Value('int64'), 'style': Value('string'), 'style_index': Value('int64'), 'content_image': Image(mode=None, decode=True), 'target_image': Image(mode=None, decode=True), 'font': Value('string')}\n","\n","============================================================\n","PUSHING TO HUB\n","============================================================\n","\n","Repository: dzungpham/font-diffusion-generated-data\n","Split: val_unseen_style_seen_char\n","Private: True\n","Uploading the dataset shards:   0% 0/1 [00:00<?, ? shards/s]\n","Map: 100% 738/738 [00:00<00:00, 21040.37 examples/s]\n","\n","Creating parquet from Arrow format: 100% 8/8 [00:00<00:00, 590.78ba/s]\n","Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n","New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n","\n","                              : 100% 10.7M/10.7M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 10.7M/10.7M [00:00<00:00, 20.8MB/s,   ???B/s  ]\n","\n","                              : 100% 10.7M/10.7M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 10.7M/10.7M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 10.7M/10.7M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 10.7M/10.7M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 10.7M/10.7M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 10.7M/10.7M [00:01<00:00, 7.08MB/s,  0.00B/s  ]\n","New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n","                              : 100% 10.7M/10.7M [00:00<?, ?B/s]\n","Uploading the dataset shards: 100% 1/1 [00:02<00:00,  2.82s/ shards]\n","README.md: 1.30kB [00:00, 5.51MB/s]\n","\n","âœ“ Successfully pushed to Hub!\n","  Dataset URL: https://huggingface.co/datasets/dzungpham/font-diffusion-generated-data\n","\n","âœ“ Done!\n"]}],"execution_count":16},{"cell_type":"code","source":"import torch, gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zXSQHJ3xVOSc","outputId":"4d89a176-c286-442f-eb6e-01b39c994e64","trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":["93"]},"metadata":{},"execution_count":17}],"execution_count":17},{"cell_type":"code","source":"# Training phase 1\nimport wandb\nwandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n!accelerate launch FontDiffusion/my_train.py \\\n    --seed=123 \\\n    --experience_name=\"FontDiffuser_training_phase_1\" \\\n    --data_root=\"my_dataset\" \\\n    --output_dir=\"outputs/FontDiffuser\" \\\n    --report_to=\"wandb\" \\\n    --resolution=96 \\\n    --style_image_size=96 \\\n    --content_image_size=96 \\\n    --content_encoder_downsample_size=3 \\\n    --channel_attn=True \\\n    --content_start_channel=64 \\\n    --style_start_channel=64 \\\n    --train_batch_size=8 \\\n    --perceptual_coefficient=0.03 \\\n    --offset_coefficient=0.7 \\\n    --max_train_steps=100 \\\n    --ckpt_interval=50 \\\n    --gradient_accumulation_steps=2 \\\n    --log_interval=50 \\\n    --learning_rate=1e-4 \\\n    --lr_scheduler=\"linear\" \\\n    --lr_warmup_steps=10000 \\\n    --drop_prob=0.1 \\\n    --mixed_precision=\"no\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FxxJ9qy4KIZH","outputId":"f554de7b-f455-4a5b-f8e5-4b6cba3f756b","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":["The following values were not passed to `accelerate launch` and had defaults used instead:\n","\t`--num_processes` was set to a value of `1`\n","\t`--num_machines` was set to a value of `1`\n","\t`--mixed_precision` was set to a value of `'no'`\n","\t`--dynamo_backend` was set to a value of `'no'`\n","To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n","/usr/local/lib/python3.12/dist-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n","  from pkg_resources import resource_stream, resource_exists\n","pygame 2.6.1 (SDL 2.28.4, Python 3.12.12)\n","Hello from the pygame community. https://www.pygame.org/contribute.html\n","Load the down block  DownBlock2D\n","Load the down block  MCADownBlock2D\n","The style_attention cross attention dim in Down Block 1 layer is 1024\n","The style_attention cross attention dim in Down Block 2 layer is 1024\n","Load the down block  MCADownBlock2D\n","The style_attention cross attention dim in Down Block 1 layer is 1024\n","The style_attention cross attention dim in Down Block 2 layer is 1024\n","Load the down block  DownBlock2D\n","Load the up block  UpBlock2D\n","Load the up block  StyleRSIUpBlock2D\n","Load the up block  StyleRSIUpBlock2D\n","Load the up block  UpBlock2D\n","Param count for Ds initialized parameters: 20591296\n","Get CG-GAN Style Encoder!\n","Param count for Ds initialized parameters: 1187008\n","Get CG-GAN Content Encoder!\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","2025-12-30 16:47:37.952030: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1767113257.976139   26471 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1767113257.983230   26471 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1767113258.000965   26471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1767113258.000989   26471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1767113258.000996   26471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1767113258.001001   26471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-12-30 16:47:38.005699: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdungngocpham171\u001b[0m (\u001b[33mdungngocpham171-university-of-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m setting up run bmpizyuv (0.1s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run bmpizyuv (0.1s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m setting up run bmpizyuv (0.1s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20251230_164744-bmpizyuv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msolar-dream-11\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dungngocpham171-university-of-science/FontDiffuser_training_phase_1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dungngocpham171-university-of-science/FontDiffuser_training_phase_1/runs/bmpizyuv\u001b[0m\n","Steps:   0% 0/100 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"/content/FontDiffusion/my_train.py\", line 433, in <module>\n","    main()\n","  File \"/content/FontDiffusion/my_train.py\", line 298, in main\n","    for step, samples in enumerate(train_dataloader):\n","                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/accelerate/data_loader.py\", line 567, in __iter__\n","    current_batch = next(dataloader_iter)\n","                    ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 732, in __next__\n","    data = self._next_data()\n","           ^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 788, in _next_data\n","    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","            ~~~~~~~~~~~~^^^^^\n","  File \"/content/FontDiffusion/dataset/font_dataset.py\", line 53, in __getitem__\n","    content_image = Image.open(content_image_path).convert('RGB')\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/PIL/Image.py\", line 3431, in open\n","    fp = builtins.open(filename, \"rb\")\n","         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/my_dataset/train/ContentImage/char492.png'\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33msolar-dream-11\u001b[0m at: \u001b[34m\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251230_164744-bmpizyuv/logs\u001b[0m\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/accelerate\", line 10, in <module>\n","    sys.exit(main())\n","             ^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n","    args.func(args)\n","  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1281, in launch_command\n","    simple_launcher(args)\n","  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 869, in simple_launcher\n","    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n","subprocess.CalledProcessError: Command '['/usr/bin/python3', 'FontDiffusion/my_train.py', '--seed=123', '--experience_name=FontDiffuser_training_phase_1', '--data_root=my_dataset', '--output_dir=outputs/FontDiffuser', '--report_to=wandb', '--resolution=96', '--style_image_size=96', '--content_image_size=96', '--content_encoder_downsample_size=3', '--channel_attn=True', '--content_start_channel=64', '--style_start_channel=64', '--train_batch_size=8', '--perceptual_coefficient=0.03', '--offset_coefficient=0.7', '--max_train_steps=100', '--ckpt_interval=50', '--gradient_accumulation_steps=2', '--log_interval=50', '--learning_rate=1e-4', '--lr_scheduler=linear', '--lr_warmup_steps=10000', '--drop_prob=0.1', '--mixed_precision=no']' returned non-zero exit status 1.\n"]}],"execution_count":18},{"cell_type":"code","source":"# Training phase 2\n!wandb login\n!accelerate launch FontDiffusion/my_train.py \\\n    --seed=123 \\\n    --experience_name=\"FontDiffuser_training_phase_2\" \\\n    --data_root=\"my_dataset\" \\\n    --output_dir=\"outputs/FontDiffuser\" \\\n    --report_to=\"wandb\" \\\n    --phase_2 \\\n    --phase_1_ckpt_dir=\"outputs/FontDiffuser/global_step_2000\" \\\n    --scr_ckpt_path=\"ckpt/scr_210000.pth\" \\\n    --sc_coefficient=0.05 \\\n    --num_neg=13 \\\n    --resolution=96 \\\n    --style_image_size=96 \\\n    --content_image_size=96 \\\n    --content_encoder_downsample_size=3 \\\n    --channel_attn=True \\\n    --content_start_channel=64 \\\n    --style_start_channel=64 \\\n    --train_batch_size=8 \\\n    --perceptual_coefficient=0.03 \\\n    --offset_coefficient=0.4 \\\n    --max_train_steps=100 \\\n    --ckpt_interval=50 \\\n    --gradient_accumulation_steps=2 \\\n    --log_interval=50 \\\n    --learning_rate=1e-5 \\\n    --lr_scheduler=\"constant\" \\\n    --lr_warmup_steps=1000 \\\n    --drop_prob=0.1 \\\n    --mixed_precision=\"no\"\n","metadata":{"id":"J4bplsS6pQna","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python /content/FontDiffusion/pth2safetensores.py \\\n    --weights_dir \"ckpt\" \\\n    --repo_id \"dzungpham/font-diffusion-weights\" \\\n    --token \"{HF_TOKEN}\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nF8opWokKcMS","outputId":"2838655d-5d24-4808-813a-39f20ec239a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","PYTORCH TO SAFETENSORS CONVERTER & HF UPLOADER\n","======================================================================\n","\n","======================================================================\n","VALIDATING INPUTS\n","======================================================================\n","âœ“ Weights directory: ckpt\n","  Contents: 4 files\n","âœ“ Repository ID: dzungpham/font-diffusion-weights\n","âœ“ HF token: ********************\n","\n","âœ“ Files to process: 5\n","  âœ“ content_encoder.pth (4.54 MB)\n","  âœ“ style_encoder.pth (78.59 MB)\n","  âœ“ unet.pth (300.55 MB)\n","  âš  total_model.pth (not found)\n","  âš  scr.pth (not found)\n","\n","======================================================================\n","CONVERTING .pth TO SAFETENSORS\n","======================================================================\n","\n","ðŸ“¦ content_encoder.pth\n","  âœ“ Saved to: ckpt/content_encoder.safetensors\n","    Original: 4.54 MB â†’ Safetensors: 4.54 MB\n","    Compression: 0.2%\n","\n","ðŸ“¦ style_encoder.pth\n","  âœ“ Saved to: ckpt/style_encoder.safetensors\n","    Original: 78.59 MB â†’ Safetensors: 78.58 MB\n","    Compression: 0.0%\n","\n","ðŸ“¦ unet.pth\n","  âœ“ Saved to: ckpt/unet.safetensors\n","    Original: 300.55 MB â†’ Safetensors: 300.34 MB\n","    Compression: 0.1%\n","\n","âš  total_model.pth: Not found, skipping\n","\n","âš  scr.pth: Not found, skipping\n","\n","----------------------------------------------------------------------\n","Conversion complete: 3 succeeded, 0 failed\n","\n","======================================================================\n","UPLOADING TO HUGGING FACE HUB\n","======================================================================\n","\n","ðŸ“¤ Creating/verifying repository...\n","  Repo ID: dzungpham/font-diffusion-weights\n","  Private: True\n","âœ“ Repository ready\n","\n","ðŸ“¤ Uploading folder: ckpt\n","Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n","New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:   1% 557k/82.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","Processing Files (0 / 2)      :   0% 4.87M/1.09G [00:02<07:27, 2.42MB/s, 2.71MB/s  ]\n","New Data Upload               :   1% 557k/67.0M [00:02<03:59, 277kB/s,  310kB/s  ]\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  10% 30.3M/315M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:   1% 557k/82.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Processing Files (0 / 3)      :   3% 35.2M/1.09G [00:02<00:50, 21.0MB/s, 17.6MB/s  ]\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:   6% 4.74M/82.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:   5% 14.8M/315M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:   1% 557k/82.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  14% 45.6M/315M [00:00<00:03, 74.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:   6% 4.74M/82.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","Processing Files (0 / 5)      :   6% 69.9M/1.09G [00:02<00:23, 43.9MB/s, 31.8MB/s  ]\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:   1% 557k/82.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  17% 53.6M/315M [00:00<00:04, 58.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:   7% 5.49M/82.4M [00:00<00:19, 3.85MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","Processing Files (0 / 5)      :   8% 86.2M/1.09G [00:02<00:19, 50.8MB/s, 36.0MB/s  ]\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:   1% 557k/82.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  22% 69.0M/315M [00:00<00:03, 64.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:   7% 5.49M/82.4M [00:00<00:40, 1.90MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","Processing Files (0 / 5)      :  10% 109M/1.09G [00:02<00:15, 63.7MB/s, 42.1MB/s  ] \n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:   5% 15.2M/284M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:01<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:   3% 2.23M/82.4M [00:01<00:48, 1.67MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  22% 69.0M/315M [00:00<00:05, 48.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:   9% 7.26M/82.4M [00:00<00:17, 4.21MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  14% 44.3M/315M [00:00<00:05, 49.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  13% 142M/1.09G [00:03<00:10, 86.1MB/s, 50.8MB/s  ]\n","New Data Upload               :   3% 2.23M/67.0M [00:03<01:15, 855kB/s,  796kB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:01<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:   4% 3.34M/82.4M [00:01<00:34, 2.32MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  24% 76.6M/315M [00:01<00:05, 46.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  19% 15.8M/82.4M [00:00<00:04, 13.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  14% 44.3M/315M [00:00<00:07, 37.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  16% 175M/1.09G [00:03<00:08, 105MB/s, 58.4MB/s  ] \n","New Data Upload               :   5% 3.34M/67.0M [00:03<00:46, 1.36MB/s, 1.11MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:01<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:   7% 5.57M/82.4M [00:01<00:21, 3.58MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  27% 84.6M/315M [00:01<00:05, 45.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  22% 17.8M/82.4M [00:00<00:04, 13.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  19% 59.7M/315M [00:00<00:05, 45.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  19% 203M/1.09G [00:03<00:07, 114MB/s, 63.4MB/s  ]\n","New Data Upload               :   8% 5.57M/67.0M [00:03<00:23, 2.64MB/s, 1.74MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:01<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:   9% 7.24M/82.4M [00:01<00:18, 4.17MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  29% 92.1M/315M [00:01<00:05, 44.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  31% 25.8M/82.4M [00:01<00:03, 17.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  21% 67.7M/315M [00:01<00:05, 44.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  22% 235M/1.09G [00:03<00:06, 127MB/s, 69.3MB/s  ]\n","New Data Upload               :  11% 7.24M/67.0M [00:03<00:16, 3.54MB/s, 2.13MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:01<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  12% 10.0M/82.4M [00:01<00:13, 5.25MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  32% 100M/315M [00:01<00:04, 43.4MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  34% 28.4M/82.4M [00:01<00:03, 16.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  24% 74.2M/315M [00:01<00:05, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  25% 271M/1.09G [00:03<00:05, 140MB/s, 75.2MB/s  ]\n","New Data Upload               :  15% 10.0M/67.0M [00:03<00:10, 5.44MB/s, 2.79MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:02<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  15% 12.3M/82.4M [00:02<00:12, 5.84MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  37% 115M/315M [00:01<00:04, 47.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  37% 30.4M/82.4M [00:01<00:03, 16.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  26% 81.3M/315M [00:01<00:05, 41.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  28% 305M/1.09G [00:04<00:05, 150MB/s, 80.4MB/s  ]\n","New Data Upload               :  18% 12.3M/67.0M [00:04<00:08, 6.65MB/s, 3.23MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:02<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  17% 13.9M/82.4M [00:02<00:11, 6.07MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  39% 123M/315M [00:02<00:04, 46.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  39% 32.0M/82.4M [00:01<00:03, 15.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  28% 88.8M/315M [00:01<00:05, 41.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  30% 331M/1.09G [00:04<00:05, 144MB/s, 82.8MB/s  ]\n","New Data Upload               :  21% 13.9M/67.0M [00:04<00:07, 7.04MB/s, 3.48MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:02<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  22% 17.8M/82.4M [00:02<00:08, 7.20MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  42% 131M/315M [00:02<00:04, 45.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  43% 35.6M/82.4M [00:01<00:03, 15.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  31% 96.7M/315M [00:01<00:05, 41.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  34% 370M/1.09G [00:04<00:04, 159MB/s, 88.2MB/s  ]\n","New Data Upload               :  27% 17.8M/67.0M [00:04<00:04, 10.1MB/s, 4.25MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:02<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  27% 22.3M/82.4M [00:02<00:07, 8.35MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  46% 146M/315M [00:02<00:03, 48.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  48% 39.6M/82.4M [00:02<00:02, 15.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  33% 104M/315M [00:02<00:05, 40.8MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  38% 409M/1.09G [00:04<00:04, 169MB/s, 93.1MB/s  ]\n","New Data Upload               :  33% 22.3M/67.0M [00:04<00:03, 13.2MB/s, 5.07MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:02<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  32% 26.2M/82.4M [00:02<00:06, 9.15MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  49% 154M/315M [00:02<00:03, 47.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  52% 43.2M/82.4M [00:02<00:02, 16.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  35% 112M/315M [00:02<00:05, 40.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  40% 439M/1.09G [00:04<00:03, 164MB/s, 95.6MB/s  ]\n","New Data Upload               :  39% 26.2M/67.0M [00:04<00:02, 15.0MB/s, 5.69MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:03<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  37% 30.1M/82.4M [00:03<00:05, 9.84MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  54% 169M/315M [00:02<00:02, 49.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  57% 46.8M/82.4M [00:02<00:02, 16.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  40% 127M/315M [00:02<00:04, 43.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  45% 485M/1.09G [00:05<00:03, 183MB/s,  101MB/s  ]\n","New Data Upload               :  45% 30.1M/67.0M [00:05<00:02, 16.3MB/s, 6.27MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:03<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  41% 34.0M/82.4M [00:03<00:04, 10.4MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  56% 177M/315M [00:03<00:02, 48.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  61% 50.4M/82.4M [00:02<00:01, 16.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  43% 135M/315M [00:02<00:04, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  47% 516M/1.09G [00:05<00:03, 174MB/s,  103MB/s  ]\n","New Data Upload               :  51% 34.0M/67.0M [00:05<00:01, 17.1MB/s, 6.80MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:03<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  47% 38.4M/82.4M [00:03<00:03, 11.1MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  59% 185M/315M [00:03<00:02, 48.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  66% 54.5M/82.4M [00:02<00:01, 16.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  47% 149M/315M [00:02<00:03, 44.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  52% 562M/1.09G [00:05<00:02, 191MB/s,  108MB/s  ]\n","New Data Upload               :  57% 38.4M/67.0M [00:05<00:01, 18.6MB/s, 7.39MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:03<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  49% 40.1M/82.4M [00:03<00:03, 11.0MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  61% 192M/315M [00:03<00:02, 47.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  68% 56.0M/82.4M [00:03<00:01, 16.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  50% 157M/315M [00:03<00:03, 44.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  54% 589M/1.09G [00:05<00:02, 174MB/s,  109MB/s  ]\n","New Data Upload               :  60% 40.1M/67.0M [00:05<00:01, 15.7MB/s, 7.43MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:03<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  51% 41.8M/82.4M [00:03<00:03, 10.8MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  64% 200M/315M [00:03<00:02, 47.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  70% 57.5M/82.4M [00:03<00:01, 15.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  52% 165M/315M [00:03<00:03, 44.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  56% 608M/1.09G [00:05<00:03, 150MB/s,  109MB/s  ]\n","New Data Upload               :  62% 41.8M/67.0M [00:05<00:01, 13.5MB/s, 7.46MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:04<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  55% 45.7M/82.4M [00:04<00:03, 11.3MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  66% 208M/315M [00:03<00:02, 46.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  74% 61.1M/82.4M [00:03<00:01, 15.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  55% 172M/315M [00:03<00:03, 43.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  60% 654M/1.09G [00:06<00:02, 173MB/s,  113MB/s  ]\n","New Data Upload               :  68% 45.7M/67.0M [00:06<00:01, 15.1MB/s, 7.88MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:04<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  61% 50.1M/82.4M [00:04<00:02, 11.8MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  69% 216M/315M [00:03<00:02, 46.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  79% 65.2M/82.4M [00:03<00:01, 15.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  57% 180M/315M [00:03<00:03, 43.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  63% 686M/1.09G [00:06<00:02, 171MB/s,  114MB/s  ]\n","New Data Upload               :  75% 50.1M/67.0M [00:06<00:00, 17.5MB/s, 8.36MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:04<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  66% 54.6M/82.4M [00:04<00:02, 12.3MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  73% 231M/315M [00:04<00:01, 47.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  84% 69.3M/82.4M [00:03<00:00, 16.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  61% 192M/315M [00:03<00:02, 44.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  67% 730M/1.09G [00:06<00:01, 186MB/s,  118MB/s  ]\n","New Data Upload               :  81% 54.6M/67.0M [00:06<00:00, 18.9MB/s, 8.81MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:04<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  70% 57.9M/82.4M [00:04<00:01, 12.5MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  76% 239M/315M [00:04<00:01, 47.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  88% 72.4M/82.4M [00:04<00:00, 16.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  63% 199M/315M [00:04<00:02, 43.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  70% 759M/1.09G [00:06<00:01, 173MB/s,  119MB/s  ]\n","New Data Upload               :  86% 57.9M/67.0M [00:06<00:00, 18.2MB/s, 9.05MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:04<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  76% 62.4M/82.4M [00:04<00:01, 12.9MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  78% 247M/315M [00:04<00:01, 47.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  93% 76.4M/82.4M [00:04<00:00, 16.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  68% 213M/315M [00:04<00:02, 45.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  73% 797M/1.09G [00:06<00:01, 180MB/s,  121MB/s  ]\n","New Data Upload               :  93% 62.4M/67.0M [00:06<00:00, 19.5MB/s, 9.46MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:04<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  80% 66.3M/82.4M [00:04<00:01, 13.1MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  81% 255M/315M [00:04<00:01, 46.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  97% 80.0M/82.4M [00:04<00:00, 16.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  70% 221M/315M [00:04<00:02, 44.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  77% 835M/1.09G [00:07<00:01, 182MB/s,  123MB/s  ]\n","New Data Upload               :  99% 66.3M/67.0M [00:07<00:00, 19.5MB/s, 9.75MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:05<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  80% 66.3M/82.4M [00:05<00:01, 12.6MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  83% 263M/315M [00:05<00:01, 46.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  97% 80.0M/82.4M [00:04<00:00, 15.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  70% 221M/315M [00:04<00:02, 42.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  78% 851M/1.09G [00:07<00:01, 150MB/s,  122MB/s  ]\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:05<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  81% 66.8M/82.4M [00:05<00:01, 12.3MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  86% 271M/315M [00:05<00:00, 46.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.5M/82.4M [00:04<00:00, 15.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  70% 221M/315M [00:04<00:02, 41.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  80% 868M/1.09G [00:07<00:01, 130MB/s,  121MB/s  ]\n","New Data Upload               :  50% 66.8M/134M [00:07<00:06, 11.1MB/s, 9.29MB/s  ] \u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:05<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  81% 66.8M/82.4M [00:05<00:01, 11.8MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  91% 286M/315M [00:05<00:00, 47.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.5M/82.4M [00:05<00:00, 14.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  70% 221M/315M [00:05<00:02, 39.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  82% 898M/1.09G [00:07<00:01, 137MB/s,  121MB/s  ]\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:05<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  81% 66.8M/82.4M [00:05<00:01, 11.4MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  92% 290M/315M [00:05<00:00, 46.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.5M/82.4M [00:05<00:00, 14.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  76% 239M/315M [00:05<00:01, 41.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  85% 929M/1.09G [00:07<00:01, 142MB/s,  122MB/s  ]\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:06<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  81% 66.8M/82.4M [00:06<00:01, 11.0MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  92% 290M/315M [00:05<00:00, 44.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.5M/82.4M [00:05<00:00, 13.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  76% 240M/315M [00:05<00:01, 40.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  87% 944M/1.09G [00:08<00:01, 123MB/s,  121MB/s  ]\n","New Data Upload               :  36% 67.4M/188M [00:08<00:19, 6.06MB/s, 8.64MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  90% 4.31M/4.77M [00:06<?, ?B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  81% 67.0M/82.4M [00:06<00:01, 10.7MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  92% 290M/315M [00:06<00:00, 43.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.7M/82.4M [00:05<00:00, 13.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  76% 240M/315M [00:05<00:01, 38.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 6)      :  87% 945M/1.09G [00:08<00:01, 86.7MB/s,  118MB/s  ]\n","New Data Upload               :  32% 67.5M/209M [00:08<00:28, 5.04MB/s, 8.45MB/s  ]\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","  ...ntent_encoder.safetensors:   1% 49.0k/4.76M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  91% 4.32M/4.77M [00:06<10:14, 730B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  81% 67.1M/82.4M [00:06<00:01, 10.4MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  92% 290M/315M [00:06<00:00, 41.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.7M/82.4M [00:05<00:00, 12.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  76% 240M/315M [00:05<00:02, 37.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  93% 264M/284M [00:05<00:00, 46.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  87% 946M/1.09G [00:08<00:02, 62.2MB/s,  115MB/s  ]\n","New Data Upload               :  33% 68.7M/209M [00:08<00:27, 5.15MB/s, 8.37MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  91% 4.32M/4.77M [00:06<10:33, 708B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  81% 67.1M/82.4M [00:06<00:01, 10.1MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  92% 290M/315M [00:06<00:00, 40.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.7M/82.4M [00:06<00:00, 12.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  76% 240M/315M [00:06<00:02, 36.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  93% 264M/284M [00:05<00:00, 44.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  87% 946M/1.09G [00:08<00:03, 44.5MB/s,  113MB/s  ]\n","New Data Upload               :  33% 69.2M/209M [00:08<00:30, 4.60MB/s, 8.24MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  91% 4.32M/4.77M [00:06<10:52, 687B/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  81% 67.1M/82.4M [00:06<00:01, 9.79MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  92% 290M/315M [00:06<00:00, 39.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.7M/82.4M [00:06<00:00, 11.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  77% 241M/315M [00:06<00:02, 35.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  93% 264M/284M [00:05<00:00, 42.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  87% 947M/1.09G [00:08<00:04, 32.8MB/s,  110MB/s  ]\n","New Data Upload               :  34% 70.3M/209M [00:08<00:28, 4.84MB/s, 8.18MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  91% 4.32M/4.77M [00:06<05:32, 1.33kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  82% 67.3M/82.4M [00:06<00:01, 9.54MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  92% 291M/315M [00:06<00:00, 38.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.7M/82.4M [00:06<00:00, 11.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  77% 242M/315M [00:06<00:02, 34.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  93% 264M/284M [00:05<00:00, 41.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  87% 949M/1.09G [00:09<00:05, 25.5MB/s,  108MB/s  ]\n","New Data Upload               :  34% 72.0M/209M [00:09<00:23, 5.77MB/s, 8.18MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  91% 4.32M/4.77M [00:07<05:42, 1.30kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  82% 67.3M/82.4M [00:07<00:01, 9.27MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  92% 291M/315M [00:07<00:00, 37.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.7M/82.4M [00:06<00:00, 11.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  77% 242M/315M [00:06<00:02, 33.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  93% 264M/284M [00:06<00:00, 40.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  87% 950M/1.09G [00:09<00:07, 18.7MB/s,  106MB/s  ]\n","New Data Upload               :  35% 72.6M/209M [00:09<00:27, 4.95MB/s, 8.06MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  91% 4.33M/4.77M [00:07<02:52, 2.53kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  82% 67.6M/82.4M [00:07<00:01, 9.06MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  92% 291M/315M [00:07<00:00, 36.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.7M/82.4M [00:06<00:00, 10.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  78% 244M/315M [00:06<00:02, 32.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  93% 266M/284M [00:06<00:00, 39.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  88% 954M/1.09G [00:09<00:06, 19.9MB/s,  104MB/s  ]\n","New Data Upload               :  37% 77.0M/209M [00:09<00:13, 9.83MB/s, 8.37MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  91% 4.35M/4.77M [00:07<01:14, 5.53kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  83% 68.4M/82.4M [00:07<00:01, 8.93MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  93% 292M/315M [00:07<00:00, 35.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.8M/82.4M [00:07<00:00, 10.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  78% 246M/315M [00:07<00:02, 32.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  94% 267M/284M [00:06<00:00, 38.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  88% 960M/1.09G [00:09<00:05, 23.2MB/s,  102MB/s  ]\n","New Data Upload               :  40% 83.1M/209M [00:09<00:07, 15.8MB/s, 8.85MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  91% 4.35M/4.77M [00:07<01:16, 5.39kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  83% 68.4M/82.4M [00:07<00:01, 8.70MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  93% 293M/315M [00:07<00:00, 34.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.8M/82.4M [00:07<00:00, 10.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  79% 250M/315M [00:07<00:02, 31.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  94% 267M/284M [00:06<00:00, 37.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  89% 964M/1.09G [00:09<00:05, 21.4MB/s,  100MB/s  ]\n","New Data Upload               :  41% 86.5M/209M [00:09<00:07, 16.0MB/s, 9.01MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  92% 4.37M/4.77M [00:08<00:56, 7.01kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  84% 68.9M/82.4M [00:08<00:01, 8.54MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  93% 293M/315M [00:07<00:00, 33.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 80.9M/82.4M [00:07<00:00, 10.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  80% 253M/315M [00:07<00:01, 31.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  95% 269M/284M [00:06<00:00, 36.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  89% 971M/1.09G [00:10<00:04, 25.2MB/s, 99.1MB/s  ]\n","New Data Upload               :  45% 93.2M/209M [00:10<00:05, 21.1MB/s, 9.51MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  92% 4.40M/4.77M [00:08<00:35, 10.3kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  85% 69.8M/82.4M [00:08<00:01, 8.45MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  94% 295M/315M [00:08<00:00, 33.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 81.0M/82.4M [00:07<00:00, 9.78MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  82% 257M/315M [00:07<00:01, 31.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  96% 273M/284M [00:07<00:00, 35.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  90% 981M/1.09G [00:10<00:03, 33.7MB/s, 98.2MB/s  ]\n","New Data Upload               :  50% 104M/209M [00:10<00:03, 30.5MB/s, 10.4MB/s  ] \u001b[A\n","\n","  .../ckpt/content_encoder.pth:  93% 4.42M/4.77M [00:08<00:27, 12.8kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  86% 70.6M/82.4M [00:08<00:01, 8.34MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  94% 296M/315M [00:08<00:00, 32.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 81.1M/82.4M [00:07<00:00, 9.55MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  83% 261M/315M [00:07<00:01, 30.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  97% 275M/284M [00:07<00:00, 35.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  91% 989M/1.09G [00:10<00:02, 35.4MB/s, 97.0MB/s  ]\n","New Data Upload               :  53% 112M/209M [00:10<00:02, 33.0MB/s, 10.9MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  93% 4.43M/4.77M [00:08<00:23, 14.1kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  86% 71.1M/82.4M [00:08<00:01, 8.21MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  94% 297M/315M [00:08<00:00, 31.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  98% 81.1M/82.4M [00:08<00:00, 9.32MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  84% 265M/315M [00:08<00:01, 30.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  97% 276M/284M [00:07<00:00, 34.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  91% 996M/1.09G [00:10<00:02, 34.2MB/s, 97.6MB/s  ]\n","New Data Upload               :  56% 118M/209M [00:10<00:02, 32.4MB/s, 11.5MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  93% 4.45M/4.77M [00:08<00:20, 15.4kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  87% 71.6M/82.4M [00:08<00:01, 8.07MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  95% 298M/315M [00:08<00:00, 31.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  99% 81.2M/82.4M [00:08<00:00, 9.11MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  85% 268M/315M [00:08<00:01, 30.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  98% 279M/284M [00:07<00:00, 33.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  92% 1.00G/1.09G [00:10<00:02, 36.6MB/s, 98.4MB/s  ]\n","New Data Upload               :  60% 126M/209M [00:10<00:02, 35.2MB/s, 12.4MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  94% 4.48M/4.77M [00:09<00:15, 18.2kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  88% 72.5M/82.4M [00:09<00:01, 8.00MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  95% 299M/315M [00:08<00:00, 30.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  99% 81.3M/82.4M [00:08<00:00, 8.91MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  86% 271M/315M [00:08<00:01, 29.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth:  99% 283M/284M [00:07<00:00, 33.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  93% 1.01G/1.09G [00:11<00:01, 38.2MB/s, 99.3MB/s  ]\n","New Data Upload               :  64% 134M/209M [00:11<00:02, 37.1MB/s, 13.2MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  95% 4.50M/4.77M [00:09<00:12, 20.8kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  89% 73.5M/82.4M [00:09<00:01, 7.93MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  95% 301M/315M [00:09<00:00, 30.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  99% 81.4M/82.4M [00:08<00:00, 8.72MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  87% 273M/315M [00:08<00:01, 29.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:08<00:00, 32.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  94% 1.02G/1.09G [00:11<00:01, 36.9MB/s, 99.9MB/s  ]\n","New Data Upload               :  67% 141M/209M [00:11<00:01, 36.0MB/s, 13.8MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  95% 4.53M/4.77M [00:09<00:10, 23.4kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  90% 74.5M/82.4M [00:09<00:01, 7.86MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  96% 302M/315M [00:09<00:00, 29.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  99% 81.5M/82.4M [00:08<00:00, 8.53MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  88% 277M/315M [00:08<00:01, 29.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:08<00:00, 32.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  94% 1.03G/1.09G [00:11<00:01, 36.0MB/s,  101MB/s  ]\n","New Data Upload               :  71% 148M/209M [00:11<00:01, 35.3MB/s, 14.5MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  96% 4.56M/4.77M [00:09<00:07, 25.8kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  92% 75.4M/82.4M [00:09<00:00, 7.80MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  96% 304M/315M [00:09<00:00, 29.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  99% 81.6M/82.4M [00:09<00:00, 8.36MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  89% 281M/315M [00:09<00:01, 28.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:08<00:00, 31.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  95% 1.03G/1.09G [00:11<00:01, 35.4MB/s,  101MB/s  ]\n","New Data Upload               :  74% 154M/209M [00:11<00:01, 34.7MB/s, 15.1MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  96% 4.59M/4.77M [00:09<00:06, 28.1kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  93% 76.4M/82.4M [00:09<00:00, 7.73MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  97% 305M/315M [00:09<00:00, 28.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  99% 81.7M/82.4M [00:09<00:00, 8.19MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  90% 284M/315M [00:09<00:01, 28.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:08<00:00, 30.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (0 / 7)      :  95% 1.04G/1.09G [00:11<00:01, 35.0MB/s,  102MB/s  ]\n","New Data Upload               :  77% 161M/209M [00:11<00:01, 34.3MB/s, 15.8MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  97% 4.61M/4.77M [00:10<00:05, 29.9kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  94% 77.2M/82.4M [00:09<00:00, 7.66MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  97% 307M/315M [00:09<00:00, 28.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  99% 81.8M/82.4M [00:09<00:00, 8.03MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  92% 288M/315M [00:09<00:00, 28.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:08<00:00, 29.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (1 / 7)      :  96% 1.05G/1.09G [00:12<00:01, 34.0MB/s,  103MB/s  ]\n","New Data Upload               :  80% 167M/209M [00:12<00:01, 33.3MB/s, 16.4MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  97% 4.63M/4.77M [00:10<00:04, 30.7kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  94% 77.6M/82.4M [00:10<00:00, 7.56MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  98% 307M/315M [00:10<00:00, 27.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  99% 81.9M/82.4M [00:09<00:00, 7.87MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  93% 292M/315M [00:09<00:00, 28.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:09<00:00, 29.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (1 / 7)      :  97% 1.05G/1.09G [00:12<00:01, 31.4MB/s,  103MB/s  ]\n","New Data Upload               :  82% 172M/209M [00:12<00:01, 30.8MB/s, 16.8MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  97% 4.64M/4.77M [00:10<00:03, 31.9kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  95% 78.3M/82.4M [00:10<00:00, 7.47MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  98% 308M/315M [00:10<00:00, 27.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth:  99% 81.9M/82.4M [00:09<00:00, 7.72MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  94% 296M/315M [00:09<00:00, 28.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:09<00:00, 28.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (1 / 7)      :  97% 1.06G/1.09G [00:12<00:01, 30.5MB/s,  100MB/s  ]\n","New Data Upload               :  85% 178M/209M [00:12<00:01, 30.0MB/s, 17.4MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  98% 4.67M/4.77M [00:10<00:02, 33.5kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  96% 79.1M/82.4M [00:10<00:00, 7.41MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  98% 310M/315M [00:10<00:00, 26.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.0M/82.4M [00:10<00:00, 7.58MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  95% 299M/315M [00:10<00:00, 27.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:09<00:00, 28.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (1 / 7)      :  98% 1.06G/1.09G [00:12<00:00, 29.9MB/s, 97.3MB/s  ]\n","New Data Upload               :  88% 183M/209M [00:12<00:00, 29.3MB/s, 17.9MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  99% 4.70M/4.77M [00:10<00:01, 35.5kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  97% 80.0M/82.4M [00:10<00:00, 7.36MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  99% 311M/315M [00:10<00:00, 26.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.1M/82.4M [00:10<00:00, 7.45MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  96% 303M/315M [00:10<00:00, 27.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:09<00:00, 27.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (1 / 7)      :  98% 1.07G/1.09G [00:12<00:00, 31.1MB/s, 96.4MB/s  ]\n","New Data Upload               :  91% 190M/209M [00:12<00:00, 30.5MB/s, 18.6MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth:  99% 4.72M/4.77M [00:10<00:01, 37.4kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  98% 81.0M/82.4M [00:10<00:00, 7.31MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      :  99% 313M/315M [00:10<00:00, 26.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.2M/82.4M [00:10<00:00, 7.32MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  96% 303M/315M [00:10<00:00, 27.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:09<00:00, 26.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (1 / 7)      :  99% 1.07G/1.09G [00:13<00:00, 26.8MB/s, 94.4MB/s  ]\n","New Data Upload               :  92% 194M/209M [00:13<00:00, 26.4MB/s, 18.9MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth: 100% 4.75M/4.77M [00:11<00:00, 39.2kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors:  99% 81.9M/82.4M [00:11<00:00, 7.26MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      : 100% 314M/315M [00:11<00:00, 25.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.4M/82.4M [00:10<00:00, 7.19MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  98% 307M/315M [00:10<00:00, 27.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:10<00:00, 26.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (1 / 7)      :  99% 1.08G/1.09G [00:13<00:00, 28.9MB/s, 91.9MB/s  ]\n","New Data Upload               :  96% 200M/209M [00:13<00:00, 28.5MB/s, 19.4MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth: 100% 4.77M/4.77M [00:11<00:00, 39.8kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors: 100% 82.4M/82.4M [00:11<00:00, 7.18MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      : 100% 315M/315M [00:11<00:00, 25.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.4M/82.4M [00:10<00:00, 7.06MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors:  99% 311M/315M [00:10<00:00, 26.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:10<00:00, 25.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (1 / 7)      : 100% 1.08G/1.09G [00:13<00:00, 28.0MB/s, 89.2MB/s  ]\n","New Data Upload               :  98% 205M/209M [00:13<00:00, 27.5MB/s, 19.8MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth: 100% 4.77M/4.77M [00:11<00:00, 39.1kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors: 100% 82.4M/82.4M [00:11<00:00, 7.05MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      : 100% 315M/315M [00:11<00:00, 25.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.4M/82.4M [00:11<00:00, 6.94MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors: 100% 314M/315M [00:11<00:00, 26.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:10<00:00, 25.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (1 / 7)      : 100% 1.09G/1.09G [00:13<00:00, 24.7MB/s, 86.8MB/s  ]\n","New Data Upload               : 100% 209M/209M [00:13<00:00, 24.2MB/s, 19.9MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth: 100% 4.77M/4.77M [00:11<00:00, 38.4kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors: 100% 82.4M/82.4M [00:11<00:00, 6.93MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      : 100% 315M/315M [00:11<00:00, 24.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.4M/82.4M [00:11<00:00, 6.82MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors: 100% 315M/315M [00:11<00:00, 26.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:10<00:00, 24.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (1 / 7)      : 100% 1.09G/1.09G [00:13<00:00, 18.2MB/s, 83.6MB/s  ]\n","New Data Upload               : 100% 209M/209M [00:13<00:00, 17.8MB/s, 19.8MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth: 100% 4.77M/4.77M [00:11<00:00, 37.8kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors: 100% 82.4M/82.4M [00:11<00:00, 6.82MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      : 100% 315M/315M [00:11<00:00, 24.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.4M/82.4M [00:11<00:00, 6.70MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors: 100% 315M/315M [00:11<00:00, 25.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:10<00:00, 24.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","  ...ntent_encoder.safetensors: 100% 4.76M/4.76M [00:05<00:00, 841kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  .../ckpt/content_encoder.pth: 100% 4.77M/4.77M [00:12<00:00, 37.1kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors: 100% 82.4M/82.4M [00:12<00:00, 6.71MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      : 100% 315M/315M [00:12<00:00, 23.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.4M/82.4M [00:11<00:00, 6.59MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors: 100% 315M/315M [00:11<00:00, 25.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:11<00:00, 24.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (5 / 7)      : 100% 1.09G/1.09G [00:14<00:00, 9.80MB/s, 76.8MB/s  ]\n","New Data Upload               : 100% 209M/209M [00:14<00:00, 9.60MB/s, 19.3MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth: 100% 4.77M/4.77M [00:12<00:00, 36.5kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors: 100% 82.4M/82.4M [00:12<00:00, 6.60MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      : 100% 315M/315M [00:12<00:00, 23.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.4M/82.4M [00:11<00:00, 6.48MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors: 100% 315M/315M [00:11<00:00, 25.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:11<00:00, 23.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","  ...ntent_encoder.safetensors: 100% 4.76M/4.76M [00:06<00:00, 785kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  .../ckpt/content_encoder.pth: 100% 4.77M/4.77M [00:12<00:00, 36.0kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors: 100% 82.4M/82.4M [00:12<00:00, 6.49MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      : 100% 315M/315M [00:12<00:00, 23.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.4M/82.4M [00:12<00:00, 6.37MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors: 100% 315M/315M [00:12<00:00, 24.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:11<00:00, 23.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","  ...ntent_encoder.safetensors: 100% 4.76M/4.76M [00:06<00:00, 759kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  .../ckpt/content_encoder.pth: 100% 4.77M/4.77M [00:12<00:00, 35.4kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors: 100% 82.4M/82.4M [00:12<00:00, 6.39MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      : 100% 315M/315M [00:12<00:00, 22.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.4M/82.4M [00:12<00:00, 6.27MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors: 100% 315M/315M [00:12<00:00, 24.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:11<00:00, 22.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (7 / 7)      : 100% 1.09G/1.09G [00:14<00:00, 5.15MB/s, 66.6MB/s  ]\n","New Data Upload               : 100% 209M/209M [00:14<00:00, 5.04MB/s, 18.3MB/s  ]\u001b[A\n","\n","  .../ckpt/content_encoder.pth: 100% 4.77M/4.77M [00:12<00:00, 34.9kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors: 100% 82.4M/82.4M [00:12<00:00, 6.30MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      : 100% 315M/315M [00:12<00:00, 22.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.4M/82.4M [00:12<00:00, 6.17MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors: 100% 315M/315M [00:12<00:00, 23.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:11<00:00, 22.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","  ...ntent_encoder.safetensors: 100% 4.76M/4.76M [00:06<00:00, 713kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  .../ckpt/content_encoder.pth: 100% 4.77M/4.77M [00:13<00:00, 34.4kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors: 100% 82.4M/82.4M [00:13<00:00, 6.21MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      : 100% 315M/315M [00:12<00:00, 21.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.4M/82.4M [00:12<00:00, 6.08MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors: 100% 315M/315M [00:12<00:00, 23.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:12<00:00, 22.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","  ...ntent_encoder.safetensors: 100% 4.76M/4.76M [00:06<00:00, 694kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","  .../ckpt/content_encoder.pth: 100% 4.77M/4.77M [00:13<00:00, 34.3kB/s]\u001b[A\u001b[A\n","\n","\n","  ...style_encoder.safetensors: 100% 82.4M/82.4M [00:13<00:00, 6.20MB/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","  /content/ckpt/unet.pth      : 100% 315M/315M [00:13<00:00, 21.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","  ...nt/ckpt/style_encoder.pth: 100% 82.4M/82.4M [00:12<00:00, 6.07MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","  ...ent/ckpt/unet.safetensors: 100% 315M/315M [00:12<00:00, 23.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:12<00:00, 22.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","Processing Files (7 / 7)      : 100% 1.09G/1.09G [00:15<00:00, 71.6MB/s, 57.3MB/s  ]\n","New Data Upload               : 100% 209M/209M [00:15<00:00, 13.8MB/s, 17.5MB/s  ]\n","  .../ckpt/content_encoder.pth: 100% 4.77M/4.77M [00:13<00:00, 34.3kB/s]\n","  ...style_encoder.safetensors: 100% 82.4M/82.4M [00:13<00:00, 6.20MB/s]\n","  /content/ckpt/unet.pth      : 100% 315M/315M [00:13<00:00, 21.9MB/s]\n","  ...nt/ckpt/style_encoder.pth: 100% 82.4M/82.4M [00:12<00:00, 6.07MB/s]\n","  ...ent/ckpt/unet.safetensors: 100% 315M/315M [00:12<00:00, 23.5MB/s]\n","  /content/ckpt/scr_210000.pth: 100% 284M/284M [00:12<00:00, 22.1MB/s]\n","  ...ntent_encoder.safetensors: 100% 4.76M/4.76M [00:06<00:00, 692kB/s]\n","\n","âœ“ Upload successful!\n","  Repository URL: https://huggingface.co/models/dzungpham/font-diffusion-weights\n","\n","======================================================================\n","âœ“ ALL DONE!\n","======================================================================\n","\n","ðŸ“¦ Your weights are now available at:\n","   https://huggingface.co/models/dzungpham/font-diffusion-weights\n","\n","ðŸ“– Load them with:\n","   from safetensors.torch import load_file\n","   state = load_file('model.safetensors')\n"]}],"execution_count":20},{"cell_type":"code","source":"import os\nimport zipfile\nfrom pathlib import Path\nfrom typing import List\ndef find_result_folders(base_path: Path, pattern_name: str) -> List[Path]:\n    return [p for p in base_path.glob(pattern_name) if p.is_dir()]\n\ndef zip_folder(folder_path: Path, output_base_path: Path) -> bool:\n    folder_name = folder_path.name\n    zip_path = output_base_path / f\"{folder_name}.zip\"\n    try:\n        print(f\"   -> Zipping folder: {folder_name}...\")\n        with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zipf:\n            for file_path in folder_path.rglob(\"*\"):\n                if file_path.is_file():\n                    arcname = file_path.relative_to(folder_path.parent)\n                    zipf.write(file_path, arcname)\n        print(f\"   âœ… Created ZIP: {zip_path.name}\")\n        return True\n    except Exception as exc:\n        print(f\"   âŒ Failed to zip {folder_name}: {exc}\")\n        return False\n\ndef zip_stats_results_folders(output_base_path: str, pattern_name: str) -> None:\n    base = Path(output_base_path)\n    base.mkdir(parents=True, exist_ok=True)\n    result_folders = find_result_folders(base, pattern_name)\n    if not result_folders:\n        print(f\"âš ï¸ No folders matching '*dataset' found in '{output_base_path}'.\")\n        return\n    print(f\"ðŸ” Found {len(result_folders)} result folder(s) to zip.\")\n    successful = sum(1 for folder in result_folders if zip_folder(folder, base))\n    print(f\"\\nâœ… DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\")\n\nif __name__ == \"__main__\":\n    try:\n        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n        if not output_root:\n            raise ValueError(\"OUTPUT_PATH not defined\")\n        zip_stats_results_folders(\n            output_base_path=OUTPUT_PATH,\n            pattern_name=\"outputs/FontDiffuser/global_step_50\")\n    except Exception as e:\n        print(f\"âŒ An error occurred: {e}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kTz9WZ9ylBZx","outputId":"ccb61bf4-7551-4269-aee7-b0742fe1517a","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":["âš ï¸ No folders matching '*dataset' found in '/content/'.\n"]}],"execution_count":21}]}