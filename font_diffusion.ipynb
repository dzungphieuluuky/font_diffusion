{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":127.636509,"end_time":"2025-12-30T18:55:25.961447","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-30T18:53:18.324938","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a95a46ef","cell_type":"code","source":"# @title Environment Setup\nimport os\nimport sys\nif 'MPLBACKEND' in os.environ:\n    del os.environ['MPLBACKEND']\n    print(\"MPLBACKEND environment variable cleared.\")\n\n# 2. Clone the repository\n!rm -rf FontDiffusion\n!git clone https://github.com/dzungphieuluuky/FontDiffusion.git","metadata":{"id":"a95a46ef","outputId":"d76d28cd-6292-42bf-fffa-a8c7efb86ed0","papermill":{"duration":12.857369,"end_time":"2025-12-30T18:53:35.066181","exception":false,"start_time":"2025-12-30T18:53:22.208812","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T19:04:18.980683Z","iopub.execute_input":"2025-12-31T19:04:18.981193Z","iopub.status.idle":"2025-12-31T19:04:30.481671Z","shell.execute_reply.started":"2025-12-31T19:04:18.981166Z","shell.execute_reply":"2025-12-31T19:04:30.480949Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"MPLBACKEND environment variable cleared.\nCloning into 'FontDiffusion'...\nremote: Enumerating objects: 20331, done.\u001b[K\nremote: Counting objects: 100% (94/94), done.\u001b[K\nremote: Compressing objects: 100% (70/70), done.\u001b[K\nremote: Total 20331 (delta 53), reused 53 (delta 24), pack-reused 20237 (from 1)\u001b[K\nReceiving objects: 100% (20331/20331), 277.46 MiB | 39.21 MiB/s, done.\nResolving deltas: 100% (785/785), done.\nUpdating files: 100% (137/137), done.\n","output_type":"stream"}],"execution_count":1},{"id":"9cdd8666","cell_type":"code","source":"import os\nimport sys\nfrom IPython import get_ipython\nfrom typing import Optional\n\ndef configure_environment_paths():\n    try:\n        if \"google.colab\" in str(get_ipython()):\n            print(\"‚úÖ Environment: Google Colab\")\n            base_data_path = \"/content/\"\n            base_output_path = \"/content/\"\n            environment_name = \"colab\"\n        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n            print(\"‚úÖ Environment: Kaggle\")\n            base_data_path = \"/kaggle/input/\"\n            base_output_path = \"/kaggle/working/\"\n            environment_name = \"kaggle\"\n        else:\n            print(\"‚ö†Ô∏è Environment: Local/Unknown\")\n            base_data_path = \"./data/\"\n            base_output_path = \"./output/\"\n            environment_name = \"local\"\n    except NameError:\n        print(\"‚ö†Ô∏è Non-interactive session. Using local paths.\")\n        base_data_path = \"./data/\"\n        base_output_path = \"./output/\"\n        environment_name = \"local\"\n    os.makedirs(base_output_path, exist_ok=True)\n    print(f\"üìÇ Data Path: {base_data_path}\")\n    print(f\"üì¶ Output Path: {base_output_path}\")\n    return base_data_path, base_output_path, environment_name\n\ndef load_secret(key_name: str) -> Optional[str]:\n    env = ENV_NAME\n    secret_value = None\n    print(f\"Attempting to load secret '{key_name}' from '{env}' environment...\")\n    try:\n        if env == \"colab\":\n            from google.colab import userdata\n            secret_value = userdata.get(key_name)\n        elif env == \"kaggle\":\n            from kaggle_secrets import UserSecretsClient\n            user_secrets = UserSecretsClient()\n            secret_value = user_secrets.get_secret(key_name)\n        else:\n            secret_value = os.getenv(key_name)\n        if not secret_value:\n            print(f\"‚ö†Ô∏è Secret '{key_name}' not found in the {env} environment.\")\n            return None\n        print(f\"‚úÖ Successfully loaded secret '{key_name}'.\")\n        return secret_value\n    except Exception as e:\n        print(f\"‚ùå An error occurred while loading secret '{key_name}': {e}\")\n        return None\n\ndef print_system_info():\n    print(\"\\nüîß System Information\")\n    print(f\"Python version: {sys.version.split()[0]}\")\n    try:\n        import torch\n        print(f\"PyTorch version: {torch.__version__}\")\n        if torch.cuda.is_available():\n            print(f\"CUDA version: {torch.version.cuda}\")\n            print(f\"GPU count: {torch.cuda.device_count()}\")\n            for i in range(torch.cuda.device_count()):\n                print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n        else:\n            print(\"CUDA not available\")\n    except ImportError:\n        print(\"PyTorch not installed\")\n    finally:\n      !nvidia-smi\n\nINPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()\nis_kaggle = (\"kaggle\" in ENV_NAME)\nis_colab = not is_kaggle\nprint_system_info()\n\nos.environ[\"WANDB_API_KEY\"] = load_secret('WANDB_API_KEY')\nos.environ[\"HF_TOKEN\"] = load_secret('HF_TOKEN')\n\n# Now, these libraries will log in automatically\nimport wandb\nimport huggingface_hub\n\nwandb.login()  # Authenticates silently\nhuggingface_hub.login(token=os.environ[\"HF_TOKEN\"]) \n# Or just rely on HF_TOKEN being in env\nHF_USERNAME = \"dzungpham\"","metadata":{"id":"9cdd8666","outputId":"8834f4e4-fc28-455c-a66c-d15b00de080a","papermill":{"duration":0.019157,"end_time":"2025-12-30T18:53:35.092303","exception":false,"start_time":"2025-12-30T18:53:35.073146","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-31T19:04:30.483040Z","iopub.execute_input":"2025-12-31T19:04:30.483290Z","iopub.status.idle":"2025-12-31T19:04:40.957368Z","shell.execute_reply.started":"2025-12-31T19:04:30.483264Z","shell.execute_reply":"2025-12-31T19:04:40.956437Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Environment: Kaggle\nüìÇ Data Path: /kaggle/input/\nüì¶ Output Path: /kaggle/working/\n\nüîß System Information\nPython version: 3.11.13\nPyTorch version: 2.6.0+cu124\nCUDA version: 12.4\nGPU count: 2\n  GPU 0: Tesla T4\n  GPU 1: Tesla T4\nWed Dec 31 19:04:32 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   40C    P8             12W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   42C    P8             11W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nAttempting to load secret 'WANDB_API_KEY' from 'kaggle' environment...\n‚úÖ Successfully loaded secret 'WANDB_API_KEY'.\nAttempting to load secret 'HF_TOKEN' from 'kaggle' environment...\n‚úÖ Successfully loaded secret 'HF_TOKEN'.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdungngocpham171\u001b[0m (\u001b[33mdungngocpham171-university-of-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\nNote: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n","output_type":"stream"}],"execution_count":2},{"id":"a73b4150","cell_type":"code","source":"!uv pip install --upgrade pip\n# 3. Install PyTorch 1.13\n%cd {OUTPUT_PATH}\n# Force reinstall torch 1.13 to match the model's training environment\n# !uv pip uninstall torch torchvision\n# !uv pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n!uv pip install torch torchvision\n# 4. Install other dependencies\nprint(\"\\n‚¨áÔ∏è Installing Dependencies (Manually fixed)...\")\n# Install xformers compatible with Torch 1.13\n!uv pip install xformers==0.0.16 -q\n\n# Install original dependencies\n!uv pip install transformers==4.33.1 accelerate==0.23.0 diffusers==0.22.0\n!uv pip install gradio==4.8.0 pyyaml pygame opencv-python info-nce-pytorch kornia\n# -----------------------------------------------------------------\n!uv pip install lpips scikit-image pytorch-fid\n!sudo apt-get update && sudo apt-get install dos2unix\n!uv pip install gdown\n!uv pip install wandb\n!uv pip install --upgrade pyarrow datasets\nprint(\"\\n‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\")","metadata":{"id":"a73b4150","outputId":"97db2cec-8e2d-438b-e5f8-38df08b7f59e","papermill":{"duration":61.239828,"end_time":"2025-12-30T18:54:36.338205","exception":false,"start_time":"2025-12-30T18:53:35.098377","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-31T19:04:40.958489Z","iopub.execute_input":"2025-12-31T19:04:40.959087Z","iopub.status.idle":"2025-12-31T19:04:48.962346Z","shell.execute_reply.started":"2025-12-31T19:04:40.959050Z","shell.execute_reply":"2025-12-31T19:04:48.961589Z"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 44ms\u001b[0m\u001b[0m                                           \u001b[0m\n\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 0.23ms\u001b[0m\u001b[0m\n/kaggle/working\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 100ms\u001b[0m\u001b[0m\n\n‚¨áÔ∏è Installing Dependencies (Manually fixed)...\n  \u001b[31m√ó\u001b[0m Failed to build `xformers==0.0.16`\n\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mThe build backend returned an error\n\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mCall to `setuptools.build_meta:__legacy__.build_wheel` failed (exit\n\u001b[31m      \u001b[0mstatus: 1)\n\n\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n\u001b[31m      \u001b[0mError in sitecustomize; set PYTHONVERBOSE for traceback:\n\u001b[31m      \u001b[0mModuleNotFoundError: No module named 'wrapt'\n\u001b[31m      \u001b[0mTraceback (most recent call last):\n\u001b[31m      \u001b[0m  File \"<string>\", line 14, in <module>\n\u001b[31m      \u001b[0m  File\n\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmpPY7wUW/lib/python3.11/site-packages/setuptools/build_meta.py\",\n\u001b[31m      \u001b[0mline 331, in get_requires_for_build_wheel\n\u001b[31m      \u001b[0m    return self._get_build_requires(config_settings, requirements=[])\n\u001b[31m      \u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[31m      \u001b[0m  File\n\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmpPY7wUW/lib/python3.11/site-packages/setuptools/build_meta.py\",\n\u001b[31m      \u001b[0mline 301, in _get_build_requires\n\u001b[31m      \u001b[0m    self.run_setup()\n\u001b[31m      \u001b[0m  File\n\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmpPY7wUW/lib/python3.11/site-packages/setuptools/build_meta.py\",\n\u001b[31m      \u001b[0mline 512, in run_setup\n\u001b[31m      \u001b[0m    super().run_setup(setup_script=setup_script)\n\u001b[31m      \u001b[0m  File\n\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmpPY7wUW/lib/python3.11/site-packages/setuptools/build_meta.py\",\n\u001b[31m      \u001b[0mline 317, in run_setup\n\u001b[31m      \u001b[0m    exec(code, locals())\n\u001b[31m      \u001b[0m  File \"<string>\", line 23, in <module>\n\u001b[31m      \u001b[0mModuleNotFoundError: No module named 'torch'\n\n\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This error likely indicates that `\u001b[36mxformers@0.0.16\u001b[39m` depends\n\u001b[31m      \u001b[0mon `\u001b[36mtorch\u001b[39m`, but doesn't declare it as a build dependency. If\n\u001b[31m      \u001b[0m`\u001b[36mxformers\u001b[39m` is a first-party package, consider adding `\u001b[36mtorch\u001b[39m` to its\n\u001b[31m      \u001b[0m`\u001b[32mbuild-system.requires\u001b[39m`. Otherwise, `\u001b[32muv pip install torch\u001b[39m` into the\n\u001b[31m      \u001b[0menvironment and re-run with `\u001b[32m--no-build-isolation\u001b[39m`.\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m44 packages\u001b[0m \u001b[2min 20ms\u001b[0m\u001b[0m                                         \u001b[0m\n\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m36.0                              \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.2.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m87 packages\u001b[0m \u001b[2min 52ms\u001b[0m\u001b[0m                                         \u001b[0m\n\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 23ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 23ms\u001b[0m\u001b[0m                                 \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe package `typer==0.16.0` does not have an extra named `all`\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 104ms\u001b[0m\u001b[0m\nHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\nHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\nHit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease                     \nHit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease               \nHit:5 http://archive.ubuntu.com/ubuntu jammy InRelease                         \nHit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                 \nHit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nHit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\nHit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\nHit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ndos2unix is already the newest version (7.4.2-2).\n0 upgraded, 0 newly installed, 0 to remove and 192 not upgraded.\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 100ms\u001b[0m\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 102ms\u001b[0m\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m39 packages\u001b[0m \u001b[2min 143ms\u001b[0m\u001b[0m                                        \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 0.56ms\u001b[0m\u001b[0m                                            \n\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 21ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 24ms\u001b[0m\u001b[0m                                \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.2.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.0\u001b[0m\n\n‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\n","output_type":"stream"}],"execution_count":3},{"id":"bd517dfe","cell_type":"code","source":"# KAGGLE CELL #1: Download checkpoint\n\nimport os\nimport sys\nfrom pathlib import Path\nos.chdir(OUTPUT_PATH)\n# Download from Hub\nif not os.path.exists(\"ckpt\") or not list(Path(\"ckpt\").glob(\"*.safetensors\")):\n    print(\"üì• Downloading checkpoint from Hugging Face Hub...\\n\")\n    from huggingface_hub import snapshot_download\n    snapshot_download(\n        repo_id=\"dzungpham/font-diffusion-weights\",\n        local_dir=\"ckpt\",\n        allow_patterns=\"*.safetensors\",\n        force_download=False\n    )\n    print(\"\\n‚úÖ Download complete!\")\nelse:\n    print(\"‚úÖ Checkpoint already downloaded\")\n# Verify\nprint(\"\\nüìÇ Files in ckpt/:\")\nfor file in os.listdir(\"ckpt\"):\n    if file.endswith(\".safetensors\"):\n        size = os.path.getsize(f\"ckpt/{file}\") / (1024**2)\n        print(f\"  ‚úì {file} ({size:.2f} MB)\")","metadata":{"id":"bd517dfe","outputId":"d83605e9-f5dc-4862-d1c9-b138a96ca47a","papermill":{"duration":12.524295,"end_time":"2025-12-30T18:54:48.878013","exception":false,"start_time":"2025-12-30T18:54:36.353718","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-31T19:04:48.964370Z","iopub.execute_input":"2025-12-31T19:04:48.964589Z","iopub.status.idle":"2025-12-31T19:04:54.933617Z","shell.execute_reply.started":"2025-12-31T19:04:48.964566Z","shell.execute_reply":"2025-12-31T19:04:54.932946Z"}},"outputs":[{"name":"stdout","text":"üì• Downloading checkpoint from Hugging Face Hub...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (incomplete total...): 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c5db7e9b61b46c5a7f49c38e136bcc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd8521e93b47483f8bf95ef9db84f2a0"}},"metadata":{}},{"name":"stdout","text":"\n‚úÖ Download complete!\n\nüìÇ Files in ckpt/:\n  ‚úì style_encoder.safetensors (78.58 MB)\n  ‚úì content_encoder.safetensors (4.54 MB)\n  ‚úì unet.safetensors (300.34 MB)\n","output_type":"stream"}],"execution_count":4},{"id":"767e8ea2","cell_type":"code","source":"# @title Unzipping all archived files\nimport os\nimport glob\nfrom zipfile import ZipFile\n\nzip_file_paths = glob.glob(os.path.join(INPUT_PATH, '*.zip'))\n\nif not zip_file_paths:\n    print(f'No .zip files found in {INPUT_PATH}.')\nelse:\n    for zip_file_path in zip_file_paths:\n        if os.path.exists(zip_file_path):\n            print(f'Unzipping {zip_file_path}...')\n            !unzip -o {zip_file_path} -d ./\n            print(f'Unzipping of {zip_file_path} complete.')\n        else:\n            print(f'Error: The file {zip_file_path} was not found (post-glob check).')","metadata":{"id":"767e8ea2","outputId":"20185e27-e772-4823-e6bc-d9bd6d0b39a1","papermill":{"duration":0.023805,"end_time":"2025-12-30T18:54:48.917163","exception":false,"start_time":"2025-12-30T18:54:48.893358","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-31T19:04:54.934315Z","iopub.execute_input":"2025-12-31T19:04:54.934604Z","iopub.status.idle":"2025-12-31T19:04:54.940950Z","shell.execute_reply.started":"2025-12-31T19:04:54.934576Z","shell.execute_reply":"2025-12-31T19:04:54.940359Z"}},"outputs":[{"name":"stdout","text":"No .zip files found in /kaggle/input/.\n","output_type":"stream"}],"execution_count":5},{"id":"51941368","cell_type":"code","source":"import pandas as pd\nimport os\ndef convert_csv_to_chars_txt(input_csv_path: str, output_txt_path: str, column_name: str = 'word'):\n    \"\"\"\n    Reads a CSV file, extracts text from a specified column, and writes each character\n    to a new line in a plain text file.\n    Args:\n        input_csv_path (str): The full path to the input CSV file.\n        output_txt_path (str): The full path for the output text file.\n        column_name (str): The name of the column in the CSV file containing the text.\n    \"\"\"\n    if not os.path.exists(input_csv_path):\n        print(f\"Error: Input CSV file not found at '{input_csv_path}'. Please ensure the file is uploaded.\")\n        return\n    try:\n        df = pd.read_csv(input_csv_path)\n    except Exception as e:\n        print(f\"Error reading CSV file '{input_csv_path}': {e}\")\n        return\n    if column_name not in df.columns:\n        print(f\"Error: Column '{column_name}' not found in the CSV file '{input_csv_path}'.\")\n        return\n    all_characters = []\n    for item in df[column_name].astype(str).dropna().tolist():\n        for char in item:\n            all_characters.append(char)\n    os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)\n    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(all_characters))\n    print(f\"Successfully converted '{input_csv_path}' to '{output_txt_path}', with one character per line.\")\nprint(\"\\n--- Demonstrating function with a dummy CSV file ---\")\ndummy_csv_path = os.path.join(OUTPUT_PATH, \"dummy_data.csv\")\ndummy_output_txt_path = os.path.join(OUTPUT_PATH, \"dummy_chars.txt\")\ndummy_data = {'word': ['hello', 'world', 'python']}\npd.DataFrame(dummy_data).to_csv(dummy_csv_path, index=False)\nprint(f\"Created a dummy CSV file at: {dummy_csv_path}\")\nconvert_csv_to_chars_txt(dummy_csv_path, dummy_output_txt_path)","metadata":{"id":"51941368","outputId":"2a2c352c-968a-4e4d-b4cc-88a02c7eb788","papermill":{"duration":1.62157,"end_time":"2025-12-30T18:54:50.594793","exception":false,"start_time":"2025-12-30T18:54:48.973223","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-31T19:04:54.941703Z","iopub.execute_input":"2025-12-31T19:04:54.941957Z","iopub.status.idle":"2025-12-31T19:04:55.455564Z","shell.execute_reply.started":"2025-12-31T19:04:54.941941Z","shell.execute_reply":"2025-12-31T19:04:55.454971Z"}},"outputs":[{"name":"stdout","text":"\n--- Demonstrating function with a dummy CSV file ---\nCreated a dummy CSV file at: /kaggle/working/dummy_data.csv\nSuccessfully converted '/kaggle/working/dummy_data.csv' to '/kaggle/working/dummy_chars.txt', with one character per line.\n","output_type":"stream"}],"execution_count":6},{"id":"4f4cf20b","cell_type":"code","source":"!ls -larth {OUTPUT_PATH}/ckpt","metadata":{"id":"4f4cf20b","outputId":"335f4192-47e7-451a-e14f-e0bd69fbdfc9","papermill":{"duration":0.140282,"end_time":"2025-12-30T18:54:50.749810","exception":false,"start_time":"2025-12-30T18:54:50.609528","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-31T19:04:55.456311Z","iopub.execute_input":"2025-12-31T19:04:55.456526Z","iopub.status.idle":"2025-12-31T19:04:55.597236Z","shell.execute_reply.started":"2025-12-31T19:04:55.456508Z","shell.execute_reply":"2025-12-31T19:04:55.596339Z"}},"outputs":[{"name":"stdout","text":"total 384M\ndrwxr-xr-x 3 root root 4.0K Dec 31 19:04 .cache\n-rw-r--r-- 1 root root 4.6M Dec 31 19:04 content_encoder.safetensors\n-rw-r--r-- 1 root root  79M Dec 31 19:04 style_encoder.safetensors\n-rw-r--r-- 1 root root 301M Dec 31 19:04 unet.safetensors\ndrwxr-xr-x 3 root root 4.0K Dec 31 19:04 .\ndrwxr-xr-x 5 root root 4.0K Dec 31 19:04 ..\n","output_type":"stream"}],"execution_count":7},{"id":"92cff682","cell_type":"code","source":"%cd {OUTPUT_PATH}\n# ==========================================\n# EXPORT / DOWNLOAD DATASET COMMANDS\n# ==========================================\n\n# Train Split\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/train\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"train\" \\\n  --token HF_TOKEN\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/train_original\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"train_original\" \\\n  --token HF_TOKEN\n# Validation: Unseen Both\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/val\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"val\" \\\n  --token HF_TOKEN","metadata":{"id":"92cff682","papermill":{"duration":0.104394,"end_time":"2025-12-30T18:54:50.869230","exception":false,"start_time":"2025-12-30T18:54:50.764836","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T19:04:55.598284Z","iopub.execute_input":"2025-12-31T19:04:55.598575Z","iopub.status.idle":"2025-12-31T19:05:22.191293Z","shell.execute_reply.started":"2025-12-31T19:04:55.598550Z","shell.execute_reply":"2025-12-31T19:05:22.190468Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n\n============================================================\nEXPORTING DATASET TO DISK\n============================================================\n\nüì• Loading dataset from Hub...\n   Repository: dzungpham/font-diffusion-generated-data\n   Split: train\nREADME.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 733/733 [00:00<00:00, 4.26MB/s]\ndata/train_original-00000-of-00001.parqu(‚Ä¶): 100%|‚ñà| 18.5M/18.5M [00:00<00:00, 2\ndata/train-00000-of-00001.parquet: 100%|‚ñà‚ñà‚ñà| 7.70M/7.70M [00:01<00:00, 4.12MB/s]\ndata/val-00000-of-00001.parquet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 563k/563k [00:00<00:00, 1.04MB/s]\nGenerating train_original split: 100%|‚ñà| 2175/2175 [00:00<00:00, 36325.67 exampl\nGenerating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 912/912 [00:00<00:00, 43443.56 examples/s]\nGenerating val split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:00<00:00, 15255.91 examples/s]\n‚úì Loaded dataset with 912 samples from Hub\n\nExporting images from dataset...\n\nüé® Exporting images...\nExporting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 912/912 [00:02<00:00, 421.70it/s]\n‚úì Exported 76 content images\n‚úì Exported 912 target images\n\nüíæ Saving results_checkpoint.json...\n  ‚úì Saved results_checkpoint.json (912 generations)\n\nüìä Metadata Statistics:\n  Total generations: 912\n  Total characters: 76\n  Total styles: 12\n  Fonts: HAN NOM A\n\n============================================================\n‚úÖ EXPORT COMPLETE!\n============================================================\n\nFiles created:\n  ‚úì my_dataset/train/ContentImage/\n  ‚úì my_dataset/train/TargetImage/\n  ‚úì my_dataset/train/results_checkpoint.json\n\n============================================================\nEXPORTING DATASET TO DISK\n============================================================\n\nüì• Loading dataset from Hub...\n   Repository: dzungpham/font-diffusion-generated-data\n   Split: train_original\n‚úì Loaded dataset with 2175 samples from Hub\n\nExporting images from dataset...\n\nüé® Exporting images...\nExporting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2175/2175 [00:05<00:00, 429.27it/s]\n‚úì Exported 145 content images\n‚úì Exported 2175 target images\n\nüíæ Saving results_checkpoint.json...\n  ‚úì Saved results_checkpoint.json (2175 generations)\n\nüìä Metadata Statistics:\n  Total generations: 2175\n  Total characters: 141\n  Total styles: 15\n  Fonts: HAN NOM A, NomNaTong-Regular\n\n============================================================\n‚úÖ EXPORT COMPLETE!\n============================================================\n\nFiles created:\n  ‚úì my_dataset/train_original/ContentImage/\n  ‚úì my_dataset/train_original/TargetImage/\n  ‚úì my_dataset/train_original/results_checkpoint.json\n\n============================================================\nEXPORTING DATASET TO DISK\n============================================================\n\nüì• Loading dataset from Hub...\n   Repository: dzungpham/font-diffusion-generated-data\n   Split: val\n‚úì Loaded dataset with 57 samples from Hub\n\nExporting images from dataset...\n\nüé® Exporting images...\nExporting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:00<00:00, 339.04it/s]\n‚úì Exported 19 content images\n‚úì Exported 57 target images\n\nüíæ Saving results_checkpoint.json...\n  ‚úì Saved results_checkpoint.json (57 generations)\n\nüìä Metadata Statistics:\n  Total generations: 57\n  Total characters: 19\n  Total styles: 3\n  Fonts: HAN NOM A\n\n============================================================\n‚úÖ EXPORT COMPLETE!\n============================================================\n\nFiles created:\n  ‚úì my_dataset/val/ContentImage/\n  ‚úì my_dataset/val/TargetImage/\n  ‚úì my_dataset/val/results_checkpoint.json\n","output_type":"stream"}],"execution_count":8},{"id":"29deed1d","cell_type":"code","source":"if is_colab:\n  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\nelse:\n  !uv pip install --upgrade \"huggingface-hub==0.25.2\" \"protobuf<5.0.0\" \"numpy<2.0.0\"\n%cd {OUTPUT_PATH}\n!python FontDiffusion/sample_batch.py \\\n    --characters \"FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\" \\\n    --style_images \"FontDiffusion/styles_images\" \\\n    --ckpt_dir \"ckpt/\" \\\n    --ttf_path \"FontDiffusion/fonts\" \\\n    --output_dir \"my_dataset/train_original\" \\\n    --num_inference_steps 20 \\\n    --guidance_scale 7.5 \\\n    --start_line 1 \\\n    --end_line 100 \\\n    --batch_size 24 \\\n    --save_interval 1 \\\n    --channels_last \\\n    --seed 42 \\\n    --compile \\\n    --enable_xformers","metadata":{"execution":{"iopub.status.busy":"2025-12-31T19:07:59.961809Z","iopub.execute_input":"2025-12-31T19:07:59.962494Z","iopub.status.idle":"2025-12-31T19:10:22.894909Z","shell.execute_reply.started":"2025-12-31T19:07:59.962460Z","shell.execute_reply":"2025-12-31T19:10:22.894188Z"},"id":"29deed1d","outputId":"749b50d0-75e3-4d36-e509-919188feb64c","papermill":{"duration":10.53661,"end_time":"2025-12-30T18:55:01.421093","exception":false,"start_time":"2025-12-30T18:54:50.884483","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m14 packages\u001b[0m \u001b[2min 78ms\u001b[0m\u001b[0m                                         \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 91ms\u001b[0m\u001b[0m                                              \n\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 35ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 25ms\u001b[0m\u001b[0m                                \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==4.25.8\u001b[0m\n/kaggle/working\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n2025-12-31 19:08:05.168759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767208085.190326    3049 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767208085.199633    3049 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\npygame 2.6.1 (SDL 2.28.4, Python 3.11.13)\nHello from the pygame community. https://www.pygame.org/contribute.html\n\n============================================================\nFONTDIFFUSER SYNTHESIS DATA GENERATION MAGIC\n============================================================\nüìñ Loading characters from file: FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\n   Lines 1 to 100 (total file: 10174 lines)\n   Processing 100 lines...\nüìñ Reading character file: 100%|\u001b[36m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\u001b[0m| 100/100 [00:00<00:00, 1205259.77line/s]\u001b[0m\n‚úÖ Successfully loaded 100 single characters.\n\nüìÇ Loading 15 style images from directory...\n‚úì Verifying style images: 100%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\u001b[0m| 15/15 [00:00]\u001b[0m\n\nInitializing font manager...\n\n============================================================\nLoading 15 fonts from directory...\n============================================================\nerror: XDG_RUNTIME_DIR not set in the environment.\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: HAN NOM A\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: HAN NOM B\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: Han-Nom Kai 1.00\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: Han-Nom-Khai-Regular-300623\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: Han-nom Minh 1.42\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: HanaMinA\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: HanaMinA\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: HanaMinB\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: HanaMinB\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: HanaMinC\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: NomNaTong-Regular\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: NomNaTong-Regular\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: NomNaTong-Regular2\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: NomNaTongLight\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded: NomNaTongLight2\n============================================================\nSuccessfully loaded 12 fonts\n\n\nüìä Configuration:\n  Dataset split: train_original\n  Characters: 100 (lines 1-100)\n  Styles: 15\n  Output Directory: my_dataset/train_original\n  Checkpoint Directory: ckpt/\n  Device: cuda\n  Batch Size: 24\nWill look for results checkpoint at my_dataset/train_original/results_checkpoint.json\n‚úì Loaded checkpoint: 2175 unique generations\n  Total raw entries: 2175\n\nLoading FontDiffuser pipeline...\nLoading FontDiffuser pipeline...\nLoad the down block  DownBlock2D\nLoad the down block  MCADownBlock2D\nThe style_attention cross attention dim in Down Block 1 layer is 1024\nThe style_attention cross attention dim in Down Block 2 layer is 1024\nLoad the down block  MCADownBlock2D\nThe style_attention cross attention dim in Down Block 1 layer is 1024\nThe style_attention cross attention dim in Down Block 2 layer is 1024\nLoad the down block  DownBlock2D\nLoad the up block  UpBlock2D\nLoad the up block  StyleRSIUpBlock2D\nLoad the up block  StyleRSIUpBlock2D\nLoad the up block  UpBlock2D\nParam count for Ds initialized parameters: 20591296\nGet CG-GAN Style Encoder!\nParam count for Ds initialized parameters: 1187008\nGet CG-GAN Content Encoder!\n‚úì Loaded model state_dict successfully\nConverting to channels-last memory format...\n‚úì Model moved to device\n‚úì Loaded training DDPM scheduler successfully\n‚úì Loaded DPM-Solver pipeline successfully\nüîß Compiling model components with torch.compile...\n/kaggle/working/FontDiffusion/sample_batch.py:1507: FutureWarning: Accessing config attribute `unet` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'unet' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.unet'.\n  if hasattr(pipe.model, \"unet\"):\n/kaggle/working/FontDiffusion/sample_batch.py:1508: FutureWarning: Accessing config attribute `unet` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'unet' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.unet'.\n  pipe.model.unet = torch.compile(pipe.model.unet)\n/kaggle/working/FontDiffusion/sample_batch.py:1509: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.style_encoder'.\n  if hasattr(pipe.model, \"style_encoder\"):\n/kaggle/working/FontDiffusion/sample_batch.py:1510: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.style_encoder'.\n  pipe.model.style_encoder = torch.compile(pipe.model.style_encoder)\n/kaggle/working/FontDiffusion/sample_batch.py:1511: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n  if hasattr(pipe.model, \"content_encoder\"):\n/kaggle/working/FontDiffusion/sample_batch.py:1513: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n  pipe.model.content_encoder\n‚úì Compilation complete.\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 233M/233M [00:01<00:00, 180MB/s]\nLoading model from: /usr/local/lib/python3.11/dist-packages/lpips/weights/v0.1/alex.pth\n\n======================================================================\n                      GENERATING CONTENT IMAGES                       \n======================================================================\n\n============================================================\nGenerating Content Images\nUsing 12 fonts\nCharacters: 100\n============================================================\nüì∏ Generating content images:   0%|\u001b[34m                            \u001b[0m| 0/100 [00:00<?]\u001b[0m^C\nüì∏ Generating content images:   0%|\u001b[34m                            \u001b[0m| 0/100 [02:03<?]\u001b[0m\n\n\n‚ö† Generation interrupted by user!\nüíæ Saving emergency checkpoint...\n","output_type":"stream"}],"execution_count":11},{"id":"f9250a14","cell_type":"code","source":"!python FontDiffusion/create_validation_split.py \\\n  --data_root my_dataset \\\n  --val_ratio 0.2 \\\n  --seed 42","metadata":{"execution":{"iopub.status.busy":"2025-12-31T19:10:27.317323Z","iopub.execute_input":"2025-12-31T19:10:27.317625Z","iopub.status.idle":"2025-12-31T19:10:27.936512Z","shell.execute_reply.started":"2025-12-31T19:10:27.317596Z","shell.execute_reply":"2025-12-31T19:10:27.935806Z"},"id":"f9250a14","outputId":"0f834d09-da00-4aa4-f486-6e70981b4137","papermill":{"duration":0.236541,"end_time":"2025-12-30T18:55:01.673705","exception":false,"start_time":"2025-12-30T18:55:01.437164","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"\n============================================================\nFONTDIFFUSION SPLIT CREATOR (SIMPLIFIED)\n============================================================\n‚úì Using source directory: my_dataset/train_original\n\n============================================================\nCREATING DATA SPLITS\n============================================================\n\n============================================================\nANALYZING TRAINING DATA\n============================================================\n\nüîç Scanning target images...\nStyles: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:00<00:00, 1414.22style/s]\n\n‚úì Initial scan:\n  Styles: 15\n  Characters: 141\n  Valid (char, style) pairs: 2115\n\nüîç Auto-detecting font parameter from existing files...\n  üìÇ Found 15 fonts in fonts/ directory\n  ‚úì Detected font parameter: 'HAN NOM A'\n    Verified with: U+34E4_„ì§_20242aa0.png\n\nüîç Validating content images (using font='HAN NOM A')...\nChecking content: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [00:00<00:00, 84934.20char/s]\n\n‚ö†Ô∏è  WARNING: 46/141 characters missing content images:\n  Examples: ['†Ñ©', '†ë¨', '†§Ü', '†¶≥', '†≥í', '†¥ù', '†∫•', '°ä∞', '°óâ', '°•µ']...\n  These characters will be excluded from splits\n\n‚úÖ After validation:\n  Characters with content images: 95\n  Valid (char, style) pairs: 1425\n  Content images found: 95\n\nüìä Character distribution per style:\n  ref_1: 95 characters\n  ref_2: 95 characters\n  ref_3: 95 characters\n  ref_4: 95 characters\n  ref_7: 95 characters\n  ref_8: 95 characters\n  ref_hanh: 95 characters\n  ref_hanhthu_1: 95 characters\n  ref_hanhthu_2: 95 characters\n  ref_khai: 95 characters\n  ... and 5 more styles\n\n============================================================\nCREATING TRAIN/VAL SPLITS\n============================================================\n\nüìä Split Statistics:\n  Styles: 12 train + 3 val\n  Chars:  76 train + 19 val\n\nüìã Splits:\n\n  train:\n    Description: Training data (seen styles + seen characters)\n    Styles: 12 (['ref_lethu_1', 'ref_thao', 'ref_lethu_2']...)\n    Chars: 76 (['Ëêì', '‰ã•', 'Ëü≥', 'Áúì', '„•¨']...)\n\n  val:\n    Description: Validation data (unseen styles + unseen characters)\n    Styles: 3 (['ref_2', 'ref_1', 'ref_le']...)\n    Chars: 19 (['Ê©∑', '„≤∏', '„ñ°', '„∫ß', '„ùπ']...)\n\nüìÅ Creating train split:\n  Valid pairs: 912\n  Unique chars: 76\n  Unique styles: 12\n\n  üì• Copying content images (font='HAN NOM A')...\n  üì• Copying target images (font='HAN NOM A')...                                \n                                                                                \n  üîç Validating split...\n  ‚úì All 912 targets have matching content\n\n  üìä Summary:\n    Content copied: 76\n    Target copied: 912\n    Skipped: 0\n  ‚úì Copied 76 content + 912 target (skipped 0)\n\n  üìã Filtering results_checkpoint.json for train...\n    ‚úì Saved checkpoint: 960/2175 generations                                    \n    üìÑ my_dataset/train/results_checkpoint.json\n\nüìÅ Creating val split:\n  Valid pairs: 57\n  Unique chars: 19\n  Unique styles: 3\n\n  üì• Copying content images (font='HAN NOM A')...\n  üì• Copying target images (font='HAN NOM A')...                                \n                                                                                \n  üîç Validating split...\n  ‚úì All 57 targets have matching content\n\n  üìä Summary:\n    Content copied: 19\n    Target copied: 57\n    Skipped: 0\n  ‚úì Copied 19 content + 57 target (skipped 0)\n\n  üìã Filtering results_checkpoint.json for val...\n    ‚úì Saved checkpoint: 57/2175 generations                                     \n    üìÑ my_dataset/val/results_checkpoint.json\n\n‚úì Saved split metadata to my_dataset/split_info.json\n\n============================================================\n‚úì SPLIT CREATION COMPLETE\n============================================================\n\n‚úÖ Created directories:\n  üìÅ train/ - Training data (seen styles + seen chars)\n    ‚îú‚îÄ‚îÄ ContentImage/\n    ‚îú‚îÄ‚îÄ TargetImage/\n    ‚îî‚îÄ‚îÄ results_checkpoint.json\n  üìÅ val/ - Validation data (unseen styles + unseen chars)\n    ‚îú‚îÄ‚îÄ ContentImage/\n    ‚îú‚îÄ‚îÄ TargetImage/\n    ‚îî‚îÄ‚îÄ results_checkpoint.json\n\nüí° Each folder guarantees:\n  ‚úì Every target image has matching content image\n  ‚úì Parsed by codepoint only (no reliance on safe char)\n  ‚úì Filtered results_checkpoint.json with split-specific data\n","output_type":"stream"}],"execution_count":12},{"id":"79508d80-fac1-4318-9174-a32613a557e3","cell_type":"code","source":"!uv pip install --upgrade pyarrow datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T19:10:46.373635Z","iopub.execute_input":"2025-12-31T19:10:46.373957Z","iopub.status.idle":"2025-12-31T19:10:47.127435Z","shell.execute_reply.started":"2025-12-31T19:10:46.373927Z","shell.execute_reply":"2025-12-31T19:10:47.126608Z"},"id":"79508d80-fac1-4318-9174-a32613a557e3"},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m39 packages\u001b[0m \u001b[2min 158ms\u001b[0m\u001b[0m                                        \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m                                               \n\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 24ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 314ms\u001b[0m\u001b[0m                               \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.12.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.10.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.25.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.2.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.0\u001b[0m\n","output_type":"stream"}],"execution_count":13},{"id":"vRL8QovYCvLY","cell_type":"code","source":"!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/train_original\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"train_original\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# Train Split\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/train\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"train\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# Train Split\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/val\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"val\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n","metadata":{"id":"vRL8QovYCvLY","outputId":"08301c52-4ae1-4268-c516-2ff8bd834783","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"outputs":[],"execution_count":null},{"id":"a87caab2","cell_type":"code","source":"import torch, gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"id":"a87caab2","papermill":{"duration":1.992585,"end_time":"2025-12-30T18:55:24.769269","exception":false,"start_time":"2025-12-30T18:55:22.776684","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"267634e8","cell_type":"code","source":"# TRAINING PHASE 1\nif is_colab:\n  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\nelse:\n  !uv pip install --upgrade \"huggingface-hub==0.25.2\"\nimport wandb\n\n# Load your Weights & Biases API key from a secure store\nwandb_key = load_secret(\"WANDB_API_KEY\")\nwandb.login(key=wandb_key)\n\n# Run the training script with the corrected flag syntax\n!accelerate launch FontDiffusion/my_train.py \\\n    --seed=123 \\\n    --experience_name=FontDiffuser_training_phase_1 \\\n    --data_root=my_dataset \\\n    --output_dir=outputs/FontDiffuser \\\n    --report_to=wandb \\\n    --resolution=96 \\\n    --style_image_size=96 \\\n    --content_image_size=96 \\\n    --content_encoder_downsample_size=3 \\\n    --channel_attn=True \\\n    --content_start_channel=64 \\\n    --style_start_channel=64 \\\n    --train_batch_size=8 \\\n    --perceptual_coefficient=0.03 \\\n    --offset_coefficient=0.7 \\\n    --max_train_steps=200 \\\n    --ckpt_interval=100 \\\n    --val_interval=200 \\\n    --gradient_accumulation_steps=1 \\\n    --log_interval=50 \\\n    --learning_rate=1e-4 \\\n    --lr_scheduler=linear \\\n    --lr_warmup_steps=10000 \\\n    --drop_prob=0.1 \\\n    --mixed_precision=no","metadata":{"id":"267634e8","papermill":{"duration":0.021927,"end_time":"2025-12-30T18:55:24.807644","exception":false,"start_time":"2025-12-30T18:55:24.785717","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"cb2a3326-6bfa-4fba-bbcc-e2e7be996c7f","cell_type":"code","source":"!ls -lr outputs/FontDiffuser","metadata":{"trusted":true,"id":"cb2a3326-6bfa-4fba-bbcc-e2e7be996c7f"},"outputs":[],"execution_count":null},{"id":"97f8136e","cell_type":"code","source":"# TRAINING PHASE 2\n!wandb login\n!python FontDiffusion/my_train.py \\\n    --seed=123 \\\n    --experience_name=\"FontDiffuser_training_phase_2\" \\\n    --data_root=\"my_dataset\" \\\n    --output_dir=\"outputs/FontDiffuser\" \\\n    --report_to=\"wandb\" \\\n    --phase_2 \\\n    --phase_1_ckpt_dir=\"outputs/FontDiffuser/global_step_2000\" \\\n    --scr_ckpt_path=\"ckpt/scr_210000.pth\" \\\n    --sc_coefficient=0.05 \\\n    --num_neg=13 \\\n    --resolution=96 \\\n    --style_image_size=96 \\\n    --content_image_size=96 \\\n    --content_encoder_downsample_size=3 \\\n    --channel_attn=True \\\n    --content_start_channel=64 \\\n    --style_start_channel=64 \\\n    --train_batch_size=8 \\\n    --perceptual_coefficient=0.03 \\\n    --offset_coefficient=0.4 \\\n    --max_train_steps=100 \\\n    --ckpt_interval=50 \\\n    --gradient_accumulation_steps=2 \\\n    --log_interval=50 \\\n    --learning_rate=1e-5 \\\n    --lr_scheduler=\"constant\" \\\n    --lr_warmup_steps=1000 \\\n    --drop_prob=0.1 \\\n    --mixed_precision=\"no\"\n","metadata":{"id":"97f8136e","papermill":{"duration":0.022471,"end_time":"2025-12-30T18:55:24.845778","exception":false,"start_time":"2025-12-30T18:55:24.823307","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"88c45e2f","cell_type":"code","source":"!python FontDiffusion/pth2safetensors.py \\\n    --weights_dir \"ckpt\" \\\n    --repo_id \"dzungpham/font-diffusion-weights\" \\\n    --token \"{HF_TOKEN}\"","metadata":{"id":"88c45e2f","papermill":{"duration":0.217876,"end_time":"2025-12-30T18:55:25.079820","exception":false,"start_time":"2025-12-30T18:55:24.861944","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"5868b20b","cell_type":"code","source":"import os\nimport zipfile\nfrom pathlib import Path\nfrom typing import List\ndef find_result_folders(base_path: Path, pattern_name: str) -> List[Path]:\n    return [p for p in base_path.glob(pattern_name) if p.is_dir()]\n\ndef zip_folder(folder_path: Path, output_base_path: Path) -> bool:\n    folder_name = folder_path.name\n    zip_path = output_base_path / f\"{folder_name}.zip\"\n    try:\n        print(f\"   -> Zipping folder: {folder_name}...\")\n        with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zipf:\n            for file_path in folder_path.rglob(\"*\"):\n                if file_path.is_file():\n                    arcname = file_path.relative_to(folder_path.parent)\n                    zipf.write(file_path, arcname)\n        print(f\"   ‚úÖ Created ZIP: {zip_path.name}\")\n        return True\n    except Exception as exc:\n        print(f\"   ‚ùå Failed to zip {folder_name}: {exc}\")\n        return False\n\ndef zip_stats_results_folders(output_base_path: str, pattern_name: str) -> None:\n    base = Path(output_base_path)\n    base.mkdir(parents=True, exist_ok=True)\n    result_folders = find_result_folders(base, pattern_name)\n    if not result_folders:\n        print(f\"‚ö†Ô∏è No folders matching '*dataset' found in '{output_base_path}'.\")\n        return\n    print(f\"üîç Found {len(result_folders)} result folder(s) to zip.\")\n    successful = sum(1 for folder in result_folders if zip_folder(folder, base))\n    print(f\"\\n‚úÖ DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\")\n\nif __name__ == \"__main__\":\n    try:\n        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n        if not output_root:\n            raise ValueError(\"OUTPUT_PATH not defined\")\n        zip_stats_results_folders(\n            output_base_path=OUTPUT_PATH,\n            pattern_name=\"my_dataset\")\n    except Exception as e:\n        print(f\"‚ùå An error occurred: {e}\")","metadata":{"id":"5868b20b","papermill":{"duration":0.031197,"end_time":"2025-12-30T18:55:25.126961","exception":false,"start_time":"2025-12-30T18:55:25.095764","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}