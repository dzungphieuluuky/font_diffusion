{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31192,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"papermill":{"default_parameters":{},"duration":127.636509,"end_time":"2025-12-30T18:55:25.961447","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-30T18:53:18.324938","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a95a46ef","cell_type":"code","source":"# @title Environment Setup\nimport os\nimport sys\nif 'MPLBACKEND' in os.environ:\n    del os.environ['MPLBACKEND']\n    print(\"MPLBACKEND environment variable cleared.\")\n\n# 2. Clone the repository\n!rm -rf FontDiffusion\n!git clone https://github.com/dzungphieuluuky/FontDiffusion.git","metadata":{"execution":{"iopub.status.busy":"2025-12-31T10:33:43.578094Z","iopub.execute_input":"2025-12-31T10:33:43.578371Z","iopub.status.idle":"2025-12-31T10:33:55.131369Z","shell.execute_reply.started":"2025-12-31T10:33:43.578348Z","shell.execute_reply":"2025-12-31T10:33:55.130648Z"},"id":"BWFvN9XJxf9K","outputId":"1ca6b669-956b-493b-e7ce-6402053d5585","papermill":{"duration":12.857369,"end_time":"2025-12-30T18:53:35.066181","exception":false,"start_time":"2025-12-30T18:53:22.208812","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"MPLBACKEND environment variable cleared.\nCloning into 'FontDiffusion'...\nremote: Enumerating objects: 20234, done.\u001b[K\nremote: Counting objects: 100% (5046/5046), done.\u001b[K\nremote: Compressing objects: 100% (4988/4988), done.\u001b[K\nremote: Total 20234 (delta 126), reused 4927 (delta 56), pack-reused 15188 (from 3)\u001b[K\nReceiving objects: 100% (20234/20234), 277.39 MiB | 38.77 MiB/s, done.\nResolving deltas: 100% (689/689), done.\nUpdating files: 100% (137/137), done.\n","output_type":"stream"}],"execution_count":1},{"id":"9cdd8666","cell_type":"code","source":"import os\nfrom IPython import get_ipython\nfrom typing import Optional\n\ndef configure_environment_paths():\n    \"\"\"Detect environment and configure paths\"\"\"\n    try:\n        if \"google.colab\" in str(get_ipython()):\n            print(\"‚úÖ Environment: Google Colab\")\n            base_data_path = \"/content/\"\n            base_output_path = \"/content/\"\n            environment_name = \"colab\"\n        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n            print(\"‚úÖ Environment: Kaggle\")\n            base_data_path = \"/kaggle/input/\"\n            base_output_path = \"/kaggle/working/\"\n            environment_name = \"kaggle\"\n        else:\n            print(\"‚ö†Ô∏è Environment: Local/Unknown\")\n            base_data_path = \"./data/\"\n            base_output_path = \"./output/\"\n            environment_name = \"local\"\n    except NameError:\n        print(\"‚ö†Ô∏è Non-interactive session. Using local paths.\")\n        base_data_path = \"./data/\"\n        base_output_path = \"./output/\"\n        environment_name = \"local\"\n    os.makedirs(base_output_path, exist_ok=True)\n    print(f\"üìÇ Data Path: {base_data_path}\")\n    print(f\"üì¶ Output Path: {base_output_path}\")\n    return base_data_path, base_output_path, environment_name\ndef load_secret(key_name: str) -> Optional[str]:\n    \"\"\"\n    Loads a secret key from the appropriate environment (Colab, Kaggle, or local env vars).\n    Args:\n        key_name (str): The name of the secret key to load (e.g., \"WANDB_API_KEY\", \"HF_TOKEN\").\n    Returns:\n        Optional[str]: The secret key value if found, otherwise None.\n    \"\"\"\n    env = ENV_NAME\n    secret_value = None\n    print(f\"Attempting to load secret '{key_name}' from '{env}' environment...\")\n    try:\n        if env == \"colab\":\n            from google.colab import userdata\n            secret_value = userdata.get(key_name)\n        elif env == \"kaggle\":\n            from kaggle_secrets import UserSecretsClient\n            user_secrets = UserSecretsClient()\n            secret_value = user_secrets.get_secret(key_name)\n        else: # Local environment\n            secret_value = os.getenv(key_name)\n        if not secret_value:\n            print(f\"‚ö†Ô∏è Secret '{key_name}' not found in the {env} environment.\")\n            return None\n        print(f\"‚úÖ Successfully loaded secret '{key_name}'.\")\n        return secret_value\n    except Exception as e:\n        print(f\"‚ùå An error occurred while loading secret '{key_name}': {e}\")\n        return None\nINPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()","metadata":{"execution":{"iopub.status.busy":"2025-12-31T10:33:55.132965Z","iopub.execute_input":"2025-12-31T10:33:55.133198Z","iopub.status.idle":"2025-12-31T10:33:55.142752Z","shell.execute_reply.started":"2025-12-31T10:33:55.133174Z","shell.execute_reply":"2025-12-31T10:33:55.142039Z"},"id":"sxdyquWfaqdm","outputId":"f4738958-8ecc-4e48-bfda-9a798d92165f","papermill":{"duration":0.019157,"end_time":"2025-12-30T18:53:35.092303","exception":false,"start_time":"2025-12-30T18:53:35.073146","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"‚úÖ Environment: Kaggle\nüìÇ Data Path: /kaggle/input/\nüì¶ Output Path: /kaggle/working/\n","output_type":"stream"}],"execution_count":2},{"id":"a73b4150","cell_type":"code","source":"!uv pip install --upgrade pip\n!uv pip install -r FontDiffusion/my_requirements.txt\n# 3. Install PyTorch 1.13\n%cd {OUTPUT_PATH}\n# Force reinstall torch 1.13 to match the model's training environment\n# !uv pip uninstall torch torchvision\n# !uv pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n!uv pip install torch torchvision\n# 4. Install other dependencies\nprint(\"\\n‚¨áÔ∏è Installing Dependencies (Manually fixed)...\")\n# Install xformers compatible with Torch 1.13\n!uv pip install xformers==0.0.16 -q\n\n# Install original dependencies\n!uv pip install transformers==4.33.1 accelerate==0.23.0 diffusers==0.22.0\n!uv pip install gradio==4.8.0 pyyaml pygame opencv-python info-nce-pytorch kornia\n# -----------------------------------------------------------------\n!uv pip install lpips scikit-image pytorch-fid\n!sudo apt-get update && sudo apt-get install dos2unix\n!uv pip install gdown\n!uv pip install wandb\nprint(\"\\n‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\")","metadata":{"execution":{"iopub.status.busy":"2025-12-31T10:33:55.143575Z","iopub.execute_input":"2025-12-31T10:33:55.143811Z","iopub.status.idle":"2025-12-31T10:34:59.133376Z","shell.execute_reply.started":"2025-12-31T10:33:55.143788Z","shell.execute_reply":"2025-12-31T10:34:59.132596Z"},"id":"ET_mqyek9bwj","outputId":"aae81910-51d7-4409-b8d0-f862b1ea1fe7","papermill":{"duration":61.239828,"end_time":"2025-12-30T18:54:36.338205","exception":false,"start_time":"2025-12-30T18:53:35.098377","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 178ms\u001b[0m\u001b[0m                                          \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 126ms\u001b[0m\u001b[0m                                              \n\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 235ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 18ms\u001b[0m\u001b[0m                                 \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpip\u001b[0m\u001b[2m==24.1.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpip\u001b[0m\u001b[2m==25.3\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K  \u001b[31m√ó\u001b[0m No solution found when resolving dependencies:                                  \u001b[0m\n\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mBecause you require importlib-metadata==4.6.4 and\n\u001b[31m      \u001b[0mimportlib-metadata==8.0.0, we can conclude that your requirements are\n\u001b[31m      \u001b[0munsatisfiable.\n/kaggle/working\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m38 packages\u001b[0m \u001b[2min 112ms\u001b[0m\u001b[0m                                        \u001b[0m\n\u001b[2mUninstalled \u001b[1m10 packages\u001b[0m \u001b[2min 45ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m10 packages\u001b[0m \u001b[2min 19.30s\u001b[0m\u001b[0m.0.70                        \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.5.3.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.3.61\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.6.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.3.83\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.1.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n\n‚¨áÔ∏è Installing Dependencies (Manually fixed)...\n  \u001b[31m√ó\u001b[0m Failed to build `xformers==0.0.16`\n\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mThe build backend returned an error\n\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mCall to `setuptools.build_meta:__legacy__.build_wheel` failed (exit\n\u001b[31m      \u001b[0mstatus: 1)\n\n\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n\u001b[31m      \u001b[0mError in sitecustomize; set PYTHONVERBOSE for traceback:\n\u001b[31m      \u001b[0mModuleNotFoundError: No module named 'wrapt'\n\u001b[31m      \u001b[0mTraceback (most recent call last):\n\u001b[31m      \u001b[0m  File \"<string>\", line 14, in <module>\n\u001b[31m      \u001b[0m  File\n\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmpI5JpWQ/lib/python3.11/site-packages/setuptools/build_meta.py\",\n\u001b[31m      \u001b[0mline 331, in get_requires_for_build_wheel\n\u001b[31m      \u001b[0m    return self._get_build_requires(config_settings, requirements=[])\n\u001b[31m      \u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[31m      \u001b[0m  File\n\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmpI5JpWQ/lib/python3.11/site-packages/setuptools/build_meta.py\",\n\u001b[31m      \u001b[0mline 301, in _get_build_requires\n\u001b[31m      \u001b[0m    self.run_setup()\n\u001b[31m      \u001b[0m  File\n\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmpI5JpWQ/lib/python3.11/site-packages/setuptools/build_meta.py\",\n\u001b[31m      \u001b[0mline 512, in run_setup\n\u001b[31m      \u001b[0m    super().run_setup(setup_script=setup_script)\n\u001b[31m      \u001b[0m  File\n\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmpI5JpWQ/lib/python3.11/site-packages/setuptools/build_meta.py\",\n\u001b[31m      \u001b[0mline 317, in run_setup\n\u001b[31m      \u001b[0m    exec(code, locals())\n\u001b[31m      \u001b[0m  File \"<string>\", line 23, in <module>\n\u001b[31m      \u001b[0mModuleNotFoundError: No module named 'torch'\n\n\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This error likely indicates that `\u001b[36mxformers@0.0.16\u001b[39m` depends\n\u001b[31m      \u001b[0mon `\u001b[36mtorch\u001b[39m`, but doesn't declare it as a build dependency. If\n\u001b[31m      \u001b[0m`\u001b[36mxformers\u001b[39m` is a first-party package, consider adding `\u001b[36mtorch\u001b[39m` to its\n\u001b[31m      \u001b[0m`\u001b[32mbuild-system.requires\u001b[39m`. Otherwise, `\u001b[32muv pip install torch\u001b[39m` into the\n\u001b[31m      \u001b[0menvironment and re-run with `\u001b[32m--no-build-isolation\u001b[39m`.\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m56 packages\u001b[0m \u001b[2min 133ms\u001b[0m\u001b[0m                                        \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m4 packages\u001b[0m \u001b[2min 462ms\u001b[0m\u001b[0m                                             \n\u001b[2mUninstalled \u001b[1m4 packages\u001b[0m \u001b[2min 590ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 41ms\u001b[0m\u001b[0m                                \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.9.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==0.23.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mdiffusers\u001b[0m\u001b[2m==0.34.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdiffusers\u001b[0m\u001b[2m==0.22.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.13.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.53.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.33.1\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m100 packages\u001b[0m \u001b[2min 161ms\u001b[0m\u001b[0m                                       \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m8 packages\u001b[0m \u001b[2min 1.07s\u001b[0m\u001b[0m                                             \n\u001b[2mUninstalled \u001b[1m7 packages\u001b[0m \u001b[2min 377ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m8 packages\u001b[0m \u001b[2min 79ms\u001b[0m\u001b[0m                                \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mgradio\u001b[0m\u001b[2m==5.38.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgradio\u001b[0m\u001b[2m==4.8.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mgradio-client\u001b[0m\u001b[2m==1.11.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgradio-client\u001b[0m\u001b[2m==0.7.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1minfo-nce-pytorch\u001b[0m\u001b[2m==0.1.4\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==2.1.5\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mopencv-python\u001b[0m\u001b[2m==4.12.0.88\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopencv-python\u001b[0m\u001b[2m==4.11.0.86\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==10.4.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtomlkit\u001b[0m\u001b[2m==0.13.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtomlkit\u001b[0m\u001b[2m==0.12.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==11.0.3\u001b[0m\n\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe package `typer==0.16.0` does not have an extra named `all`\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m47 packages\u001b[0m \u001b[2min 58ms\u001b[0m\u001b[0m                                         \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 26ms\u001b[0m\u001b[0m                                              \n\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m                                 \u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlpips\u001b[0m\u001b[2m==0.1.4\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpytorch-fid\u001b[0m\u001b[2m==0.3.0\u001b[0m\nGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\nGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nGet:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]           \nHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease                         \nGet:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.6 kB]\nGet:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \nGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,225 kB]\nGet:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \nGet:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\nGet:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,860 kB]\nHit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\nHit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease   \nGet:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,573 kB]\nGet:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]     \nGet:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\nGet:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,287 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,966 kB]\nGet:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\nGet:20 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\nGet:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\nGet:22 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\nGet:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\nGet:24 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,633 kB]\nFetched 38.5 MB in 3s (11.7 MB/s)                              \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  dos2unix\n0 upgraded, 1 newly installed, 0 to remove and 192 not upgraded.\nNeed to get 384 kB of archives.\nAfter this operation, 1,367 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dos2unix amd64 7.4.2-2 [384 kB]\nFetched 384 kB in 1s (494 kB/s)   \ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\ndebconf: falling back to frontend: Readline\nSelecting previously unselected package dos2unix.\n(Reading database ... 128639 files and directories currently installed.)\nPreparing to unpack .../dos2unix_7.4.2-2_amd64.deb ...\nUnpacking dos2unix (7.4.2-2) ...\nSetting up dos2unix (7.4.2-2) ...\nProcessing triggers for man-db (2.10.2-1) ...\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 106ms\u001b[0m\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 106ms\u001b[0m\u001b[0m\n\n‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\n","output_type":"stream"}],"execution_count":3},{"id":"bd517dfe","cell_type":"code","source":"# KAGGLE CELL #1: Download checkpoint\n\nimport os\nimport sys\nfrom pathlib import Path\nos.chdir(OUTPUT_PATH)\n# Download from Hub\nif not os.path.exists(\"ckpt\") or not list(Path(\"ckpt\").glob(\"*.safetensors\")):\n    print(\"üì• Downloading checkpoint from Hugging Face Hub...\\n\")\n    from huggingface_hub import snapshot_download\n    snapshot_download(\n        repo_id=\"dzungpham/font-diffusion-weights\",\n        local_dir=\"ckpt\",\n        allow_patterns=\"*.safetensors\",\n        force_download=False\n    )\n    print(\"\\n‚úÖ Download complete!\")\nelse:\n    print(\"‚úÖ Checkpoint already downloaded\")\n# Verify\nprint(\"\\nüìÇ Files in ckpt/:\")\nfor file in os.listdir(\"ckpt\"):\n    if file.endswith(\".safetensors\"):\n        size = os.path.getsize(f\"ckpt/{file}\") / (1024**2)\n        print(f\"  ‚úì {file} ({size:.2f} MB)\")","metadata":{"execution":{"iopub.status.busy":"2025-12-31T10:34:59.135157Z","iopub.execute_input":"2025-12-31T10:34:59.135568Z","iopub.status.idle":"2025-12-31T10:35:02.612623Z","shell.execute_reply.started":"2025-12-31T10:34:59.135543Z","shell.execute_reply":"2025-12-31T10:35:02.611828Z"},"id":"9PsLgUs0cYmO","outputId":"77e74ba9-f348-4ffb-e675-0717fd7e74a7","papermill":{"duration":12.524295,"end_time":"2025-12-30T18:54:48.878013","exception":false,"start_time":"2025-12-30T18:54:36.353718","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"üì• Downloading checkpoint from Hugging Face Hub...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"699293d79b3a4e05ad753116ce03499e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"content_encoder.safetensors:   0%|          | 0.00/4.76M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ed98c8cd8fd48f6b19c1a1ea033b5cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unet.safetensors:   0%|          | 0.00/315M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e679edac449b4f55ac68456e6553f37c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"style_encoder.safetensors:   0%|          | 0.00/82.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78e0b71c75ce408a884f0fd6fe02521f"}},"metadata":{}},{"name":"stdout","text":"\n‚úÖ Download complete!\n\nüìÇ Files in ckpt/:\n  ‚úì content_encoder.safetensors (4.54 MB)\n  ‚úì style_encoder.safetensors (78.58 MB)\n  ‚úì unet.safetensors (300.34 MB)\n","output_type":"stream"}],"execution_count":4},{"id":"767e8ea2","cell_type":"code","source":"# @title Unzipping all archived files\nimport os\nimport glob\nfrom zipfile import ZipFile\n\nzip_file_paths = glob.glob(os.path.join(INPUT_PATH, '*.zip'))\n\nif not zip_file_paths:\n    print(f'No .zip files found in {INPUT_PATH}.')\nelse:\n    for zip_file_path in zip_file_paths:\n        if os.path.exists(zip_file_path):\n            print(f'Unzipping {zip_file_path}...')\n            !unzip -o {zip_file_path} -d ./\n            print(f'Unzipping of {zip_file_path} complete.')\n        else:\n            print(f'Error: The file {zip_file_path} was not found (post-glob check).')","metadata":{"execution":{"iopub.status.busy":"2025-12-31T10:35:02.613570Z","iopub.execute_input":"2025-12-31T10:35:02.613908Z","iopub.status.idle":"2025-12-31T10:35:02.622301Z","shell.execute_reply.started":"2025-12-31T10:35:02.613880Z","shell.execute_reply":"2025-12-31T10:35:02.621316Z"},"id":"ecfc18e0","outputId":"7b925027-747c-4cb1-977e-2bee34d1865f","papermill":{"duration":0.023805,"end_time":"2025-12-30T18:54:48.917163","exception":false,"start_time":"2025-12-30T18:54:48.893358","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"No .zip files found in /kaggle/input/.\n","output_type":"stream"}],"execution_count":5},{"id":"51941368","cell_type":"code","source":"import pandas as pd\nimport os\n\ndef convert_csv_to_chars_txt(input_csv_path: str, output_txt_path: str, column_name: str = 'word'):\n    \"\"\"\n    Reads a CSV file, extracts text from a specified column, and writes each character\n    to a new line in a plain text file.\n\n    Args:\n        input_csv_path (str): The full path to the input CSV file.\n        output_txt_path (str): The full path for the output text file.\n        column_name (str): The name of the column in the CSV file containing the text.\n    \"\"\"\n    if not os.path.exists(input_csv_path):\n        print(f\"Error: Input CSV file not found at '{input_csv_path}'. Please ensure the file is uploaded.\")\n        return\n\n    try:\n        df = pd.read_csv(input_csv_path)\n    except Exception as e:\n        print(f\"Error reading CSV file '{input_csv_path}': {e}\")\n        return\n\n    if column_name not in df.columns:\n        print(f\"Error: Column '{column_name}' not found in the CSV file '{input_csv_path}'.\")\n        return\n\n    all_characters = []\n    # Ensure the column values are treated as strings before iterating over them\n    for item in df[column_name].astype(str).dropna().tolist():\n        for char in item:\n            all_characters.append(char)\n\n    # Ensure output directory exists\n    os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)\n\n    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(all_characters))\n    print(f\"Successfully converted '{input_csv_path}' to '{output_txt_path}', with one character per line.\")\n\n# --- Example Usage (demonstration with a dummy file) ---\n# As the original file 'Ds_300_ChuNom_TuTao.csv' was not found in the previous execution,\n# let's create a dummy file to demonstrate the function's usage.\nprint(\"\\n--- Demonstrating function with a dummy CSV file ---\")\ndummy_csv_path = os.path.join(OUTPUT_PATH, \"dummy_data.csv\")\ndummy_output_txt_path = os.path.join(OUTPUT_PATH, \"dummy_chars.txt\")\n\n# Create a dummy CSV file\ndummy_data = {'word': ['hello', 'world', 'python']}\npd.DataFrame(dummy_data).to_csv(dummy_csv_path, index=False)\nprint(f\"Created a dummy CSV file at: {dummy_csv_path}\")\n\nconvert_csv_to_chars_txt(dummy_csv_path, dummy_output_txt_path)\n\n# --- How to use with your actual file ---\n# Uncomment the lines below and replace 'your_actual_file.csv' and 'your_output.txt'\n# with the correct paths for your use case.\n#\n# original_csv_file = os.path.join(INPUT_PATH, \"Ds_300_ChuNom_TuTao.csv\") # Or the full path to your CSV\n# original_output_txt = os.path.join(OUTPUT_PATH, \"nom_tu_tao.txt\") # Or your desired output path\n# convert_csv_to_chars_txt(original_csv_file, original_output_txt)\n","metadata":{"execution":{"iopub.status.busy":"2025-12-31T10:35:02.623272Z","iopub.execute_input":"2025-12-31T10:35:02.623541Z","iopub.status.idle":"2025-12-31T10:35:08.868147Z","shell.execute_reply.started":"2025-12-31T10:35:02.623519Z","shell.execute_reply":"2025-12-31T10:35:08.867427Z"},"id":"Mx5uS5WQaqdn","outputId":"951a03aa-416d-4e8d-ca55-121d410bb302","papermill":{"duration":1.62157,"end_time":"2025-12-30T18:54:50.594793","exception":false,"start_time":"2025-12-30T18:54:48.973223","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\n--- Demonstrating function with a dummy CSV file ---\nCreated a dummy CSV file at: /kaggle/working/dummy_data.csv\nSuccessfully converted '/kaggle/working/dummy_data.csv' to '/kaggle/working/dummy_chars.txt', with one character per line.\n","output_type":"stream"}],"execution_count":6},{"id":"4f4cf20b","cell_type":"code","source":"!ls -larth {OUTPUT_PATH}/ckpt","metadata":{"execution":{"iopub.status.busy":"2025-12-31T10:35:08.869018Z","iopub.execute_input":"2025-12-31T10:35:08.869365Z","iopub.status.idle":"2025-12-31T10:35:08.993991Z","shell.execute_reply.started":"2025-12-31T10:35:08.869337Z","shell.execute_reply":"2025-12-31T10:35:08.993324Z"},"id":"Sxz63qgifNlV","outputId":"c86d1ccf-dcc8-44ec-d61f-9035dd9d0369","papermill":{"duration":0.140282,"end_time":"2025-12-30T18:54:50.749810","exception":false,"start_time":"2025-12-30T18:54:50.609528","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"total 384M\ndrwxr-xr-x 3 root root 4.0K Dec 31 10:34 .cache\n-rw-r--r-- 1 root root 4.6M Dec 31 10:35 content_encoder.safetensors\n-rw-r--r-- 1 root root  79M Dec 31 10:35 style_encoder.safetensors\n-rw-r--r-- 1 root root 301M Dec 31 10:35 unet.safetensors\ndrwxr-xr-x 3 root root 4.0K Dec 31 10:35 .\ndrwxr-xr-x 5 root root 4.0K Dec 31 10:35 ..\n","output_type":"stream"}],"execution_count":7},{"id":"92cff682","cell_type":"code","source":"%cd {OUTPUT_PATH}\nfrom huggingface_hub import login\nHF_TOKEN = load_secret(\"HF_TOKEN\")\nlogin(HF_TOKEN)\nHF_USERNAME = \"dzungpham\"\n\n# ==========================================\n# EXPORT / DOWNLOAD DATASET COMMANDS\n# ==========================================\n\n# Train Split\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/train\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"train\" \\\n  --token HF_TOKEN \n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/train_original\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"train_original\" \\\n  --token HF_TOKEN \n# Validation: Unseen Both\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/val_unseen_both\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"val_unseen_both\" \\\n  --token HF_TOKEN \n# Validation: Seen Style, Unseen Char\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/val_seen_style_unseen_char\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"val_seen_style_unseen_char\" \\\n  --token HF_TOKEN \n# Validation: Unseen Style, Seen Char\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/val_unseen_style_seen_char\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"val_unseen_style_seen_char\" \\\n  --token HF_TOKEN \n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/test\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"test\" \\\n  --token HF_TOKEN \nprint(\"SUCCESSFULLY EXPORT HF DATASET TO LOCAL DIRECTORY\")","metadata":{"execution":{"iopub.status.busy":"2025-12-31T10:35:08.995026Z","iopub.execute_input":"2025-12-31T10:35:08.995256Z","iopub.status.idle":"2025-12-31T10:36:21.121326Z","shell.execute_reply.started":"2025-12-31T10:35:08.995234Z","shell.execute_reply":"2025-12-31T10:36:21.120400Z"},"id":"MvEJIiH5fNlV","outputId":"95a0a309-3e6b-482b-c971-cf93efac61bf","papermill":{"duration":0.104394,"end_time":"2025-12-30T18:54:50.869230","exception":false,"start_time":"2025-12-30T18:54:50.764836","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working\nAttempting to load secret 'HF_TOKEN' from 'kaggle' environment...\n‚úÖ Successfully loaded secret 'HF_TOKEN'.\n\n============================================================\nEXPORTING DATASET TO DISK\n============================================================\n\nüì• Loading dataset from Hub...\n   Repository: dzungpham/font-diffusion-generated-data\n   Split: train\nREADME.md: 1.29kB [00:00, 804kB/s]\ndata/train-00000-of-00001.parquet: 100%|‚ñà‚ñà‚ñà| 39.5M/39.5M [00:01<00:00, 32.5MB/s]\ndata/val-00000-of-00001.parquet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 262k/262k [00:00<00:00, 1.27MB/s]\ndata/val_unseen_both-00000-of-00001.parq(‚Ä¶): 100%|‚ñà| 2.52M/2.52M [00:00<00:00, 1\ndata/val_seen_style_unseen_char-00000-of(‚Ä¶): 100%|‚ñà| 8.02M/8.02M [00:00<00:00, 1\ndata/val_unseen_style_seen_char-00000-of(‚Ä¶): 100%|‚ñà| 10.5M/10.5M [00:00<00:00, 3\ndata/train_original-00000-of-00001.parqu(‚Ä¶): 100%|‚ñà| 68.8M/68.8M [00:00<00:00, 2\ndata/test-00000-of-00001.parquet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 646k/646k [00:00<00:00, 4.37MB/s]\nGenerating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 2585/2585 [00:00<00:00, 19827.22 examples/s]\nGenerating val split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:00<00:00, 7095.96 examples/s]\nGenerating val_unseen_both split: 100%|‚ñà| 240/240 [00:00<00:00, 24767.68 example\nGenerating val_seen_style_unseen_char split: 100%|‚ñà| 880/880 [00:00<00:00, 35538\nGenerating val_unseen_style_seen_char split: 100%|‚ñà| 705/705 [00:00<00:00, 25859\nGenerating train_original split: 100%|‚ñà| 4500/4500 [00:00<00:00, 25361.92 exampl\nGenerating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [00:00<00:00, 6062.20 examples/s]\n‚úì Loaded dataset with 2585 samples from Hub\n\nüì• Attempting to load results_checkpoint.json from Hub...\nresults_checkpoint.json: 1.57MB [00:00, 392MB/s]\n  ‚úì Loaded results_checkpoint.json (4500 generations)\n\nExporting images from dataset...\n\nüìù Exporting content images...\nContent images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2585/2585 [00:05<00:00, 481.27it/s]\n‚úì Exported content images\n\nüé® Exporting target images...\nTarget images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2585/2585 [00:05<00:00, 448.57it/s]\n‚úì Exported target images\n\n‚úÖ Using original results_checkpoint.json\n\nüíæ Saving results_checkpoint.json...\n  ‚úì Saved results_checkpoint.json (4500 generations)\n\nüìä Metadata Statistics:\n  Total generations: 4500\n  Total characters: 300\n  Total styles: 15\n  Fonts: NomNaTongLight2\n\n============================================================\n‚úÖ EXPORT COMPLETE!\n============================================================\n\nFiles created:\n  ‚úì my_dataset/train/ContentImage/\n  ‚úì my_dataset/train/TargetImage/\n  ‚úì my_dataset/train/results_checkpoint.json\n\n============================================================\nEXPORTING DATASET TO DISK\n============================================================\n\nüì• Loading dataset from Hub...\n   Repository: dzungpham/font-diffusion-generated-data\n   Split: train_original\n‚úì Loaded dataset with 4500 samples from Hub\n\nüì• Attempting to load results_checkpoint.json from Hub...\n  ‚úì Loaded results_checkpoint.json (4500 generations)\n\nExporting images from dataset...\n\nüìù Exporting content images...\nContent images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4500/4500 [00:09<00:00, 481.23it/s]\n‚úì Exported content images\n\nüé® Exporting target images...\nTarget images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4500/4500 [00:09<00:00, 453.83it/s]\n‚úì Exported target images\n\n‚úÖ Using original results_checkpoint.json\n\nüíæ Saving results_checkpoint.json...\n  ‚úì Saved results_checkpoint.json (4500 generations)\n\nüìä Metadata Statistics:\n  Total generations: 4500\n  Total characters: 300\n  Total styles: 15\n  Fonts: NomNaTongLight2\n\n============================================================\n‚úÖ EXPORT COMPLETE!\n============================================================\n\nFiles created:\n  ‚úì my_dataset/train_original/ContentImage/\n  ‚úì my_dataset/train_original/TargetImage/\n  ‚úì my_dataset/train_original/results_checkpoint.json\n\n============================================================\nEXPORTING DATASET TO DISK\n============================================================\n\nüì• Loading dataset from Hub...\n   Repository: dzungpham/font-diffusion-generated-data\n   Split: val_unseen_both\n‚úì Loaded dataset with 240 samples from Hub\n\nüì• Attempting to load results_checkpoint.json from Hub...\n  ‚úì Loaded results_checkpoint.json (4500 generations)\n\nExporting images from dataset...\n\nüìù Exporting content images...\nContent images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 477.30it/s]\n‚úì Exported content images\n\nüé® Exporting target images...\nTarget images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 490.91it/s]\n‚úì Exported target images\n\n‚úÖ Using original results_checkpoint.json\n\nüíæ Saving results_checkpoint.json...\n  ‚úì Saved results_checkpoint.json (4500 generations)\n\nüìä Metadata Statistics:\n  Total generations: 4500\n  Total characters: 300\n  Total styles: 15\n  Fonts: NomNaTongLight2\n\n============================================================\n‚úÖ EXPORT COMPLETE!\n============================================================\n\nFiles created:\n  ‚úì my_dataset/val_unseen_both/ContentImage/\n  ‚úì my_dataset/val_unseen_both/TargetImage/\n  ‚úì my_dataset/val_unseen_both/results_checkpoint.json\n\n============================================================\nEXPORTING DATASET TO DISK\n============================================================\n\nüì• Loading dataset from Hub...\n   Repository: dzungpham/font-diffusion-generated-data\n   Split: val_seen_style_unseen_char\n‚úì Loaded dataset with 880 samples from Hub\n\nüì• Attempting to load results_checkpoint.json from Hub...\n  ‚úì Loaded results_checkpoint.json (4500 generations)\n\nExporting images from dataset...\n\nüìù Exporting content images...\nContent images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 880/880 [00:01<00:00, 472.79it/s]\n‚úì Exported content images\n\nüé® Exporting target images...\nTarget images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 880/880 [00:01<00:00, 456.73it/s]\n‚úì Exported target images\n\n‚úÖ Using original results_checkpoint.json\n\nüíæ Saving results_checkpoint.json...\n  ‚úì Saved results_checkpoint.json (4500 generations)\n\nüìä Metadata Statistics:\n  Total generations: 4500\n  Total characters: 300\n  Total styles: 15\n  Fonts: NomNaTongLight2\n\n============================================================\n‚úÖ EXPORT COMPLETE!\n============================================================\n\nFiles created:\n  ‚úì my_dataset/val_seen_style_unseen_char/ContentImage/\n  ‚úì my_dataset/val_seen_style_unseen_char/TargetImage/\n  ‚úì my_dataset/val_seen_style_unseen_char/results_checkpoint.json\n\n============================================================\nEXPORTING DATASET TO DISK\n============================================================\n\nüì• Loading dataset from Hub...\n   Repository: dzungpham/font-diffusion-generated-data\n   Split: val_unseen_style_seen_char\n‚úì Loaded dataset with 705 samples from Hub\n\nüì• Attempting to load results_checkpoint.json from Hub...\n  ‚úì Loaded results_checkpoint.json (4500 generations)\n\nExporting images from dataset...\n\nüìù Exporting content images...\nContent images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 705/705 [00:01<00:00, 490.44it/s]\n‚úì Exported content images\n\nüé® Exporting target images...\nTarget images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 705/705 [00:01<00:00, 492.43it/s]\n‚úì Exported target images\n\n‚úÖ Using original results_checkpoint.json\n\nüíæ Saving results_checkpoint.json...\n  ‚úì Saved results_checkpoint.json (4500 generations)\n\nüìä Metadata Statistics:\n  Total generations: 4500\n  Total characters: 300\n  Total styles: 15\n  Fonts: NomNaTongLight2\n\n============================================================\n‚úÖ EXPORT COMPLETE!\n============================================================\n\nFiles created:\n  ‚úì my_dataset/val_unseen_style_seen_char/ContentImage/\n  ‚úì my_dataset/val_unseen_style_seen_char/TargetImage/\n  ‚úì my_dataset/val_unseen_style_seen_char/results_checkpoint.json\n\n============================================================\nEXPORTING DATASET TO DISK\n============================================================\n\nüì• Loading dataset from Hub...\n   Repository: dzungpham/font-diffusion-generated-data\n   Split: test\n‚úì Loaded dataset with 41 samples from Hub\n\nüì• Attempting to load results_checkpoint.json from Hub...\n  ‚úì Loaded results_checkpoint.json (4500 generations)\n\nExporting images from dataset...\n\nüìù Exporting content images...\nContent images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [00:00<00:00, 446.86it/s]\n‚úì Exported content images\n\nüé® Exporting target images...\nTarget images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [00:00<00:00, 407.78it/s]\n‚úì Exported target images\n\n‚úÖ Using original results_checkpoint.json\n\nüíæ Saving results_checkpoint.json...\n  ‚úì Saved results_checkpoint.json (4500 generations)\n\nüìä Metadata Statistics:\n  Total generations: 4500\n  Total characters: 300\n  Total styles: 15\n  Fonts: NomNaTongLight2\n\n============================================================\n‚úÖ EXPORT COMPLETE!\n============================================================\n\nFiles created:\n  ‚úì my_dataset/test/ContentImage/\n  ‚úì my_dataset/test/TargetImage/\n  ‚úì my_dataset/test/results_checkpoint.json\nSUCCESSFULLY EXPORT HF DATASET TO LOCAL DIRECTORY\n","output_type":"stream"}],"execution_count":8},{"id":"d77b3a8e-60ec-4299-ad46-a82ee2852629","cell_type":"code","source":"!uv pip install \"huggingface-hub==0.25.2\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T10:36:21.122579Z","iopub.execute_input":"2025-12-31T10:36:21.122908Z","iopub.status.idle":"2025-12-31T10:36:21.487291Z","shell.execute_reply.started":"2025-12-31T10:36:21.122853Z","shell.execute_reply":"2025-12-31T10:36:21.486544Z"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m12 packages\u001b[0m \u001b[2min 57ms\u001b[0m\u001b[0m                                         \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 58ms\u001b[0m\u001b[0m                                               \n\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 12ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m25.2                              \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.25.2\u001b[0m\n","output_type":"stream"}],"execution_count":9},{"id":"29deed1d","cell_type":"code","source":"# already change sample_batch file to save all data in train_original\n%cd {OUTPUT_PATH}\n!python FontDiffusion/sample_batch.py \\\n    --characters \"FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\" \\\n    --style_images \"FontDiffusion/styles_images\" \\\n    --ckpt_dir \"ckpt/\" \\\n    --ttf_path \"FontDiffusion/fonts/NomNaTong-Regular.otf\" \\\n    --output_dir \"my_dataset/train_original\" \\\n    --resume_from \"my_dataset/train_original/results_checkpoint.json\" \\\n    --num_inference_steps 20 \\\n    --guidance_scale 7.5 \\\n    --start_line 1 \\\n    --end_line 50 \\\n    --batch_size 24 \\\n    --save_interval 1 \\\n    --channels_last \\\n    --seed 42 \\\n    --compile \\\n    --enable_xformers","metadata":{"execution":{"iopub.status.busy":"2025-12-31T10:36:21.489825Z","iopub.execute_input":"2025-12-31T10:36:21.490082Z"},"id":"gma02BZvhx8I","outputId":"a8a54761-e26c-488d-d87d-cd14d686e77f","papermill":{"duration":10.53661,"end_time":"2025-12-30T18:55:01.421093","exception":false,"start_time":"2025-12-30T18:54:50.884483","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working\nThe cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n0it [00:00, ?it/s]\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n2025-12-31 10:36:32.715306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767177392.938785    1250 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767177393.000827    1250 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\npygame 2.6.1 (SDL 2.28.4, Python 3.11.13)\nHello from the pygame community. https://www.pygame.org/contribute.html\n\n============================================================\nFONTDIFFUSER - SIMPLIFIED (Only results_checkpoint.json)\n============================================================\nLoading characters from lines 1 to 50 (total: 10174 lines)\nüìñ Reading character file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 592415.82line/s]\nSuccessfully loaded 50 single characters.\n\nüìÇ Loading 15 style images from directory...\n‚úì Verifying style images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:00<00:00, 145635.56image/s]\n\nInitializing font manager...\nerror: XDG_RUNTIME_DIR not set in the environment.\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n‚úì Loaded font: NomNaTong-Regular\n\nüìä Configuration:\n  Dataset split: train_original\n  Number of Characters: 50 (lines 1-50)\n  Number of Styles: 15\n  Output Directory: my_dataset/train_original\n  Save Interval: Every 1 styles\n‚úì Loaded checkpoint (4500 generations)\n‚úì Loaded results.json:\n  Characters: 300\n  Styles: 15\n  Existing pairs: 4500\n\nLoading FontDiffuser pipeline...\nLoading FontDiffuser pipeline...\nLoad the down block  DownBlock2D\nLoad the down block  MCADownBlock2D\nThe style_attention cross attention dim in Down Block 1 layer is 1024\nThe style_attention cross attention dim in Down Block 2 layer is 1024\nLoad the down block  MCADownBlock2D\nThe style_attention cross attention dim in Down Block 1 layer is 1024\nThe style_attention cross attention dim in Down Block 2 layer is 1024\nLoad the down block  DownBlock2D\nLoad the up block  UpBlock2D\nLoad the up block  StyleRSIUpBlock2D\nLoad the up block  StyleRSIUpBlock2D\nLoad the up block  UpBlock2D\nParam count for Ds initialized parameters: 20591296\nGet CG-GAN Style Encoder!\nParam count for Ds initialized parameters: 1187008\nGet CG-GAN Content Encoder!\n‚úì Loaded model state_dict successfully\nConverting to channels-last memory format...\n‚úì Model moved to device\n‚úì Loaded training DDPM scheduler successfully\n‚úì Loaded DPM-Solver pipeline successfully\n\n============================================================\nCompiling pipeline with torch.compile...\n============================================================\n‚ö†Ô∏è  Warning: torch.compile failed: \n  Continuing without compilation...\n  Note: torch.compile requires PyTorch 2.0+ and may not work with all models\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 233M/233M [00:01<00:00, 210MB/s]\nLoading model from: /usr/local/lib/python3.11/dist-packages/lpips/weights/v0.1/alex.pth\nüì• Resuming: 4500 pairs already processed\n\n======================================================================\n                        BATCH IMAGE GENERATION                        \n======================================================================\nFont Number:          1\nFont Names:           NomNaTong-Regular\nStyles:               15\nCharacters:           50\nBatch size:           24\nGuidance scale:       7.5\nInference steps:      20\nOutput Dir:           my_dataset/train_original\n======================================================================\n\nUsing font: NomNaTong-Regular\n======================================================================\n\nüé® Generating styles:   0%|                                                  | 0/15 [00:00<?, ?it/s]\n  üìù Preparing NomNaTong-Regular:   0%|                | 0/50 [00:00<?, ?char/s]\u001b[A\n                                                                                \u001b[A\n  üé® Inferencing:   0%|                                | 0/3 [00:00<?, ?batch/s]\u001b[A/kaggle/working/FontDiffusion/src/model.py:99: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.style_encoder'.\n  style_img_feature, _, style_residual_features = self.style_encoder(style_images)\n/kaggle/working/FontDiffusion/src/model.py:107: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n  content_img_feture, content_residual_features = self.content_encoder(\n/kaggle/working/FontDiffusion/src/model.py:112: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n  style_content_feature, style_content_res_features = self.content_encoder(\n/kaggle/working/FontDiffusion/src/model.py:124: FutureWarning: Accessing config attribute `unet` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'unet' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.unet'.\n  out = self.unet(\n\n  üé® Inferencing:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 1/3 [00:38<01:17, 38.72s/batch]\u001b[A\n  üé® Inferencing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:18<00:00, 24.82s/batch]\u001b[A\n  üé® Inferencing: 5batch [01:22, 13.02s/batch]                                  \u001b[A\n  ‚úì style0: 50 images in 82.36s                                                                     \nüé® Generating styles:   0%|                                                  | 0/15 [01:25<?, ?it/s]\n======================================================================\n                              CHECKPOINT                              \n======================================================================\nProgress:           1/15 styles\nGenerated:          50 pairs\nSkipped:            0 pairs\nElapsed time:       1.4 minutes\nEst. remaining:     20.0 minutes\n======================================================================\n  ‚úÖ Saved results_checkpoint.json (4550 generations)\nüé® Generating styles:   7%|‚ñà‚ñà‚ñä                                       | 1/15 [01:25<20:03, 85.96s/it]\n  üìù Preparing NomNaTong-Regular:   0%|                | 0/50 [00:00<?, ?char/s]\u001b[A\n                                                                                \u001b[A\n  üé® Inferencing:   0%|                                | 0/3 [00:00<?, ?batch/s]\u001b[A\n  üé® Inferencing:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 1/3 [00:43<01:26, 43.11s/batch]\u001b[A\n  üé® Inferencing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:28<00:00, 28.10s/batch]\u001b[A\n  üé® Inferencing: 5batch [01:32, 14.65s/batch]                                  \u001b[A\n  ‚úì style1: 50 images in 92.69s                                                                     \nüé® Generating styles:   7%|‚ñà‚ñà‚ñä                                       | 1/15 [03:01<20:03, 85.96s/it]\n======================================================================\n                              CHECKPOINT                              \n======================================================================\nProgress:           2/15 styles\nGenerated:          100 pairs\nSkipped:            0 pairs\nElapsed time:       3.0 minutes\nEst. remaining:     19.7 minutes\n======================================================================\n  ‚úÖ Saved results_checkpoint.json (4600 generations)\nüé® Generating styles:  13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                    | 2/15 [03:02<19:55, 91.93s/it]\n  üìù Preparing NomNaTong-Regular:   0%|                | 0/50 [00:00<?, ?char/s]\u001b[A\n                                                                                \u001b[A\n  üé® Inferencing:   0%|                                | 0/3 [00:00<?, ?batch/s]\u001b[A\n  üé® Inferencing:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 1/3 [00:44<01:29, 44.69s/batch]\u001b[A\n  üé® Inferencing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:29<00:00, 28.10s/batch]\u001b[A\n  üé® Inferencing: 5batch [01:33, 14.65s/batch]                                  \u001b[A\n  ‚úì style2: 50 images in 93.18s                                                                     \nüé® Generating styles:  13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                    | 2/15 [04:38<19:55, 91.93s/it]\n======================================================================\n                              CHECKPOINT                              \n======================================================================\nProgress:           3/15 styles\nGenerated:          150 pairs\nSkipped:            0 pairs\nElapsed time:       4.6 minutes\nEst. remaining:     18.6 minutes\n======================================================================\n  ‚úÖ Saved results_checkpoint.json (4650 generations)\nüé® Generating styles:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                 | 3/15 [04:38<18:48, 94.06s/it]\n  üìù Preparing NomNaTong-Regular:   0%|                | 0/50 [00:00<?, ?char/s]\u001b[A\n                                                                                \u001b[A\n  üé® Inferencing:   0%|                                | 0/3 [00:00<?, ?batch/s]\u001b[A","output_type":"stream"}],"execution_count":null},{"id":"f9250a14","cell_type":"code","source":"!python FontDiffusion/create_validation_split.py \\\n  --data_root my_dataset \\\n  --val_ratio 0.2 \\\n  --test_ratio 0.1 \\\n  --seed 42","metadata":{"id":"XoppW2x5fNlW","outputId":"759bca6c-0025-454a-b3b6-ae8170f7edb4","papermill":{"duration":0.236541,"end_time":"2025-12-30T18:55:01.673705","exception":false,"start_time":"2025-12-30T18:55:01.437164","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e92a9392","cell_type":"code","source":"# --- RAW DATA (Before Splitting) ---\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/train_original\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"train_original\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# --- ORGANIZED SPLITS (After Splitting) ---\n\n# Train Split\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/train\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"train\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# Test Split\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/test\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"test\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# Validation: Unseen Both\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/val_unseen_both\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"val_unseen_both\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# Validation: Seen Style, Unseen Char\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/val_seen_style_unseen_char\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"val_seen_style_unseen_char\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# Validation: Unseen Style, Seen Char\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/val_unseen_style_seen_char\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"val_unseen_style_seen_char\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\nprint(\"SUCCESSFULLY UPLOAD LOCAL MY_DATASET TO HUGGINGFACE DATASETS SPACE\")","metadata":{"id":"v-a7paEbfNlW","outputId":"c792e189-6d31-4114-be92-644d4372efa7","papermill":{"duration":21.070659,"end_time":"2025-12-30T18:55:22.760587","exception":false,"start_time":"2025-12-30T18:55:01.689928","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"a87caab2","cell_type":"code","source":"import torch, gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"id":"zXSQHJ3xVOSc","outputId":"4d89a176-c286-442f-eb6e-01b39c994e64","papermill":{"duration":1.992585,"end_time":"2025-12-30T18:55:24.769269","exception":false,"start_time":"2025-12-30T18:55:22.776684","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"267634e8","cell_type":"code","source":"# TRAINING PHASE 1\nimport wandb\n\n# Load your Weights & Biases API key from a secure store\nwandb_key = load_secret(\"WANDB_API_KEY\")\nwandb.login(key=wandb_key)\n\n# Run the training script with the corrected flag syntax\n!python FontDiffusion/my_train.py \\\n    --seed=123 \\\n    --experience_name=FontDiffuser_training_phase_1 \\\n    --data_root=my_dataset \\\n    --output_dir=outputs/FontDiffuser \\\n    --report_to=wandb \\\n    --resolution=96 \\\n    --style_image_size=96 \\\n    --content_image_size=96 \\\n    --content_encoder_downsample_size=3 \\\n    --channel_attn=True \\\n    --content_start_channel=64 \\\n    --style_start_channel=64 \\\n    --train_batch_size=8 \\\n    --perceptual_coefficient=0.03 \\\n    --offset_coefficient=0.7 \\\n    --max_train_steps=2000 \\\n    --ckpt_interval=1000 \\\n    --val_interval=200 \\\n    --gradient_accumulation_steps=1 \\\n    --log_interval=50 \\\n    --learning_rate=1e-4 \\\n    --lr_scheduler=linear \\\n    --lr_warmup_steps=10000 \\\n    --drop_prob=0.1 \\\n    --mixed_precision=no","metadata":{"id":"FxxJ9qy4KIZH","outputId":"f554de7b-f455-4a5b-f8e5-4b6cba3f756b","papermill":{"duration":0.021927,"end_time":"2025-12-30T18:55:24.807644","exception":false,"start_time":"2025-12-30T18:55:24.785717","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"cb2a3326-6bfa-4fba-bbcc-e2e7be996c7f","cell_type":"code","source":"!ls -lr outputs/FontDiffuser","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"97f8136e","cell_type":"code","source":"# TRAINING PHASE 2\n!wandb login\n!python FontDiffusion/my_train.py \\\n    --seed=123 \\\n    --experience_name=\"FontDiffuser_training_phase_2\" \\\n    --data_root=\"my_dataset\" \\\n    --output_dir=\"outputs/FontDiffuser\" \\\n    --report_to=\"wandb\" \\\n    --phase_2 \\\n    --phase_1_ckpt_dir=\"outputs/FontDiffuser/global_step_2000\" \\\n    --scr_ckpt_path=\"ckpt/scr_210000.pth\" \\\n    --sc_coefficient=0.05 \\\n    --num_neg=13 \\\n    --resolution=96 \\\n    --style_image_size=96 \\\n    --content_image_size=96 \\\n    --content_encoder_downsample_size=3 \\\n    --channel_attn=True \\\n    --content_start_channel=64 \\\n    --style_start_channel=64 \\\n    --train_batch_size=8 \\\n    --perceptual_coefficient=0.03 \\\n    --offset_coefficient=0.4 \\\n    --max_train_steps=100 \\\n    --ckpt_interval=50 \\\n    --gradient_accumulation_steps=2 \\\n    --log_interval=50 \\\n    --learning_rate=1e-5 \\\n    --lr_scheduler=\"constant\" \\\n    --lr_warmup_steps=1000 \\\n    --drop_prob=0.1 \\\n    --mixed_precision=\"no\"\n","metadata":{"id":"J4bplsS6pQna","papermill":{"duration":0.022471,"end_time":"2025-12-30T18:55:24.845778","exception":false,"start_time":"2025-12-30T18:55:24.823307","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"88c45e2f","cell_type":"code","source":"!python FontDiffusion/pth2safetensors.py \\\n    --weights_dir \"ckpt\" \\\n    --repo_id \"dzungpham/font-diffusion-weights\" \\\n    --token \"{HF_TOKEN}\"","metadata":{"id":"nF8opWokKcMS","outputId":"2838655d-5d24-4808-813a-39f20ec239a8","papermill":{"duration":0.217876,"end_time":"2025-12-30T18:55:25.079820","exception":false,"start_time":"2025-12-30T18:55:24.861944","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"5868b20b","cell_type":"code","source":"import os\nimport zipfile\nfrom pathlib import Path\nfrom typing import List\ndef find_result_folders(base_path: Path, pattern_name: str) -> List[Path]:\n    return [p for p in base_path.glob(pattern_name) if p.is_dir()]\n\ndef zip_folder(folder_path: Path, output_base_path: Path) -> bool:\n    folder_name = folder_path.name\n    zip_path = output_base_path / f\"{folder_name}.zip\"\n    try:\n        print(f\"   -> Zipping folder: {folder_name}...\")\n        with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zipf:\n            for file_path in folder_path.rglob(\"*\"):\n                if file_path.is_file():\n                    arcname = file_path.relative_to(folder_path.parent)\n                    zipf.write(file_path, arcname)\n        print(f\"   ‚úÖ Created ZIP: {zip_path.name}\")\n        return True\n    except Exception as exc:\n        print(f\"   ‚ùå Failed to zip {folder_name}: {exc}\")\n        return False\n\ndef zip_stats_results_folders(output_base_path: str, pattern_name: str) -> None:\n    base = Path(output_base_path)\n    base.mkdir(parents=True, exist_ok=True)\n    result_folders = find_result_folders(base, pattern_name)\n    if not result_folders:\n        print(f\"‚ö†Ô∏è No folders matching '*dataset' found in '{output_base_path}'.\")\n        return\n    print(f\"üîç Found {len(result_folders)} result folder(s) to zip.\")\n    successful = sum(1 for folder in result_folders if zip_folder(folder, base))\n    print(f\"\\n‚úÖ DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\")\n\nif __name__ == \"__main__\":\n    try:\n        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n        if not output_root:\n            raise ValueError(\"OUTPUT_PATH not defined\")\n        zip_stats_results_folders(\n            output_base_path=OUTPUT_PATH,\n            pattern_name=\"outputs/FontDiffuser/global*\")\n    except Exception as e:\n        print(f\"‚ùå An error occurred: {e}\")","metadata":{"id":"kTz9WZ9ylBZx","outputId":"ccb61bf4-7551-4269-aee7-b0742fe1517a","papermill":{"duration":0.031197,"end_time":"2025-12-30T18:55:25.126961","exception":false,"start_time":"2025-12-30T18:55:25.095764","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}