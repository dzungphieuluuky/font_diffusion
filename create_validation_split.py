"""
Create validation/test splits from training data
âœ… CORRECTED: Properly handles train/val splits with checkpoint filtering
âœ… Parses filenames as codepoint_char_hash.png and codepoint_char_style_hash.png
âœ… Ensures checkpoint.json contains only relevant generations
âœ… Validates contentâ†”target pairs exist before including in split
"""

import os
import shutil
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Set, Optional
from dataclasses import dataclass
from collections import defaultdict
import random

from tqdm import tqdm
import hashlib


# Setup logging with tqdm compatibility
class TqdmLoggingHandler(logging.Handler):
    def emit(self, record):
        try:
            msg = self.format(record)
            tqdm.write(msg)
            self.flush()
        except Exception:
            self.handleError(record)


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[TqdmLoggingHandler()],
)


def compute_file_hash(char: str, style: str, font: str = "") -> str:
    """Compute deterministic hash for a (character, style, font) combination"""
    content = f"{char}_{style}_{font}"
    return hashlib.sha256(content.encode("utf-8")).hexdigest()[:8]


def get_content_filename(char: str, font: str = "") -> str:
    """
    Get content image filename for character
    Format: {unicode_codepoint}_{char}_{hash}.png
    """
    codepoint = f"U+{ord(char):04X}"
    hash_val = compute_file_hash(char, "", font)
    return f"{codepoint}_{char}_{hash_val}.png"


def get_target_filename(char: str, style: str, font: str = "") -> str:
    """
    Get target image filename
    Format: {unicode_codepoint}_{char}_{style}_{hash}.png
    """
    codepoint = f"U+{ord(char):04X}"
    hash_val = compute_file_hash(char, style, font)
    return f"{codepoint}_{char}_{style}_{hash_val}.png"


def parse_content_filename(filename: str) -> Optional[Tuple[str, str]]:
    """
    Parse content filename: U+XXXX_{char}_{hash}.png
    Returns: (char, hash) or None if parse fails
    """
    if not filename.endswith('.png'):
        return None
    
    stem = filename[:-4]  # Remove .png
    parts = stem.split('_')
    
    if len(parts) < 3:
        return None
    
    codepoint = parts[0]
    hash_val = parts[-1]
    
    if not codepoint.startswith("U+"):
        return None
    
    try:
        char_code = int(codepoint.replace("U+", ""), 16)
        char = chr(char_code)
        return (char, hash_val)
    except (ValueError, OverflowError):
        return None


def parse_target_filename(filename: str) -> Optional[Tuple[str, str, str]]:
    """
    Parse target filename: U+XXXX_{char}_{style}_{hash}.png
    Correctly handles styles with underscores like 'ref_trien', 'my_style_2', etc.
    Hash is always the last 8 hex characters before .png
    Returns: (char, style, hash) or None if parse fails
    
    Examples:
    - U+200E9_ğ ƒ©_ref_trien_fdf6dcd1.png â†’ ('ğ ƒ©', 'ref_trien', 'fdf6dcd1')
    - U+2693E_ğ¦¤¾_ref_1_bc1ad149.png â†’ ('ğ¦¤¾', 'ref_1', 'bc1ad149')
    """
    if not filename.endswith('.png'):
        return None
    
    stem = filename[:-4]  # Remove .png
    parts = stem.split('_')
    
    # Need at least: codepoint, char, style_part, hash (4 parts minimum)
    if len(parts) < 4:
        return None
    
    codepoint = parts[0]
    
    if not codepoint.startswith("U+"):
        return None
    
    try:
        char_code = int(codepoint.replace("U+", ""), 16)
        char = chr(char_code)
        
        # The hash is always 8 hex characters and is the LAST part
        hash_val = parts[-1]
        
        # Validate hash is 8 hex chars
        if len(hash_val) != 8 or not all(c in '0123456789abcdef' for c in hash_val.lower()):
            return None
        
        # parts[0] = codepoint (U+XXXX)
        # parts[1] = character itself
        # parts[2:-1] = style parts (can have underscores)
        # parts[-1] = hash
        
        # Extract style (everything after char, before hash)
        style_parts = parts[2:4]
        
        if not style_parts:  # No style found
            return None
        
        style = "_".join(style_parts)
        
        return (char, style, hash_val)
    except (ValueError, OverflowError, IndexError):
        return None
    
@dataclass
class ValidationSplitConfig:
    """Configuration for validation split creation"""
    data_root: str
    val_split_ratio: float = 0.2
    random_seed: int = 42


class ValidationSplitCreator:
    """Create train/val splits with proper checkpoint filtering"""

    def __init__(self, config: ValidationSplitConfig):
        self.config = config
        self.data_root = Path(config.data_root)
        
        self.original_train_dir = self.data_root / "train_original"
        self.train_dir = self.data_root / "train"
        self.val_dir = self.data_root / "val"
        
        random.seed(config.random_seed)
        self.detected_font = ""
        
        self._validate_structure()

    def _validate_structure(self) -> None:
        """Validate training directory structure"""
        source_dir = (
            self.original_train_dir
            if self.original_train_dir.exists()
            else self.train_dir
        )
        
        if not (source_dir / "TargetImage").exists():
            raise ValueError(f"TargetImage not found in {source_dir}")
        if not (source_dir / "ContentImage").exists():
            raise ValueError(f"ContentImage not found in {source_dir}")
        
        self.source_train_dir = source_dir
        logging.info(f"âœ“ Using source directory: {self.source_train_dir}")

    def analyze_data(self) -> Tuple[Dict[str, str], Dict[Tuple[str, str], str], Dict[str, List[str]]]:
        """
        âœ… CORRECTED: Analyze by scanning actual files and matching contentâ†”target pairs
        
        Returns:
        - content_files: {char -> hash} (what content images exist)
        - target_files: {(char, style) -> hash} (what target images exist with their hash)
        - char_to_styles: {char -> [styles]} (which styles exist for each char)
        """
        logging.info("\n" + "=" * 70)
        logging.info("ANALYZING TRAINING DATA")
        logging.info("=" * 70)
        
        content_dir = self.source_train_dir / "ContentImage"
        target_dir = self.source_train_dir / "TargetImage"
        
        content_files: Dict[str, str] = {}  # char -> hash
        target_files: Dict[Tuple[str, str], str] = {}  # (char, style) -> hash
        char_to_styles: Dict[str, List[str]] = defaultdict(set)
        
        # Scan content images
        logging.info("\nğŸ” Scanning content images...")
        if content_dir.exists():
            for img_file in tqdm(
                list(content_dir.glob("*.png")),
                desc="Content images",
                ncols=100,
                unit="img",
            ):
                result = parse_content_filename(img_file.name)
                if result:
                    char, hash_val = result
                    content_files[char] = hash_val
        
        logging.info(f"  âœ“ Found {len(content_files)} content images")
        
        # Scan target images
        logging.info("\nğŸ” Scanning target images...")
        total_targets = 0
        for style_folder in tqdm(
            sorted(target_dir.iterdir()),
            desc="Styles",
            ncols=100,
            unit="style",
        ):
            if not style_folder.is_dir():
                continue
            
            style_name = style_folder.name
            
            for img_file in style_folder.glob("*.png"):
                result = parse_target_filename(img_file.name)
                if result:
                    char, style, hash_val = result
                    
                    # Validate style matches folder
                    if style != style_name:
                        tqdm.write(
                            f"  âš ï¸  Style mismatch: {img_file.name} "
                            f"(extracted '{style}' != folder '{style_name}')"
                        )
                        continue
                    
                    target_files[(char, style)] = hash_val
                    char_to_styles[char].add(style)
                    total_targets += 1
        
        logging.info(f"  âœ“ Found {total_targets} target images")
        
        # Validate contentâ†”target pairing
        logging.info("\nğŸ” Validating contentâ†”target pairs...")
        valid_pairs: Dict[Tuple[str, str], bool] = {}
        missing_count = 0
        
        for (char, style), target_hash in tqdm(
            target_files.items(),
            desc="Validating pairs",
            ncols=100,
            unit="pair",
        ):
            if char not in content_files:
                tqdm.write(
                    f"  âš ï¸  Missing content for: {char} (style: {style})"
                )
                missing_count += 1
                valid_pairs[(char, style)] = False
            else:
                valid_pairs[(char, style)] = True
        
        # Filter to only valid pairs
        valid_target_files = {
            pair: hash_val
            for pair, hash_val in target_files.items()
            if valid_pairs.get(pair, False)
        }
        
        logging.info(f"\nâœ… Summary:")
        logging.info(f"  Content images: {len(content_files)}")
        logging.info(f"  Target images: {len(target_files)}")
        logging.info(f"  Valid pairs: {len(valid_target_files)}")
        if missing_count > 0:
            logging.info(f"  Missing content: {missing_count}")
        
        return content_files, valid_target_files, dict(char_to_styles)

    def create_simple_splits(
        self,
        content_files: Dict[str, str],
        target_files: Dict[Tuple[str, str], str],
        char_to_styles: Dict[str, List[str]],
    ) -> Dict[str, Dict]:
        """
        âœ… CORRECTED: Create train/val splits
        - Train gets: random subset of chars + their styles
        - Val gets: remaining chars + their styles
        """
        logging.info("\n" + "=" * 70)
        logging.info("CREATING TRAIN/VAL SPLITS")
        logging.info("=" * 70)
        
        all_chars = sorted(list(content_files.keys()))
        num_chars = len(all_chars)
        
        num_val_chars = max(1, int(num_chars * self.config.val_split_ratio))
        num_train_chars = num_chars - num_val_chars
        
        # Shuffle and split characters
        shuffled_chars = all_chars.copy()
        random.shuffle(shuffled_chars)
        
        train_chars = set(shuffled_chars[:num_train_chars])
        val_chars = set(shuffled_chars[num_train_chars:])
        
        # For each split, get the styles that exist for those chars
        train_styles = set()
        val_styles = set()
        
        for (char, style) in target_files.keys():
            if char in train_chars:
                train_styles.add(style)
            elif char in val_chars:
                val_styles.add(style)
        
        scenarios = {
            "train": {
                "characters": sorted(list(train_chars)),
                "styles": sorted(list(train_styles)),
                "description": "Training split (seen characters + seen styles)",
            },
            "val": {
                "characters": sorted(list(val_chars)),
                "styles": sorted(list(val_styles)),
                "description": "Validation split (unseen characters + unseen styles)",
            },
        }
        
        logging.info("\nğŸ“Š Split Statistics:")
        logging.info(f"  Total chars: {num_chars} â†’ train: {num_train_chars}, val: {num_val_chars}")
        
        for split_name, split_data in scenarios.items():
            logging.info(
                f"\n  {split_name}:"
            )
            logging.info(f"    Chars: {len(split_data['characters'])}")
            logging.info(f"    Styles: {len(split_data['styles'])}")
        
        return scenarios

    def copy_images_for_split(
        self,
        split_name: str,
        split_dir: Path,
        scenarios: Dict[str, Dict],
        content_files: Dict[str, str],
        target_files: Dict[Tuple[str, str], str],
    ) -> Tuple[int, int, int]:
        """Copy images for a specific split"""
        split_config = scenarios[split_name]
        allowed_chars = set(split_config["characters"])
        allowed_styles = set(split_config["styles"])
        
        # Create directories
        split_content_dir = split_dir / "ContentImage"
        split_target_dir = split_dir / "TargetImage"
        split_content_dir.mkdir(parents=True, exist_ok=True)
        split_target_dir.mkdir(parents=True, exist_ok=True)
        
        # Create style subdirectories
        for style in allowed_styles:
            (split_target_dir / style).mkdir(exist_ok=True)
        
        source_content_dir = self.source_train_dir / "ContentImage"
        source_target_dir = self.source_train_dir / "TargetImage"
        
        content_copied = 0
        target_copied = 0
        skipped = 0
        
        # Copy content images
        logging.info(f"\n  ğŸ“¥ Copying content images for {split_name}...")
        for char in tqdm(
            sorted(allowed_chars),
            desc="  Content",
            ncols=80,
            unit="char",
            leave=False,
        ):
            if char not in content_files:
                skipped += 1
                continue
            
            hash_val = content_files[char]
            content_filename = get_content_filename(char)
            
            src_path = source_content_dir / content_filename
            dst_path = split_content_dir / content_filename
            
            if src_path.exists() and src_path.resolve() != dst_path.resolve():
                shutil.copy2(src_path, dst_path)
                content_copied += 1
            elif src_path.exists():
                content_copied += 1
            else:
                tqdm.write(f"    âš ï¸  Not found: {content_filename}")
                skipped += 1
        
        # Copy target images
        logging.info(f"  ğŸ“¥ Copying target images for {split_name}...")
        for (char, style), hash_val in tqdm(
            sorted(target_files.items()),
            desc="  Target",
            ncols=80,
            unit="pair",
            leave=False,
        ):
            # Only copy if char and style are in this split
            if char not in allowed_chars or style not in allowed_styles:
                continue
            
            target_filename = get_target_filename(char, style)
            style_dir = source_target_dir / style
            src_path = style_dir / target_filename
            dst_path = split_target_dir / style / target_filename
            
            if src_path.exists() and src_path.resolve() != dst_path.resolve():
                shutil.copy2(src_path, dst_path)
                target_copied += 1
            elif src_path.exists():
                target_copied += 1
            else:
                tqdm.write(f"    âš ï¸  Not found: {target_filename}")
                skipped += 1
        
        logging.info(
            f"  âœ“ {split_name}: {content_copied} content, {target_copied} target (skipped: {skipped})"
        )
        
        return content_copied, target_copied, skipped

    def _copy_and_filter_checkpoint(
        self,
        split_name: str,
        split_dir: Path,
        allowed_chars: Set[str],
        allowed_styles: Set[str],
        target_files: Dict[Tuple[str, str], str],
    ) -> None:
        """
        âœ… Filter results_checkpoint.json to only include generations
        that have both content and target in this split
        """
        logging.info(f"\n  ğŸ“‹ Filtering checkpoint for {split_name}...")
        
        original_checkpoint_path = self.source_train_dir / "results_checkpoint.json"
        
        if not original_checkpoint_path.exists():
            logging.info(f"    âš ï¸  No checkpoint found, skipping")
            return
        
        try:
            with open(original_checkpoint_path, "r", encoding="utf-8") as f:
                original_data = json.load(f)
        except Exception as e:
            logging.info(f"    âš ï¸  Error loading checkpoint: {e}")
            return
        
        # Filter generations
        original_generations = original_data.get("generations", [])
        filtered_generations = []
        
        for gen in tqdm(
            original_generations,
            desc="    Filtering",
            ncols=80,
            unit="gen",
            leave=False,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]",
        ):
            char = gen.get("character")
            style = gen.get("style")
            
            # Include only if:
            # 1. Character is in this split
            # 2. Style is in this split
            # 3. Target image actually exists in this split
            if (char in allowed_chars and 
                style in allowed_styles and 
                (char, style) in target_files):
                filtered_generations.append(gen)
        
        # Create checkpoint for this split
        split_checkpoint = {
            "split": split_name,
            "num_characters": len(allowed_chars),
            "num_styles": len(allowed_styles),
            "num_generations": len(filtered_generations),
            "characters": sorted(list(allowed_chars)),
            "styles": sorted(list(allowed_styles)),
            "generations": filtered_generations,
            "fonts": original_data.get("fonts", []),
            "metrics": {},
            "original_source": str(self.source_train_dir),
            "filtered_from": str(original_checkpoint_path),
        }
        
        split_checkpoint_path = split_dir / "results_checkpoint.json"
        
        with open(split_checkpoint_path, "w", encoding="utf-8") as f:
            json.dump(split_checkpoint, f, indent=2, ensure_ascii=False)
        
        logging.info(
            f"    âœ“ Saved: {len(filtered_generations)}/{len(original_generations)} generations"
        )

    def create_splits(self) -> None:
        """Main function to create train/val splits"""
        logging.info("\n" + "=" * 70)
        logging.info("FONTDIFFUSION VALIDATION SPLIT CREATOR")
        logging.info("=" * 70)
        
        # Analyze data
        content_files, target_files, char_to_styles = self.analyze_data()
        
        # Create split scenarios
        scenarios = self.create_simple_splits(
            content_files, target_files, char_to_styles
        )
        
        # Create train split
        logging.info("\nğŸ“ Creating train split...")
        self.copy_images_for_split(
            "train", self.train_dir, scenarios, content_files, target_files
        )
        self._copy_and_filter_checkpoint(
            "train",
            self.train_dir,
            set(scenarios["train"]["characters"]),
            set(scenarios["train"]["styles"]),
            {pair: hash_val for pair, hash_val in target_files.items()
             if pair[0] in scenarios["train"]["characters"]},
        )
        
        # Create val split
        logging.info(f"\nğŸ“ Creating val split...")
        self.copy_images_for_split(
            "val", self.val_dir, scenarios, content_files, target_files
        )
        self._copy_and_filter_checkpoint(
            "val",
            self.val_dir,
            set(scenarios["val"]["characters"]),
            set(scenarios["val"]["styles"]),
            {pair: hash_val for pair, hash_val in target_files.items()
             if pair[0] in scenarios["val"]["characters"]},
        )
        
        # Save metadata
        self._save_metadata(scenarios)

    def _save_metadata(self, scenarios: Dict[str, Dict]) -> None:
        """Save split information to JSON"""
        metadata_path = self.data_root / "split_info.json"
        
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(scenarios, f, indent=2, ensure_ascii=False)
        
        logging.info(f"\nâœ“ Saved split metadata to {metadata_path}")


def create_validation_split(
    data_root: str,
    val_split_ratio: float = 0.2,
    random_seed: int = 42,
) -> None:
    """Create validation splits with proper checkpoint filtering"""
    config = ValidationSplitConfig(
        data_root=data_root,
        val_split_ratio=val_split_ratio,
        random_seed=random_seed,
    )
    
    creator = ValidationSplitCreator(config)
    creator.create_splits()
    
    logging.info("\n" + "=" * 70)
    logging.info("âœ“ SPLIT CREATION COMPLETE")
    logging.info("=" * 70)
    logging.info("\nâœ… Created:")
    logging.info("  ğŸ“ train/")
    logging.info("    â”œâ”€â”€ ContentImage/ (training chars)")
    logging.info("    â”œâ”€â”€ TargetImage/ (training styles)")
    logging.info("    â””â”€â”€ results_checkpoint.json (filtered)")
    logging.info("  ğŸ“ val/")
    logging.info("    â”œâ”€â”€ ContentImage/ (validation chars)")
    logging.info("    â”œâ”€â”€ TargetImage/ (validation styles)")
    logging.info("    â””â”€â”€ results_checkpoint.json (filtered)")
    logging.info("\nğŸ’¡ Guarantees:")
    logging.info("  âœ“ Every target has matching content")
    logging.info("  âœ“ Checkpoint contains only relevant generations")
    logging.info("  âœ“ Train and val are completely disjoint")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Create train/val splits with checkpoint filtering"
    )
    parser.add_argument(
        "--data_root", type=str, default="data_examples", help="Root data directory"
    )
    parser.add_argument(
        "--val_ratio", type=float, default=0.2, help="Validation split ratio"
    )
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    
    args = parser.parse_args()
    
    try:
        create_validation_split(
            data_root=args.data_root,
            val_split_ratio=args.val_ratio,
            random_seed=args.seed,
        )
    except Exception as e:
        logging.error(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        import sys
        sys.exit(1)